{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f45f3c-9656-4d44-be67-173dc2aa7872",
   "metadata": {},
   "source": [
    "### Breakdown and Explanation\n",
    "\n",
    "#### Intrinsic Trade-Off in CUDA Device Memories\n",
    "\n",
    "1. **Global Memory**:\n",
    "    - **Size**: Large\n",
    "    - **Speed**: Slow\n",
    "    - **Description**: Global memory resides off the processor chip and is implemented with DRAM technology. It allows large amounts of data to be stored but has long access latencies and relatively low access bandwidth.\n",
    "\n",
    "2. **Shared Memory**:\n",
    "    - **Size**: Small\n",
    "    - **Speed**: Fast\n",
    "    - **Description**: Shared memory is on-chip and provides very short access latencies and high access bandwidth. It is a critical resource for achieving high execution speed in CUDA kernels.\n",
    "\n",
    "#### Common Strategy: Tiling\n",
    "\n",
    "To optimize the use of these memory types, a common strategy is to **partition the data into subsets called tiles**. Each tile is a subset of the data that can fit into the shared memory.\n",
    "\n",
    "- **Tiling Analogy**:\n",
    "    - **Large Wall**: Represents the global memory data.\n",
    "    - **Tiles**: Represent subsets of the data that each fit into the shared memory.\n",
    "\n",
    "- **Criterion for Tiling**:\n",
    "    - The kernel computation on these tiles should be **independent** of each other. This ensures that each tile can be processed separately without needing to access data from other tiles.\n",
    "\n",
    "#### Matrix Multiplication Example\n",
    "\n",
    "To illustrate the concept of tiling, consider a matrix multiplication example.\n",
    "\n",
    "- **Matrix Multiplication**:\n",
    "    - **d_P[y*Width + x]**: Abbreviated as $\\mathrm{P}_{y,x}$\n",
    "    - **d_M[y*Width + x]**: Abbreviated as $\\mathrm{M}_{y,x}$\n",
    "    - **d_N[y*Width + x]**: Abbreviated as $\\mathrm{N}_{y,x}$\n",
    "\n",
    "- **Block Configuration**:\n",
    "    - The example uses four $2 \\times 2$ blocks to compute the matrix $P$.\n",
    "\n",
    "- **Block (0,0) Computation**:\n",
    "    - The four threads in block $(0,0)$ compute $\\mathrm{P}_{0,0}$, $\\mathrm{P}_{0,1}$, $\\mathrm{P}_{1,0}$, and $\\mathrm{P}_{1,1}$.\n",
    "\n",
    "- **Thread Access Pattern**:\n",
    "    - **Thread $(0,0)$**:\n",
    "        - Reads $\\mathrm{M}_{0,0}$ and $\\mathrm{N}_{0,0}$\n",
    "        - Followed by $\\mathrm{M}_{0,1}$ and $\\mathrm{N}_{1,0}$\n",
    "        - Followed by $\\mathrm{M}_{0,2}$ and $\\mathrm{N}_{2,0}$\n",
    "        - Followed by $\\mathrm{M}_{0,3}$ and $\\mathrm{N}_{3,0}$\n",
    "\n",
    "    - **Thread $(0,1)$**:\n",
    "        - Reads $\\mathrm{M}_{0,0}$ and $\\mathrm{N}_{0,1}$\n",
    "        - Followed by $\\mathrm{M}_{0,1}$ and $\\mathrm{N}_{1,1}$\n",
    "        - Followed by $\\mathrm{M}_{0,2}$ and $\\mathrm{N}_{2,1}$\n",
    "        - Followed by $\\mathrm{M}_{0,3}$ and $\\mathrm{N}_{3,1}$\n",
    "\n",
    "### Concrete Example\n",
    "\n",
    "Suppose we have two matrices $M$ and $N$, and we want to compute their product $P$, where $P = M \\times N$.\n",
    "\n",
    "#### Matrices\n",
    "\n",
    "- **Matrix $M$** (4x4):\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "\\mathrm{M}_{0,0} & \\mathrm{M}_{0,1} & \\mathrm{M}_{0,2} & \\mathrm{M}_{0,3} \\\\\n",
    "\\mathrm{M}_{1,0} & \\mathrm{M}_{1,1} & \\mathrm{M}_{1,2} & \\mathrm{M}_{1,3} \\\\\n",
    "\\mathrm{M}_{2,0} & \\mathrm{M}_{2,1} & \\mathrm{M}_{2,2} & \\mathrm{M}_{2,3} \\\\\n",
    "\\mathrm{M}_{3,0} & \\mathrm{M}_{3,1} & \\mathrm{M}_{3,2} & \\mathrm{M}_{3,3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "- **Matrix $N$** (4x4):\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "\\mathrm{N}_{0,0} & \\mathrm{N}_{0,1} & \\mathrm{N}_{0,2} & \\mathrm{N}_{0,3} \\\\\n",
    "\\mathrm{N}_{1,0} & \\mathrm{N}_{1,1} & \\mathrm{N}_{1,2} & \\mathrm{N}_{1,3} \\\\\n",
    "\\mathrm{N}_{2,0} & \\mathrm{N}_{2,1} & \\mathrm{N}_{2,2} & \\mathrm{N}_{2,3} \\\\\n",
    "\\mathrm{N}_{3,0} & \\mathrm{N}_{3,1} & \\mathrm{N}_{3,2} & \\mathrm{N}_{3,3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "#### Tiling\n",
    "\n",
    "- **Tiles**: Suppose we partition the matrices into $2 \\times 2$ tiles.\n",
    "\n",
    "- **Tile for Block $(0,0)$**:\n",
    "    - **Submatrix of $M$**:\n",
    "    \\[\n",
    "    \\begin{bmatrix}\n",
    "    \\mathrm{M}_{0,0} & \\mathrm{M}_{0,1} \\\\\n",
    "    \\mathrm{M}_{1,0} & \\mathrm{M}_{1,1} \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\]\n",
    "    - **Submatrix of $N$**:\n",
    "    \\[\n",
    "    \\begin{bmatrix}\n",
    "    \\mathrm{N}_{0,0} & \\mathrm{N}_{0,1} \\\\\n",
    "    \\mathrm{N}_{1,0} & \\mathrm{N}_{1,1} \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\]\n",
    "\n",
    "- **Computation by Threads in Block $(0,0)$**:\n",
    "    - **Thread $(0,0)$**:\n",
    "        - Computes $\\mathrm{P}_{0,0} = \\mathrm{M}_{0,0} \\times \\mathrm{N}_{0,0} + \\mathrm{M}_{0,1} \\times \\mathrm{N}_{1,0} + \\mathrm{M}_{0,2} \\times \\mathrm{N}_{2,0} + \\mathrm{M}_{0,3} \\times \\mathrm{N}_{3,0}$\n",
    "    - **Thread $(0,1)$**:\n",
    "        - Computes $\\mathrm{P}_{0,1} = \\mathrm{M}_{0,0} \\times \\mathrm{N}_{0,1} + \\mathrm{M}_{0,1} \\times \\mathrm{N}_{1,1} + \\mathrm{M}_{0,2} \\times \\mathrm{N}_{2,1} + \\mathrm{M}_{0,3} \\times \\mathrm{N}_{3,1}$\n",
    "    - **Thread $(1,0)$**:\n",
    "        - Computes $\\mathrm{P}_{1,0} = \\mathrm{M}_{1,0} \\times \\mathrm{N}_{0,0} + \\mathrm{M}_{1,1} \\times \\mathrm{N}_{1,0} + \\mathrm{M}_{1,2} \\times \\mathrm{N}_{2,0} + \\mathrm{M}_{1,3} \\times \\mathrm{N}_{3,0}$\n",
    "    - **Thread $(1,1)$**:\n",
    "        - Computes $\\mathrm{P}_{1,1} = \\mathrm{M}_{1,0} \\times \\mathrm{N}_{0,1} + \\mathrm{M}_{1,1} \\times \\mathrm{N}_{1,1} + \\mathrm{M}_{1,2} \\times \\mathrm{N}_{2,1} + \\mathrm{M}_{1,3} \\times \\mathrm{N}_{3,1}$\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Trade-Off**: Global memory is large but slow, while shared memory is small but fast.\n",
    "- **Tiling Strategy**: Partition data into tiles that fit into shared memory to optimize performance.\n",
    "- **Matrix Multiplication Example**: Illustrates how tiling can be applied, showing how threads in a block compute submatrix products independently.\n",
    "\n",
    "By understanding and applying these concepts, CUDA programmers can significantly enhance the performance of their kernels by minimizing global memory access and maximizing the use of shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15565d-c0de-4ac0-9b5d-e49f22f8915d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
