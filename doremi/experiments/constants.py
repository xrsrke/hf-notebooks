# NOTE: not verify if the following is correct
# NOTE: from experiment 2
REFERENCE_1B_200K_TOKEN_RATIO = {
    "arc:challenge": {
    "acc": 0.2,
    "acc_stderr": 0.012655439943366641,
    "acc_norm": 0.255,
    "acc_norm_stderr": 0.013790038620872832
    },
    "arc:easy": {
    "acc": 0.452,
    "acc_stderr": 0.01574623586588068,
    "acc_norm": 0.415,
    "acc_norm_stderr": 0.015589035185604633
    },
    "commonsense_qa": {
    "acc": 0.269,
    "acc_stderr": 0.014029819522568193,
    "acc_norm": 0.293,
    "acc_norm_stderr": 0.01439994299844127
    },
    "hellaswag": {
    "acc": 0.331,
    "acc_stderr": 0.014888272588203938,
    "acc_norm": 0.365,
    "acc_norm_stderr": 0.015231776226264898
    },
    "mmlu:abstract_algebra": {
    "acc": 0.22,
    "acc_stderr": 0.0416333199893227,
    "acc_norm": 0.25,
    "acc_norm_stderr": 0.04351941398892446
    },
    "mmlu:anatomy": {
    "acc": 0.32592592592592595,
    "acc_stderr": 0.040491220417025055,
    "acc_norm": 0.2814814814814815,
    "acc_norm_stderr": 0.03885004245800254
    },
    "mmlu:astronomy": {
    "acc": 0.20394736842105263,
    "acc_stderr": 0.0327900040631005,
    "acc_norm": 0.3157894736842105,
    "acc_norm_stderr": 0.0378272898086547
    },
    "mmlu:business_ethics": {
    "acc": 0.46,
    "acc_stderr": 0.05009082659620333,
    "acc_norm": 0.32,
    "acc_norm_stderr": 0.046882617226215034
    },
    "mmlu:clinical_knowledge": {
    "acc": 0.22264150943396227,
    "acc_stderr": 0.0256042334708991,
    "acc_norm": 0.30943396226415093,
    "acc_norm_stderr": 0.028450154794118627
    },
    "mmlu:college_biology": {
    "acc": 0.2847222222222222,
    "acc_stderr": 0.03773809990686935,
    "acc_norm": 0.25,
    "acc_norm_stderr": 0.03621034121889507
    },
    "mmlu:college_chemistry": {
    "acc": 0.29,
    "acc_stderr": 0.045604802157206845,
    "acc_norm": 0.24,
    "acc_norm_stderr": 0.04292346959909282
    },
    "mmlu:college_computer_science": {
    "acc": 0.23,
    "acc_stderr": 0.04229525846816505,
    "acc_norm": 0.25,
    "acc_norm_stderr": 0.04351941398892446
    },
    "mmlu:college_mathematics": {
    "acc": 0.16,
    "acc_stderr": 0.03684529491774709,
    "acc_norm": 0.19,
    "acc_norm_stderr": 0.03942772444036625
    },
    "mmlu:college_medicine": {
    "acc": 0.2543352601156069,
    "acc_stderr": 0.0332055644308557,
    "acc_norm": 0.20809248554913296,
    "acc_norm_stderr": 0.030952890217749895
    },
    "mmlu:college_physics": {
    "acc": 0.17647058823529413,
    "acc_stderr": 0.03793281185307809,
    "acc_norm": 0.19607843137254902,
    "acc_norm_stderr": 0.03950581861179964
    },
    "mmlu:computer_security": {
    "acc": 0.21,
    "acc_stderr": 0.040936018074033256,
    "acc_norm": 0.29,
    "acc_norm_stderr": 0.04560480215720684
    },
    "mmlu:conceptual_physics": {
    "acc": 0.33617021276595743,
    "acc_stderr": 0.03088161852067694,
    "acc_norm": 0.26382978723404255,
    "acc_norm_stderr": 0.028809989854102967
    },
    "mmlu:econometrics": {
    "acc": 0.21052631578947367,
    "acc_stderr": 0.03835153954399419,
    "acc_norm": 0.21929824561403508,
    "acc_norm_stderr": 0.038924311065187546
    },
    "mmlu:electrical_engineering": {
    "acc": 0.25517241379310346,
    "acc_stderr": 0.03632984052707842,
    "acc_norm": 0.2689655172413793,
    "acc_norm_stderr": 0.036951833116502325
    },
    "mmlu:elementary_mathematics": {
    "acc": 0.25132275132275134,
    "acc_stderr": 0.022340482339643898,
    "acc_norm": 0.23015873015873015,
    "acc_norm_stderr": 0.02167921966369315
    },
    "mmlu:formal_logic": {
    "acc": 0.30158730158730157,
    "acc_stderr": 0.04104947269903394,
    "acc_norm": 0.29365079365079366,
    "acc_norm_stderr": 0.040735243221471255
    },
    "mmlu:global_facts": {
    "acc": 0.26,
    "acc_stderr": 0.044084400227680794,
    "acc_norm": 0.26,
    "acc_norm_stderr": 0.04408440022768078
    },
    "mmlu:high_school_biology": {
    "acc": 0.23548387096774193,
    "acc_stderr": 0.024137632429337707,
    "acc_norm": 0.2870967741935484,
    "acc_norm_stderr": 0.025736542745594525
    },
    "mmlu:high_school_chemistry": {
    "acc": 0.16748768472906403,
    "acc_stderr": 0.026273086047535407,
    "acc_norm": 0.21674876847290642,
    "acc_norm_stderr": 0.028990331252516235
    },
    "mmlu:high_school_computer_science": {
    "acc": 0.24,
    "acc_stderr": 0.042923469599092816,
    "acc_norm": 0.34,
    "acc_norm_stderr": 0.047609522856952365
    },
    "mmlu:high_school_european_history": {
    "acc": 0.22424242424242424,
    "acc_stderr": 0.03256866661681102,
    "acc_norm": 0.3333333333333333,
    "acc_norm_stderr": 0.036810508691615486
    },
    "mmlu:high_school_geography": {
    "acc": 0.2727272727272727,
    "acc_stderr": 0.03173071239071724,
    "acc_norm": 0.3181818181818182,
    "acc_norm_stderr": 0.033184773338453315
    },
    "mmlu:high_school_government_and_politics": {
    "acc": 0.26424870466321243,
    "acc_stderr": 0.03182155050916646,
    "acc_norm": 0.3160621761658031,
    "acc_norm_stderr": 0.033553973696861736
    },
    "mmlu:high_school_macroeconomics": {
    "acc": 0.22564102564102564,
    "acc_stderr": 0.02119363252514853,
    "acc_norm": 0.24871794871794872,
    "acc_norm_stderr": 0.021916957709213803
    },
    "mmlu:high_school_mathematics": {
    "acc": 0.13703703703703704,
    "acc_stderr": 0.020967126643677114,
    "acc_norm": 0.2074074074074074,
    "acc_norm_stderr": 0.02472071319395216
    },
    "mmlu:high_school_microeconomics": {
    "acc": 0.25210084033613445,
    "acc_stderr": 0.028205545033277726,
    "acc_norm": 0.3319327731092437,
    "acc_norm_stderr": 0.030588697013783663
    },
    "mmlu:high_school_physics": {
    "acc": 0.25165562913907286,
    "acc_stderr": 0.035433042343899844,
    "acc_norm": 0.2781456953642384,
    "acc_norm_stderr": 0.03658603262763743
    },
    "mmlu:high_school_psychology": {
    "acc": 0.326605504587156,
    "acc_stderr": 0.0201069908899373,
    "acc_norm": 0.29908256880733947,
    "acc_norm_stderr": 0.019630417285415164
    },
    "mmlu:high_school_statistics": {
    "acc": 0.27314814814814814,
    "acc_stderr": 0.030388051301678116,
    "acc_norm": 0.26851851851851855,
    "acc_norm_stderr": 0.030225226160012407
    },
    "mmlu:high_school_us_history": {
    "acc": 0.2549019607843137,
    "acc_stderr": 0.030587591351604257,
    "acc_norm": 0.29901960784313725,
    "acc_norm_stderr": 0.032133257173736156
    },
    "mmlu:high_school_world_history": {
    "acc": 0.21940928270042195,
    "acc_stderr": 0.026939106581553945,
    "acc_norm": 0.27848101265822783,
    "acc_norm_stderr": 0.029178682304842555
    },
    "mmlu:human_aging": {
    "acc": 0.30493273542600896,
    "acc_stderr": 0.030898610882477515,
    "acc_norm": 0.2645739910313901,
    "acc_norm_stderr": 0.02960510321703833
    },
    "mmlu:human_sexuality": {
    "acc": 0.33587786259541985,
    "acc_stderr": 0.04142313771996663,
    "acc_norm": 0.32061068702290074,
    "acc_norm_stderr": 0.04093329229834278
    },
    "mmlu:international_law": {
    "acc": 0.14049586776859505,
    "acc_stderr": 0.031722334260021585,
    "acc_norm": 0.23140495867768596,
    "acc_norm_stderr": 0.038498560987940904
    },
    "mmlu:jurisprudence": {
    "acc": 0.2037037037037037,
    "acc_stderr": 0.03893542518824846,
    "acc_norm": 0.25925925925925924,
    "acc_norm_stderr": 0.042365112580946315
    },
    "mmlu:logical_fallacies": {
    "acc": 0.22699386503067484,
    "acc_stderr": 0.032910995786157686,
    "acc_norm": 0.36809815950920244,
    "acc_norm_stderr": 0.03789213935838396
    },
    "mmlu:machine_learning": {
    "acc": 0.25892857142857145,
    "acc_stderr": 0.04157751539865629,
    "acc_norm": 0.25892857142857145,
    "acc_norm_stderr": 0.04157751539865629
    },
    "mmlu:management": {
    "acc": 0.2621359223300971,
    "acc_stderr": 0.043546310772605935,
    "acc_norm": 0.3592233009708738,
    "acc_norm_stderr": 0.04750458399041693
    },
    "mmlu:marketing": {
    "acc": 0.36752136752136755,
    "acc_stderr": 0.031585391577456365,
    "acc_norm": 0.3717948717948718,
    "acc_norm_stderr": 0.03166098891888077
    },
    "mmlu:medical_genetics": {
    "acc": 0.27,
    "acc_stderr": 0.044619604333847394,
    "acc_norm": 0.31,
    "acc_norm_stderr": 0.04648231987117316
    },
    "mmlu:miscellaneous": {
    "acc": 0.31417624521072796,
    "acc_stderr": 0.016599291735884904,
    "acc_norm": 0.3128991060025543,
    "acc_norm_stderr": 0.016580935940304034
    },
    "mmlu:moral_disputes": {
    "acc": 0.23121387283236994,
    "acc_stderr": 0.022698657167855716,
    "acc_norm": 0.21676300578034682,
    "acc_norm_stderr": 0.022183477668412853
    },
    "mmlu:moral_scenarios": {
    "acc": 0.23798882681564246,
    "acc_stderr": 0.014242630070574915,
    "acc_norm": 0.27262569832402234,
    "acc_norm_stderr": 0.014893391735249588
    },
    "mmlu:nutrition": {
    "acc": 0.238562091503268,
    "acc_stderr": 0.024404394928087877,
    "acc_norm": 0.28431372549019607,
    "acc_norm_stderr": 0.02582916327275748
    },
    "mmlu:philosophy": {
    "acc": 0.2347266881028939,
    "acc_stderr": 0.024071805887677048,
    "acc_norm": 0.2765273311897106,
    "acc_norm_stderr": 0.02540383297817962
    },
    "mmlu:prehistory": {
    "acc": 0.2839506172839506,
    "acc_stderr": 0.025089478523765127,
    "acc_norm": 0.20987654320987653,
    "acc_norm_stderr": 0.02265834408598137
    },
    "mmlu:professional_accounting": {
    "acc": 0.26595744680851063,
    "acc_stderr": 0.02635806569888059,
    "acc_norm": 0.2553191489361702,
    "acc_norm_stderr": 0.026011992930902016
    },
    "mmlu:professional_law": {
    "acc": 0.249,
    "acc_stderr": 0.0136816002787023,
    "acc_norm": 0.261,
    "acc_norm_stderr": 0.013895037677965127
    },
    "mmlu:professional_medicine": {
    "acc": 0.25,
    "acc_stderr": 0.026303648393696036,
    "acc_norm": 0.25735294117647056,
    "acc_norm_stderr": 0.026556519470041524
    },
    "mmlu:professional_psychology": {
    "acc": 0.2647058823529412,
    "acc_stderr": 0.017848089574913226,
    "acc_norm": 0.2647058823529412,
    "acc_norm_stderr": 0.01784808957491322
    },
    "mmlu:public_relations": {
    "acc": 0.35454545454545455,
    "acc_stderr": 0.04582004841505417,
    "acc_norm": 0.21818181818181817,
    "acc_norm_stderr": 0.039559328617958335
    },
    "mmlu:security_studies": {
    "acc": 0.3224489795918367,
    "acc_stderr": 0.029923100563683913,
    "acc_norm": 0.20408163265306123,
    "acc_norm_stderr": 0.02580128347509051
    },
    "mmlu:sociology": {
    "acc": 0.23383084577114427,
    "acc_stderr": 0.029929415408348387,
    "acc_norm": 0.22388059701492538,
    "acc_norm_stderr": 0.0294752502360172
    },
    "mmlu:us_foreign_policy": {
    "acc": 0.23,
    "acc_stderr": 0.04229525846816506,
    "acc_norm": 0.24,
    "acc_norm_stderr": 0.04292346959909284
    },
    "mmlu:virology": {
    "acc": 0.26506024096385544,
    "acc_stderr": 0.03436024037944967,
    "acc_norm": 0.3072289156626506,
    "acc_norm_stderr": 0.03591566797824665
    },
    "mmlu:world_religions": {
    "acc": 0.2222222222222222,
    "acc_stderr": 0.03188578017686398,
    "acc_norm": 0.2631578947368421,
    "acc_norm_stderr": 0.033773102522091945
    },
    "openbookqa": {
    "acc": 0.148,
    "acc_stderr": 0.015896458561251242,
    "acc_norm": 0.268,
    "acc_norm_stderr": 0.019827714859587578
    },
    "piqa": {
    "acc": 0.646,
    "acc_stderr": 0.015129868238451772,
    "acc_norm": 0.665,
    "acc_norm_stderr": 0.014933117490932577
    },
    "siqa": {
    "acc": 0.379,
    "acc_stderr": 0.01534909100222535,
    "acc_norm": 0.4,
    "acc_norm_stderr": 0.015499685165842592
    },
    "winogrande": {
    "acc": 0.512,
    "acc_stderr": 0.015814743314581818,
    "acc_norm": 0.494,
    "acc_norm_stderr": 0.015818160898606715
    },
    "arc:_average": {
    "acc": 0.326,
    "acc_stderr": 0.014200837904623662,
    "acc_norm": 0.33499999999999996,
    "acc_norm_stderr": 0.014689536903238733
    },
    "mmlu:_average": {
    "acc": 0.2555524660548234,
    "acc_stderr": 0.0323545937553876,
    "acc_norm": 0.27142658511281265,
    "acc_norm_stderr": 0.03318514238761682
    },
}
REFERENCE_1B_110K_TOKEN_RATIO = {
    "arc:challenge": {
"acc": 0.207,
"acc_stderr": 0.012818553557843974,
"acc_norm": 0.247,
"acc_norm_stderr": 0.013644675781314135
},
"arc:easy": {
"acc": 0.438,
"acc_stderr": 0.01569721001969469,
"acc_norm": 0.407,
"acc_norm_stderr": 0.015543249100255544
},
"commonsense_qa": {
"acc": 0.265,
"acc_stderr": 0.013963164754809949,
"acc_norm": 0.292,
"acc_norm_stderr": 0.014385511563477345
},
"hellaswag": {
"acc": 0.322,
"acc_stderr": 0.014782913600996664,
"acc_norm": 0.355,
"acc_norm_stderr": 0.015139491543780529
},
"mmlu:abstract_algebra": {
"acc": 0.21,
"acc_stderr": 0.040936018074033256,
"acc_norm": 0.2,
"acc_norm_stderr": 0.04020151261036843
},
"mmlu:anatomy": {
"acc": 0.3111111111111111,
"acc_stderr": 0.03999262876617722,
"acc_norm": 0.25925925925925924,
"acc_norm_stderr": 0.037857144650666544
},
"mmlu:astronomy": {
"acc": 0.25,
"acc_stderr": 0.03523807393012047,
"acc_norm": 0.32894736842105265,
"acc_norm_stderr": 0.038234289699266046
},
"mmlu:business_ethics": {
"acc": 0.46,
"acc_stderr": 0.05009082659620333,
"acc_norm": 0.32,
"acc_norm_stderr": 0.04688261722621504
},
"mmlu:clinical_knowledge": {
"acc": 0.2641509433962264,
"acc_stderr": 0.027134291628741723,
"acc_norm": 0.3132075471698113,
"acc_norm_stderr": 0.02854479331905533
},
"mmlu:college_biology": {
"acc": 0.25,
"acc_stderr": 0.03621034121889507,
"acc_norm": 0.25,
"acc_norm_stderr": 0.03621034121889507
},
"mmlu:college_chemistry": {
"acc": 0.25,
"acc_stderr": 0.04351941398892446,
"acc_norm": 0.23,
"acc_norm_stderr": 0.04229525846816506
},
"mmlu:college_computer_science": {
"acc": 0.23,
"acc_stderr": 0.04229525846816505,
"acc_norm": 0.24,
"acc_norm_stderr": 0.042923469599092816
},
"mmlu:college_mathematics": {
"acc": 0.15,
"acc_stderr": 0.03588702812826371,
"acc_norm": 0.21,
"acc_norm_stderr": 0.040936018074033256
},
"mmlu:college_medicine": {
"acc": 0.2774566473988439,
"acc_stderr": 0.034140140070440354,
"acc_norm": 0.2254335260115607,
"acc_norm_stderr": 0.03186209851641143
},
"mmlu:college_physics": {
"acc": 0.17647058823529413,
"acc_stderr": 0.0379328118530781,
"acc_norm": 0.16666666666666666,
"acc_norm_stderr": 0.03708284662416545
},
"mmlu:computer_security": {
"acc": 0.19,
"acc_stderr": 0.03942772444036625,
"acc_norm": 0.28,
"acc_norm_stderr": 0.04512608598542127
},
"mmlu:conceptual_physics": {
"acc": 0.2978723404255319,
"acc_stderr": 0.02989614568209546,
"acc_norm": 0.24680851063829787,
"acc_norm_stderr": 0.028185441301234106
},
"mmlu:econometrics": {
"acc": 0.21929824561403508,
"acc_stderr": 0.03892431106518755,
"acc_norm": 0.20175438596491227,
"acc_norm_stderr": 0.03775205013583639
},
"mmlu:electrical_engineering": {
"acc": 0.2620689655172414,
"acc_stderr": 0.036646663372252565,
"acc_norm": 0.2689655172413793,
"acc_norm_stderr": 0.036951833116502325
},
"mmlu:elementary_mathematics": {
"acc": 0.24603174603174602,
"acc_stderr": 0.022182037202948368,
"acc_norm": 0.2037037037037037,
"acc_norm_stderr": 0.020742740560122663
},
"mmlu:formal_logic": {
"acc": 0.2698412698412698,
"acc_stderr": 0.03970158273235172,
"acc_norm": 0.24603174603174602,
"acc_norm_stderr": 0.03852273364924315
},
"mmlu:global_facts": {
"acc": 0.22,
"acc_stderr": 0.04163331998932269,
"acc_norm": 0.24,
"acc_norm_stderr": 0.042923469599092816
},
"mmlu:high_school_biology": {
"acc": 0.22903225806451613,
"acc_stderr": 0.023904914311782644,
"acc_norm": 0.2870967741935484,
"acc_norm_stderr": 0.025736542745594528
},
"mmlu:high_school_chemistry": {
"acc": 0.18226600985221675,
"acc_stderr": 0.02716334085964515,
"acc_norm": 0.2315270935960591,
"acc_norm_stderr": 0.02967833314144445
},
"mmlu:high_school_computer_science": {
"acc": 0.22,
"acc_stderr": 0.04163331998932269,
"acc_norm": 0.33,
"acc_norm_stderr": 0.04725815626252606
},
"mmlu:high_school_european_history": {
"acc": 0.23636363636363636,
"acc_stderr": 0.033175059300091805,
"acc_norm": 0.3151515151515151,
"acc_norm_stderr": 0.0362773057502241
},
"mmlu:high_school_geography": {
"acc": 0.2878787878787879,
"acc_stderr": 0.03225883512300992,
"acc_norm": 0.3181818181818182,
"acc_norm_stderr": 0.033184773338453315
},
"mmlu:high_school_government_and_politics": {
"acc": 0.24870466321243523,
"acc_stderr": 0.031195840877700293,
"acc_norm": 0.29015544041450775,
"acc_norm_stderr": 0.03275264467791516
},
"mmlu:high_school_macroeconomics": {
"acc": 0.24615384615384617,
"acc_stderr": 0.021840866990423077,
"acc_norm": 0.27692307692307694,
"acc_norm_stderr": 0.022688042352424994
},
"mmlu:high_school_mathematics": {
"acc": 0.15185185185185185,
"acc_stderr": 0.021881130957380472,
"acc_norm": 0.18518518518518517,
"acc_norm_stderr": 0.02368407558532269
},
"mmlu:high_school_microeconomics": {
"acc": 0.25210084033613445,
"acc_stderr": 0.02820554503327773,
"acc_norm": 0.3403361344537815,
"acc_norm_stderr": 0.030778057422931673
},
"mmlu:high_school_physics": {
"acc": 0.25165562913907286,
"acc_stderr": 0.035433042343899844,
"acc_norm": 0.2847682119205298,
"acc_norm_stderr": 0.036848815213890225
},
"mmlu:high_school_psychology": {
"acc": 0.3211009174311927,
"acc_stderr": 0.020018149772733744,
"acc_norm": 0.29724770642201837,
"acc_norm_stderr": 0.019595707224643544
},
"mmlu:high_school_statistics": {
"acc": 0.2777777777777778,
"acc_stderr": 0.030546745264953185,
"acc_norm": 0.2777777777777778,
"acc_norm_stderr": 0.030546745264953195
},
"mmlu:high_school_us_history": {
"acc": 0.24019607843137256,
"acc_stderr": 0.02998373305591362,
"acc_norm": 0.3137254901960784,
"acc_norm_stderr": 0.03256685484460389
},
"mmlu:high_school_world_history": {
"acc": 0.2320675105485232,
"acc_stderr": 0.02747974455080852,
"acc_norm": 0.27848101265822783,
"acc_norm_stderr": 0.029178682304842555
},
"mmlu:human_aging": {
"acc": 0.3094170403587444,
"acc_stderr": 0.03102441174057221,
"acc_norm": 0.2825112107623318,
"acc_norm_stderr": 0.030216831011508766
},
"mmlu:human_sexuality": {
"acc": 0.3435114503816794,
"acc_stderr": 0.041649760719448786,
"acc_norm": 0.29770992366412213,
"acc_norm_stderr": 0.040103589424622034
},
"mmlu:international_law": {
"acc": 0.14049586776859505,
"acc_stderr": 0.031722334260021585,
"acc_norm": 0.2396694214876033,
"acc_norm_stderr": 0.038968789850704164
},
"mmlu:jurisprudence": {
"acc": 0.18518518518518517,
"acc_stderr": 0.03755265865037182,
"acc_norm": 0.25,
"acc_norm_stderr": 0.04186091791394607
},
"mmlu:logical_fallacies": {
"acc": 0.25153374233128833,
"acc_stderr": 0.03408997886857529,
"acc_norm": 0.39263803680981596,
"acc_norm_stderr": 0.03836740907831028
},
"mmlu:machine_learning": {
"acc": 0.22321428571428573,
"acc_stderr": 0.039523019677025116,
"acc_norm": 0.25892857142857145,
"acc_norm_stderr": 0.04157751539865629
},
"mmlu:management": {
"acc": 0.27184466019417475,
"acc_stderr": 0.044052680241409216,
"acc_norm": 0.33980582524271846,
"acc_norm_stderr": 0.04689765937278135
},
"mmlu:marketing": {
"acc": 0.3333333333333333,
"acc_stderr": 0.03088273697413866,
"acc_norm": 0.36752136752136755,
"acc_norm_stderr": 0.03158539157745636
},
"mmlu:medical_genetics": {
"acc": 0.22,
"acc_stderr": 0.04163331998932269,
"acc_norm": 0.28,
"acc_norm_stderr": 0.04512608598542127
},
"mmlu:miscellaneous": {
"acc": 0.30395913154533843,
"acc_stderr": 0.016448321686769043,
"acc_norm": 0.3103448275862069,
"acc_norm_stderr": 0.01654378502604832
},
"mmlu:moral_disputes": {
"acc": 0.23410404624277456,
"acc_stderr": 0.022797110278071145,
"acc_norm": 0.21965317919075145,
"acc_norm_stderr": 0.022289638852617904
},
"mmlu:moral_scenarios": {
"acc": 0.23798882681564246,
"acc_stderr": 0.014242630070574915,
"acc_norm": 0.27262569832402234,
"acc_norm_stderr": 0.014893391735249588
},
"mmlu:nutrition": {
"acc": 0.25163398692810457,
"acc_stderr": 0.024848018263875202,
"acc_norm": 0.27124183006535946,
"acc_norm_stderr": 0.025457756696667867
},
"mmlu:philosophy": {
"acc": 0.2347266881028939,
"acc_stderr": 0.024071805887677048,
"acc_norm": 0.27009646302250806,
"acc_norm_stderr": 0.025218040373410622
},
"mmlu:prehistory": {
"acc": 0.27469135802469136,
"acc_stderr": 0.024836057868294677,
"acc_norm": 0.19753086419753085,
"acc_norm_stderr": 0.02215288992789897
},
"mmlu:professional_accounting": {
"acc": 0.25886524822695034,
"acc_stderr": 0.026129572527180848,
"acc_norm": 0.2553191489361702,
"acc_norm_stderr": 0.026011992930902013
},
"mmlu:professional_law": {
"acc": 0.245,
"acc_stderr": 0.01360735683959812,
"acc_norm": 0.261,
"acc_norm_stderr": 0.013895037677965127
},
"mmlu:professional_medicine": {
"acc": 0.23897058823529413,
"acc_stderr": 0.02590528064489301,
"acc_norm": 0.2757352941176471,
"acc_norm_stderr": 0.027146271936625162
},
"mmlu:professional_psychology": {
"acc": 0.25326797385620914,
"acc_stderr": 0.01759348689536683,
"acc_norm": 0.24673202614379086,
"acc_norm_stderr": 0.0174408203674025
},
"mmlu:public_relations": {
"acc": 0.37272727272727274,
"acc_stderr": 0.04631381319425465,
"acc_norm": 0.2545454545454545,
"acc_norm_stderr": 0.041723430387053825
},
"mmlu:security_studies": {
"acc": 0.3183673469387755,
"acc_stderr": 0.02982253379398205,
"acc_norm": 0.1836734693877551,
"acc_norm_stderr": 0.02478907133200763
},
"mmlu:sociology": {
"acc": 0.22388059701492538,
"acc_stderr": 0.029475250236017197,
"acc_norm": 0.22885572139303484,
"acc_norm_stderr": 0.02970528405677243
},
"mmlu:us_foreign_policy": {
"acc": 0.25,
"acc_stderr": 0.04351941398892446,
"acc_norm": 0.24,
"acc_norm_stderr": 0.04292346959909284
},
"mmlu:virology": {
"acc": 0.2469879518072289,
"acc_stderr": 0.03357351982064536,
"acc_norm": 0.28313253012048195,
"acc_norm_stderr": 0.03507295431370519
},
"mmlu:world_religions": {
"acc": 0.21052631578947367,
"acc_stderr": 0.03126781714663179,
"acc_norm": 0.29239766081871343,
"acc_norm_stderr": 0.03488647713457922
},
"openbookqa": {
"acc": 0.14,
"acc_stderr": 0.015533272840269641,
"acc_norm": 0.26,
"acc_norm_stderr": 0.019635965529725512
},
"piqa": {
"acc": 0.649,
"acc_stderr": 0.015100563798316407,
"acc_norm": 0.668,
"acc_norm_stderr": 0.014899597242811494
},
"siqa": {
"acc": 0.384,
"acc_stderr": 0.01538768276189707,
"acc_norm": 0.407,
"acc_norm_stderr": 0.015543249100255545
},
"winogrande": {
"acc": 0.502,
"acc_stderr": 0.015819173374302706,
"acc_norm": 0.49,
"acc_norm_stderr": 0.0158161357527732
},
"arc:_average": {
"acc": 0.3225,
"acc_stderr": 0.014257881788769331,
"acc_norm": 0.32699999999999996,
"acc_norm_stderr": 0.01459396244078484
},
"mmlu:_average": {
"acc": 0.2516786765286935,
"acc_stderr": 0.03215950484038863,
"acc_norm": 0.26717550864821876,
"acc_norm_stderr": 0.03296252600780637
}
}

# NOTE: from experiment 10
REFERENCE_1B_200K_UNIFORM = {
    "arc:challenge": {
"acc": 0.178,
"acc_stderr": 0.01210216767618358,
"acc_norm": 0.235,
"acc_norm_stderr": 0.013414729030247126
},
"arc:easy": {
"acc": 0.321,
"acc_stderr": 0.014770821817934644,
"acc_norm": 0.34,
"acc_norm_stderr": 0.014987482264363935
},
"commonsense_qa": {
"acc": 0.205,
"acc_stderr": 0.012772554096113102,
"acc_norm": 0.237,
"acc_norm_stderr": 0.013454070462577954
},
"hellaswag": {
"acc": 0.275,
"acc_stderr": 0.014127086556490526,
"acc_norm": 0.288,
"acc_norm_stderr": 0.01432694179723156
},
"mmlu:abstract_algebra": {
"acc": 0.23,
"acc_stderr": 0.042295258468165065,
"acc_norm": 0.25,
"acc_norm_stderr": 0.04351941398892446
},
"mmlu:anatomy": {
"acc": 0.23703703703703705,
"acc_stderr": 0.03673731683969506,
"acc_norm": 0.25925925925925924,
"acc_norm_stderr": 0.03785714465066654
},
"mmlu:astronomy": {
"acc": 0.20394736842105263,
"acc_stderr": 0.03279000406310051,
"acc_norm": 0.27631578947368424,
"acc_norm_stderr": 0.03639057569952924
},
"mmlu:business_ethics": {
"acc": 0.32,
"acc_stderr": 0.04688261722621505,
"acc_norm": 0.29,
"acc_norm_stderr": 0.04560480215720684
},
"mmlu:clinical_knowledge": {
"acc": 0.2188679245283019,
"acc_stderr": 0.025447863825108625,
"acc_norm": 0.3018867924528302,
"acc_norm_stderr": 0.02825420034443866
},
"mmlu:college_biology": {
"acc": 0.2152777777777778,
"acc_stderr": 0.03437079344106133,
"acc_norm": 0.2152777777777778,
"acc_norm_stderr": 0.03437079344106135
},
"mmlu:college_chemistry": {
"acc": 0.27,
"acc_stderr": 0.044619604333847394,
"acc_norm": 0.25,
"acc_norm_stderr": 0.04351941398892446
},
"mmlu:college_computer_science": {
"acc": 0.27,
"acc_stderr": 0.0446196043338474,
"acc_norm": 0.18,
"acc_norm_stderr": 0.03861229196653697
},
"mmlu:college_mathematics": {
"acc": 0.14,
"acc_stderr": 0.03487350880197771,
"acc_norm": 0.25,
"acc_norm_stderr": 0.04351941398892446
},
"mmlu:college_medicine": {
"acc": 0.2774566473988439,
"acc_stderr": 0.03414014007044036,
"acc_norm": 0.27167630057803466,
"acc_norm_stderr": 0.03391750322321659
},
"mmlu:college_physics": {
"acc": 0.20588235294117646,
"acc_stderr": 0.040233822736177476,
"acc_norm": 0.20588235294117646,
"acc_norm_stderr": 0.04023382273617746
},
"mmlu:computer_security": {
"acc": 0.25,
"acc_stderr": 0.04351941398892446,
"acc_norm": 0.27,
"acc_norm_stderr": 0.044619604333847394
},
"mmlu:conceptual_physics": {
"acc": 0.3191489361702128,
"acc_stderr": 0.03047297336338005,
"acc_norm": 0.24680851063829787,
"acc_norm_stderr": 0.028185441301234106
},
"mmlu:econometrics": {
"acc": 0.21929824561403508,
"acc_stderr": 0.03892431106518755,
"acc_norm": 0.19298245614035087,
"acc_norm_stderr": 0.037124548537213684
},
"mmlu:electrical_engineering": {
"acc": 0.2413793103448276,
"acc_stderr": 0.03565998174135302,
"acc_norm": 0.2620689655172414,
"acc_norm_stderr": 0.036646663372252565
},
"mmlu:elementary_mathematics": {
"acc": 0.1931216931216931,
"acc_stderr": 0.020330538160035643,
"acc_norm": 0.19576719576719576,
"acc_norm_stderr": 0.020435730971541794
},
"mmlu:formal_logic": {
"acc": 0.2619047619047619,
"acc_stderr": 0.039325376803928724,
"acc_norm": 0.24603174603174602,
"acc_norm_stderr": 0.038522733649243156
},
"mmlu:global_facts": {
"acc": 0.25,
"acc_stderr": 0.04351941398892446,
"acc_norm": 0.21,
"acc_norm_stderr": 0.040936018074033256
},
"mmlu:high_school_biology": {
"acc": 0.23225806451612904,
"acc_stderr": 0.02402225613030824,
"acc_norm": 0.2709677419354839,
"acc_norm_stderr": 0.025284416114900156
},
"mmlu:high_school_chemistry": {
"acc": 0.18719211822660098,
"acc_stderr": 0.027444924966882615,
"acc_norm": 0.2413793103448276,
"acc_norm_stderr": 0.030108330718011625
},
"mmlu:high_school_computer_science": {
"acc": 0.14,
"acc_stderr": 0.03487350880197772,
"acc_norm": 0.25,
"acc_norm_stderr": 0.04351941398892446
},
"mmlu:high_school_european_history": {
"acc": 0.22424242424242424,
"acc_stderr": 0.03256866661681102,
"acc_norm": 0.2787878787878788,
"acc_norm_stderr": 0.03501438706296781
},
"mmlu:high_school_geography": {
"acc": 0.25757575757575757,
"acc_stderr": 0.03115626951964685,
"acc_norm": 0.32323232323232326,
"acc_norm_stderr": 0.03332299921070645
},
"mmlu:high_school_government_and_politics": {
"acc": 0.22797927461139897,
"acc_stderr": 0.030276909945178253,
"acc_norm": 0.22797927461139897,
"acc_norm_stderr": 0.03027690994517826
},
"mmlu:high_school_macroeconomics": {
"acc": 0.23846153846153847,
"acc_stderr": 0.02160629449464773,
"acc_norm": 0.26153846153846155,
"acc_norm_stderr": 0.02228214120420442
},
"mmlu:high_school_mathematics": {
"acc": 0.16296296296296298,
"acc_stderr": 0.02251856199768264,
"acc_norm": 0.2074074074074074,
"acc_norm_stderr": 0.02472071319395217
},
"mmlu:high_school_microeconomics": {
"acc": 0.23109243697478993,
"acc_stderr": 0.027381406927868956,
"acc_norm": 0.28991596638655465,
"acc_norm_stderr": 0.029472485833136094
},
"mmlu:high_school_physics": {
"acc": 0.2582781456953642,
"acc_stderr": 0.035737053147634576,
"acc_norm": 0.24503311258278146,
"acc_norm_stderr": 0.03511807571804725
},
"mmlu:high_school_psychology": {
"acc": 0.27155963302752295,
"acc_stderr": 0.019069098363191442,
"acc_norm": 0.25871559633027524,
"acc_norm_stderr": 0.018776052319619627
},
"mmlu:high_school_statistics": {
"acc": 0.2777777777777778,
"acc_stderr": 0.030546745264953185,
"acc_norm": 0.3055555555555556,
"acc_norm_stderr": 0.03141554629402543
},
"mmlu:high_school_us_history": {
"acc": 0.24509803921568626,
"acc_stderr": 0.030190282453501936,
"acc_norm": 0.2696078431372549,
"acc_norm_stderr": 0.03114557065948678
},
"mmlu:high_school_world_history": {
"acc": 0.20253164556962025,
"acc_stderr": 0.02616056824660146,
"acc_norm": 0.25738396624472576,
"acc_norm_stderr": 0.028458820991460288
},
"mmlu:human_aging": {
"acc": 0.3632286995515695,
"acc_stderr": 0.032277904428505,
"acc_norm": 0.2556053811659193,
"acc_norm_stderr": 0.029275891003969927
},
"mmlu:human_sexuality": {
"acc": 0.3282442748091603,
"acc_stderr": 0.041184385658062976,
"acc_norm": 0.3435114503816794,
"acc_norm_stderr": 0.04164976071944878
},
"mmlu:international_law": {
"acc": 0.12396694214876033,
"acc_stderr": 0.03008309871603521,
"acc_norm": 0.19834710743801653,
"acc_norm_stderr": 0.03640118271990945
},
"mmlu:jurisprudence": {
"acc": 0.19444444444444445,
"acc_stderr": 0.038260763248848646,
"acc_norm": 0.23148148148148148,
"acc_norm_stderr": 0.04077494709252627
},
"mmlu:logical_fallacies": {
"acc": 0.26380368098159507,
"acc_stderr": 0.03462419931615624,
"acc_norm": 0.37423312883435583,
"acc_norm_stderr": 0.03802068102899616
},
"mmlu:machine_learning": {
"acc": 0.26785714285714285,
"acc_stderr": 0.04203277291467763,
"acc_norm": 0.19642857142857142,
"acc_norm_stderr": 0.03770970049347019
},
"mmlu:management": {
"acc": 0.20388349514563106,
"acc_stderr": 0.039891398595317706,
"acc_norm": 0.2621359223300971,
"acc_norm_stderr": 0.043546310772605956
},
"mmlu:marketing": {
"acc": 0.2863247863247863,
"acc_stderr": 0.02961432369045665,
"acc_norm": 0.2777777777777778,
"acc_norm_stderr": 0.02934311479809447
},
"mmlu:medical_genetics": {
"acc": 0.23,
"acc_stderr": 0.04229525846816505,
"acc_norm": 0.23,
"acc_norm_stderr": 0.04229525846816506
},
"mmlu:miscellaneous": {
"acc": 0.25287356321839083,
"acc_stderr": 0.015543377313719681,
"acc_norm": 0.24904214559386972,
"acc_norm_stderr": 0.015464676163395967
},
"mmlu:moral_disputes": {
"acc": 0.21676300578034682,
"acc_stderr": 0.022183477668412853,
"acc_norm": 0.19653179190751446,
"acc_norm_stderr": 0.02139396140436385
},
"mmlu:moral_scenarios": {
"acc": 0.23798882681564246,
"acc_stderr": 0.014242630070574915,
"acc_norm": 0.27262569832402234,
"acc_norm_stderr": 0.014893391735249588
},
"mmlu:nutrition": {
"acc": 0.20915032679738563,
"acc_stderr": 0.023287685312334806,
"acc_norm": 0.29411764705882354,
"acc_norm_stderr": 0.02609016250427905
},
"mmlu:philosophy": {
"acc": 0.2379421221864952,
"acc_stderr": 0.024185150647818707,
"acc_norm": 0.2572347266881029,
"acc_norm_stderr": 0.024826171289250888
},
"mmlu:prehistory": {
"acc": 0.25308641975308643,
"acc_stderr": 0.024191808600713002,
"acc_norm": 0.19135802469135801,
"acc_norm_stderr": 0.02188770461339616
},
"mmlu:professional_accounting": {
"acc": 0.25177304964539005,
"acc_stderr": 0.0258921511567094,
"acc_norm": 0.23404255319148937,
"acc_norm_stderr": 0.025257861359432414
},
"mmlu:professional_law": {
"acc": 0.255,
"acc_stderr": 0.013790038620872826,
"acc_norm": 0.26,
"acc_norm_stderr": 0.013877773329774166
},
"mmlu:professional_medicine": {
"acc": 0.22426470588235295,
"acc_stderr": 0.025336848563332376,
"acc_norm": 0.2426470588235294,
"acc_norm_stderr": 0.026040662474201268
},
"mmlu:professional_psychology": {
"acc": 0.25,
"acc_stderr": 0.01751781884501444,
"acc_norm": 0.2565359477124183,
"acc_norm_stderr": 0.017667841612378984
},
"mmlu:public_relations": {
"acc": 0.3181818181818182,
"acc_stderr": 0.04461272175910508,
"acc_norm": 0.2545454545454545,
"acc_norm_stderr": 0.041723430387053825
},
"mmlu:security_studies": {
"acc": 0.30612244897959184,
"acc_stderr": 0.029504896454595968,
"acc_norm": 0.19183673469387755,
"acc_norm_stderr": 0.025206963154225374
},
"mmlu:sociology": {
"acc": 0.21393034825870647,
"acc_stderr": 0.02899690969332891,
"acc_norm": 0.2835820895522388,
"acc_norm_stderr": 0.031871875379197986
},
"mmlu:us_foreign_policy": {
"acc": 0.23,
"acc_stderr": 0.04229525846816505,
"acc_norm": 0.21,
"acc_norm_stderr": 0.040936018074033256
},
"mmlu:virology": {
"acc": 0.2469879518072289,
"acc_stderr": 0.03357351982064536,
"acc_norm": 0.3072289156626506,
"acc_norm_stderr": 0.03591566797824665
},
"mmlu:world_religions": {
"acc": 0.13450292397660818,
"acc_stderr": 0.026168221344662297,
"acc_norm": 0.2222222222222222,
"acc_norm_stderr": 0.03188578017686398
},
"openbookqa": {
"acc": 0.126,
"acc_stderr": 0.014855617750787533,
"acc_norm": 0.248,
"acc_norm_stderr": 0.019332342821239103
},
"piqa": {
"acc": 0.563,
"acc_stderr": 0.015693223928730377,
"acc_norm": 0.555,
"acc_norm_stderr": 0.015723301886760934
},
"siqa": {
"acc": 0.349,
"acc_stderr": 0.0150806639915631,
"acc_norm": 0.365,
"acc_norm_stderr": 0.015231776226264905
},
"winogrande": {
"acc": 0.491,
"acc_stderr": 0.015816736995005392,
"acc_norm": 0.491,
"acc_norm_stderr": 0.015816736995005392
},
"arc:_average": {
"acc": 0.2495,
"acc_stderr": 0.013436494747059113,
"acc_norm": 0.2875,
"acc_norm_stderr": 0.01420110564730553
},
"mmlu:_average": {
"acc": 0.23825706757304266,
"acc_stderr": 0.031647398412359425,
"acc_norm": 0.2518339195820701,
"acc_norm_stderr": 0.032370925761616196
}
}


# NOTE: from experiment 10
# NOTE: uniform here mean the proxy's reference model was trained using uniform weights
# DOREMI_1B_200K_WITH_200K_PROXY_UNIFORM = {}


# # NOTE: from experiment 11
# DOREMI_1B_200K_WITH_200K_PROXY_TOKEN_RATIO = {}


DOMAIN_NAMES = [
    "fineweb", "stack_full", "c4", "arxiv", "synthetic-data", "stack-pull-requests",
    "stack-jupyter-scripts", "stack-jupyter-structured", "open-web-math",
    "stack-issues", "stackoverflow", "wikipedia", "project-gutenberg", "deepmind-math",
    "stack-kaggle-scripts", "stack-documentation"
]

