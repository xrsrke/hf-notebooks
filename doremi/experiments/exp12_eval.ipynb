{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90a0458-161b-458e-8097-87682e001e43",
   "metadata": {},
   "source": [
    "### 1B at 110k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d7088a7-4e7f-418a-8513-be50dc7bcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/huggingface/DATA/notebooks/hf-notebooks/doremi/experiments/\")\n",
    "from utils import plot_domain_weights, print_name_weights, compute_eval_stats, plot_eval, compute_avg_eval\n",
    "from constants import REFERENCE_1B_110K_TOKEN_RATIO, REFERENCE_1B_200K_UNIFORM, REFERENCE_1B_200K_TOKEN_RATIO, REFERENCE_1B_200K_TOKEN_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e904fa-57c5-4a1b-a762-50fd90b03288",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOREMI_1B_110K = {\n",
    "    \"arc:challenge\": {\n",
    "\"acc\": 0.2,\n",
    "\"acc_stderr\": 0.012655439943366646,\n",
    "\"acc_norm\": 0.233,\n",
    "\"acc_norm_stderr\": 0.013374972519220067\n",
    "},\n",
    "\"arc:easy\": {\n",
    "\"acc\": 0.429,\n",
    "\"acc_stderr\": 0.015658997547870247,\n",
    "\"acc_norm\": 0.425,\n",
    "\"acc_norm_stderr\": 0.015640320317040105\n",
    "},\n",
    "\"commonsense_qa\": {\n",
    "\"acc\": 0.263,\n",
    "\"acc_stderr\": 0.013929286594259745,\n",
    "\"acc_norm\": 0.282,\n",
    "\"acc_norm_stderr\": 0.014236526215291331\n",
    "},\n",
    "\"hellaswag\": {\n",
    "\"acc\": 0.329,\n",
    "\"acc_stderr\": 0.014865395385928357,\n",
    "\"acc_norm\": 0.363,\n",
    "\"acc_norm_stderr\": 0.015213890444671287\n",
    "},\n",
    "\"mmlu:abstract_algebra\": {\n",
    "\"acc\": 0.24,\n",
    "\"acc_stderr\": 0.04292346959909283,\n",
    "\"acc_norm\": 0.25,\n",
    "\"acc_norm_stderr\": 0.04351941398892446\n",
    "},\n",
    "\"mmlu:anatomy\": {\n",
    "\"acc\": 0.2962962962962963,\n",
    "\"acc_stderr\": 0.03944624162501116,\n",
    "\"acc_norm\": 0.2814814814814815,\n",
    "\"acc_norm_stderr\": 0.03885004245800254\n",
    "},\n",
    "\"mmlu:astronomy\": {\n",
    "\"acc\": 0.21710526315789475,\n",
    "\"acc_stderr\": 0.03355045304882923,\n",
    "\"acc_norm\": 0.3026315789473684,\n",
    "\"acc_norm_stderr\": 0.037385206761196686\n",
    "},\n",
    "\"mmlu:business_ethics\": {\n",
    "\"acc\": 0.44,\n",
    "\"acc_stderr\": 0.04988876515698589,\n",
    "\"acc_norm\": 0.32,\n",
    "\"acc_norm_stderr\": 0.046882617226215034\n",
    "},\n",
    "\"mmlu:clinical_knowledge\": {\n",
    "\"acc\": 0.21132075471698114,\n",
    "\"acc_stderr\": 0.02512576648482784,\n",
    "\"acc_norm\": 0.3132075471698113,\n",
    "\"acc_norm_stderr\": 0.02854479331905533\n",
    "},\n",
    "\"mmlu:college_biology\": {\n",
    "\"acc\": 0.2986111111111111,\n",
    "\"acc_stderr\": 0.03827052357950756,\n",
    "\"acc_norm\": 0.2986111111111111,\n",
    "\"acc_norm_stderr\": 0.038270523579507554\n",
    "},\n",
    "\"mmlu:college_chemistry\": {\n",
    "\"acc\": 0.26,\n",
    "\"acc_stderr\": 0.044084400227680794,\n",
    "\"acc_norm\": 0.23,\n",
    "\"acc_norm_stderr\": 0.04229525846816506\n",
    "},\n",
    "\"mmlu:college_computer_science\": {\n",
    "\"acc\": 0.26,\n",
    "\"acc_stderr\": 0.0440844002276808,\n",
    "\"acc_norm\": 0.25,\n",
    "\"acc_norm_stderr\": 0.04351941398892446\n",
    "},\n",
    "\"mmlu:college_mathematics\": {\n",
    "\"acc\": 0.16,\n",
    "\"acc_stderr\": 0.03684529491774709,\n",
    "\"acc_norm\": 0.24,\n",
    "\"acc_norm_stderr\": 0.04292346959909282\n",
    "},\n",
    "\"mmlu:college_medicine\": {\n",
    "\"acc\": 0.23121387283236994,\n",
    "\"acc_stderr\": 0.03214737302029469,\n",
    "\"acc_norm\": 0.2254335260115607,\n",
    "\"acc_norm_stderr\": 0.03186209851641144\n",
    "},\n",
    "\"mmlu:college_physics\": {\n",
    "\"acc\": 0.1568627450980392,\n",
    "\"acc_stderr\": 0.03618664819936246,\n",
    "\"acc_norm\": 0.19607843137254902,\n",
    "\"acc_norm_stderr\": 0.03950581861179963\n",
    "},\n",
    "\"mmlu:computer_security\": {\n",
    "\"acc\": 0.19,\n",
    "\"acc_stderr\": 0.03942772444036625,\n",
    "\"acc_norm\": 0.27,\n",
    "\"acc_norm_stderr\": 0.044619604333847394\n",
    "},\n",
    "\"mmlu:conceptual_physics\": {\n",
    "\"acc\": 0.3276595744680851,\n",
    "\"acc_stderr\": 0.030683020843231,\n",
    "\"acc_norm\": 0.23829787234042554,\n",
    "\"acc_norm_stderr\": 0.02785125297388977\n",
    "},\n",
    "\"mmlu:econometrics\": {\n",
    "\"acc\": 0.21052631578947367,\n",
    "\"acc_stderr\": 0.038351539543994194,\n",
    "\"acc_norm\": 0.21052631578947367,\n",
    "\"acc_norm_stderr\": 0.03835153954399419\n",
    "},\n",
    "\"mmlu:electrical_engineering\": {\n",
    "\"acc\": 0.2896551724137931,\n",
    "\"acc_stderr\": 0.03780019230438014,\n",
    "\"acc_norm\": 0.3103448275862069,\n",
    "\"acc_norm_stderr\": 0.03855289616378948\n",
    "},\n",
    "\"mmlu:elementary_mathematics\": {\n",
    "\"acc\": 0.22486772486772486,\n",
    "\"acc_stderr\": 0.021502096078229147,\n",
    "\"acc_norm\": 0.23015873015873015,\n",
    "\"acc_norm_stderr\": 0.021679219663693152\n",
    "},\n",
    "\"mmlu:formal_logic\": {\n",
    "\"acc\": 0.31746031746031744,\n",
    "\"acc_stderr\": 0.041634530313028585,\n",
    "\"acc_norm\": 0.2619047619047619,\n",
    "\"acc_norm_stderr\": 0.03932537680392869\n",
    "},\n",
    "\"mmlu:global_facts\": {\n",
    "\"acc\": 0.15,\n",
    "\"acc_stderr\": 0.03588702812826372,\n",
    "\"acc_norm\": 0.17,\n",
    "\"acc_norm_stderr\": 0.03775251680686371\n",
    "},\n",
    "\"mmlu:high_school_biology\": {\n",
    "\"acc\": 0.25161290322580643,\n",
    "\"acc_stderr\": 0.024685979286239956,\n",
    "\"acc_norm\": 0.29354838709677417,\n",
    "\"acc_norm_stderr\": 0.02590608702131929\n",
    "},\n",
    "\"mmlu:high_school_chemistry\": {\n",
    "\"acc\": 0.18719211822660098,\n",
    "\"acc_stderr\": 0.027444924966882618,\n",
    "\"acc_norm\": 0.2512315270935961,\n",
    "\"acc_norm_stderr\": 0.030516530732694436\n",
    "},\n",
    "\"mmlu:high_school_computer_science\": {\n",
    "\"acc\": 0.21,\n",
    "\"acc_stderr\": 0.04093601807403326,\n",
    "\"acc_norm\": 0.3,\n",
    "\"acc_norm_stderr\": 0.046056618647183814\n",
    "},\n",
    "\"mmlu:high_school_european_history\": {\n",
    "\"acc\": 0.23030303030303031,\n",
    "\"acc_stderr\": 0.0328766675860349,\n",
    "\"acc_norm\": 0.28484848484848485,\n",
    "\"acc_norm_stderr\": 0.03524390844511783\n",
    "},\n",
    "\"mmlu:high_school_geography\": {\n",
    "\"acc\": 0.2727272727272727,\n",
    "\"acc_stderr\": 0.03173071239071724,\n",
    "\"acc_norm\": 0.3383838383838384,\n",
    "\"acc_norm_stderr\": 0.03371124142626303\n",
    "},\n",
    "\"mmlu:high_school_government_and_politics\": {\n",
    "\"acc\": 0.25906735751295334,\n",
    "\"acc_stderr\": 0.03161877917935411,\n",
    "\"acc_norm\": 0.30569948186528495,\n",
    "\"acc_norm_stderr\": 0.033248379397581594\n",
    "},\n",
    "\"mmlu:high_school_macroeconomics\": {\n",
    "\"acc\": 0.25384615384615383,\n",
    "\"acc_stderr\": 0.022066054378726257,\n",
    "\"acc_norm\": 0.26666666666666666,\n",
    "\"acc_norm_stderr\": 0.022421273612923717\n",
    "},\n",
    "\"mmlu:high_school_mathematics\": {\n",
    "\"acc\": 0.15925925925925927,\n",
    "\"acc_stderr\": 0.022310394630040618,\n",
    "\"acc_norm\": 0.2,\n",
    "\"acc_norm_stderr\": 0.02438843043398766\n",
    "},\n",
    "\"mmlu:high_school_microeconomics\": {\n",
    "\"acc\": 0.23949579831932774,\n",
    "\"acc_stderr\": 0.027722065493361276,\n",
    "\"acc_norm\": 0.31512605042016806,\n",
    "\"acc_norm_stderr\": 0.030176808288974337\n",
    "},\n",
    "\"mmlu:high_school_physics\": {\n",
    "\"acc\": 0.2251655629139073,\n",
    "\"acc_stderr\": 0.03410435282008936,\n",
    "\"acc_norm\": 0.2980132450331126,\n",
    "\"acc_norm_stderr\": 0.037345356767871984\n",
    "},\n",
    "\"mmlu:high_school_psychology\": {\n",
    "\"acc\": 0.3211009174311927,\n",
    "\"acc_stderr\": 0.020018149772733744,\n",
    "\"acc_norm\": 0.29541284403669726,\n",
    "\"acc_norm_stderr\": 0.019560619182976\n",
    "},\n",
    "\"mmlu:high_school_statistics\": {\n",
    "\"acc\": 0.2824074074074074,\n",
    "\"acc_stderr\": 0.030701372111510934,\n",
    "\"acc_norm\": 0.3101851851851852,\n",
    "\"acc_norm_stderr\": 0.03154696285656628\n",
    "},\n",
    "\"mmlu:high_school_us_history\": {\n",
    "\"acc\": 0.24509803921568626,\n",
    "\"acc_stderr\": 0.03019028245350194,\n",
    "\"acc_norm\": 0.29901960784313725,\n",
    "\"acc_norm_stderr\": 0.03213325717373616\n",
    "},\n",
    "\"mmlu:high_school_world_history\": {\n",
    "\"acc\": 0.21518987341772153,\n",
    "\"acc_stderr\": 0.02675082699467617,\n",
    "\"acc_norm\": 0.2911392405063291,\n",
    "\"acc_norm_stderr\": 0.029571601065753374\n",
    "},\n",
    "\"mmlu:human_aging\": {\n",
    "\"acc\": 0.31390134529147984,\n",
    "\"acc_stderr\": 0.031146796482972465,\n",
    "\"acc_norm\": 0.29596412556053814,\n",
    "\"acc_norm_stderr\": 0.03063659134869982\n",
    "},\n",
    "\"mmlu:human_sexuality\": {\n",
    "\"acc\": 0.3511450381679389,\n",
    "\"acc_stderr\": 0.04186445163013751,\n",
    "\"acc_norm\": 0.31297709923664124,\n",
    "\"acc_norm_stderr\": 0.04066962905677697\n",
    "},\n",
    "\"mmlu:international_law\": {\n",
    "\"acc\": 0.15702479338842976,\n",
    "\"acc_stderr\": 0.0332124484254713,\n",
    "\"acc_norm\": 0.2396694214876033,\n",
    "\"acc_norm_stderr\": 0.03896878985070416\n",
    "},\n",
    "\"mmlu:jurisprudence\": {\n",
    "\"acc\": 0.17592592592592593,\n",
    "\"acc_stderr\": 0.036809181416738807,\n",
    "\"acc_norm\": 0.26851851851851855,\n",
    "\"acc_norm_stderr\": 0.04284467968052191\n",
    "},\n",
    "\"mmlu:logical_fallacies\": {\n",
    "\"acc\": 0.22699386503067484,\n",
    "\"acc_stderr\": 0.032910995786157686,\n",
    "\"acc_norm\": 0.3374233128834356,\n",
    "\"acc_norm_stderr\": 0.037149084099355745\n",
    "},\n",
    "\"mmlu:machine_learning\": {\n",
    "\"acc\": 0.24107142857142858,\n",
    "\"acc_stderr\": 0.04059867246952687,\n",
    "\"acc_norm\": 0.23214285714285715,\n",
    "\"acc_norm_stderr\": 0.040073418097558065\n",
    "},\n",
    "\"mmlu:management\": {\n",
    "\"acc\": 0.2621359223300971,\n",
    "\"acc_stderr\": 0.043546310772605935,\n",
    "\"acc_norm\": 0.32038834951456313,\n",
    "\"acc_norm_stderr\": 0.0462028408228004\n",
    "},\n",
    "\"mmlu:marketing\": {\n",
    "\"acc\": 0.36752136752136755,\n",
    "\"acc_stderr\": 0.031585391577456365,\n",
    "\"acc_norm\": 0.38461538461538464,\n",
    "\"acc_norm_stderr\": 0.03187195347942466\n",
    "},\n",
    "\"mmlu:medical_genetics\": {\n",
    "\"acc\": 0.24,\n",
    "\"acc_stderr\": 0.042923469599092816,\n",
    "\"acc_norm\": 0.32,\n",
    "\"acc_norm_stderr\": 0.04688261722621504\n",
    "},\n",
    "\"mmlu:miscellaneous\": {\n",
    "\"acc\": 0.3282247765006386,\n",
    "\"acc_stderr\": 0.016791685640192892,\n",
    "\"acc_norm\": 0.2988505747126437,\n",
    "\"acc_norm_stderr\": 0.01636925681509311\n",
    "},\n",
    "\"mmlu:moral_disputes\": {\n",
    "\"acc\": 0.2254335260115607,\n",
    "\"acc_stderr\": 0.022497230190967544,\n",
    "\"acc_norm\": 0.19653179190751446,\n",
    "\"acc_norm_stderr\": 0.02139396140436385\n",
    "},\n",
    "\"mmlu:moral_scenarios\": {\n",
    "\"acc\": 0.23798882681564246,\n",
    "\"acc_stderr\": 0.014242630070574915,\n",
    "\"acc_norm\": 0.27262569832402234,\n",
    "\"acc_norm_stderr\": 0.014893391735249588\n",
    "},\n",
    "\"mmlu:nutrition\": {\n",
    "\"acc\": 0.21241830065359477,\n",
    "\"acc_stderr\": 0.023420375478296132,\n",
    "\"acc_norm\": 0.27450980392156865,\n",
    "\"acc_norm_stderr\": 0.025553169991826524\n",
    "},\n",
    "\"mmlu:philosophy\": {\n",
    "\"acc\": 0.2508038585209003,\n",
    "\"acc_stderr\": 0.024619771956697168,\n",
    "\"acc_norm\": 0.28938906752411575,\n",
    "\"acc_norm_stderr\": 0.02575586592263294\n",
    "},\n",
    "\"mmlu:prehistory\": {\n",
    "\"acc\": 0.2777777777777778,\n",
    "\"acc_stderr\": 0.024922001168886352,\n",
    "\"acc_norm\": 0.19444444444444445,\n",
    "\"acc_norm_stderr\": 0.022021366100220197\n",
    "},\n",
    "\"mmlu:professional_accounting\": {\n",
    "\"acc\": 0.25886524822695034,\n",
    "\"acc_stderr\": 0.02612957252718085,\n",
    "\"acc_norm\": 0.2375886524822695,\n",
    "\"acc_norm_stderr\": 0.02538951255272991\n",
    "},\n",
    "\"mmlu:professional_law\": {\n",
    "\"acc\": 0.242,\n",
    "\"acc_stderr\": 0.01355063170555594,\n",
    "\"acc_norm\": 0.26,\n",
    "\"acc_norm_stderr\": 0.013877773329774166\n",
    "},\n",
    "\"mmlu:professional_medicine\": {\n",
    "\"acc\": 0.2610294117647059,\n",
    "\"acc_stderr\": 0.02667925227010312,\n",
    "\"acc_norm\": 0.23897058823529413,\n",
    "\"acc_norm_stderr\": 0.02590528064489301\n",
    "},\n",
    "\"mmlu:professional_psychology\": {\n",
    "\"acc\": 0.2761437908496732,\n",
    "\"acc_stderr\": 0.018087276935663137,\n",
    "\"acc_norm\": 0.25163398692810457,\n",
    "\"acc_norm_stderr\": 0.017555818091322267\n",
    "},\n",
    "    \"mmlu:public_relations\": {\n",
    "\"acc\": 0.37272727272727274,\n",
    "\"acc_stderr\": 0.04631381319425463,\n",
    "\"acc_norm\": 0.24545454545454545,\n",
    "\"acc_norm_stderr\": 0.04122066502878284\n",
    "},\n",
    "\"mmlu:security_studies\": {\n",
    "\"acc\": 0.3142857142857143,\n",
    "\"acc_stderr\": 0.029719329422417465,\n",
    "\"acc_norm\": 0.2163265306122449,\n",
    "\"acc_norm_stderr\": 0.026358916334904035\n",
    "},\n",
    "\"mmlu:sociology\": {\n",
    "\"acc\": 0.23880597014925373,\n",
    "\"acc_stderr\": 0.030147775935409224,\n",
    "\"acc_norm\": 0.22388059701492538,\n",
    "\"acc_norm_stderr\": 0.0294752502360172\n",
    "},\n",
    "\"mmlu:us_foreign_policy\": {\n",
    "\"acc\": 0.26,\n",
    "\"acc_stderr\": 0.044084400227680794,\n",
    "\"acc_norm\": 0.23,\n",
    "\"acc_norm_stderr\": 0.04229525846816505\n",
    "},\n",
    "\"mmlu:virology\": {\n",
    "\"acc\": 0.2289156626506024,\n",
    "\"acc_stderr\": 0.03270745277352477,\n",
    "\"acc_norm\": 0.30120481927710846,\n",
    "\"acc_norm_stderr\": 0.0357160923005348\n",
    "},\n",
    "\"mmlu:world_religions\": {\n",
    "\"acc\": 0.16374269005847952,\n",
    "\"acc_stderr\": 0.028380919596145866,\n",
    "\"acc_norm\": 0.25146198830409355,\n",
    "\"acc_norm_stderr\": 0.033275044238468436\n",
    "},\n",
    "\"openbookqa\": {\n",
    "\"acc\": 0.152,\n",
    "\"acc_stderr\": 0.016071982367911724,\n",
    "\"acc_norm\": 0.284,\n",
    "\"acc_norm_stderr\": 0.02018670369357085\n",
    "},\n",
    "\"piqa\": {\n",
    "\"acc\": 0.643,\n",
    "\"acc_stderr\": 0.015158521721486762,\n",
    "\"acc_norm\": 0.664,\n",
    "\"acc_norm_stderr\": 0.01494414023379502\n",
    "},\n",
    "\"siqa\": {\n",
    "\"acc\": 0.382,\n",
    "\"acc_stderr\": 0.015372453034968522,\n",
    "\"acc_norm\": 0.396,\n",
    "\"acc_norm_stderr\": 0.015473313265859408\n",
    "},\n",
    "\"winogrande\": {\n",
    "\"acc\": 0.508,\n",
    "\"acc_stderr\": 0.01581727492920901,\n",
    "\"acc_norm\": 0.505,\n",
    "\"acc_norm_stderr\": 0.015818508944436656\n",
    "},\n",
    "\"arc:_average\": {\n",
    "\"acc\": 0.3145,\n",
    "\"acc_stderr\": 0.014157218745618446,\n",
    "\"acc_norm\": 0.329,\n",
    "\"acc_norm_stderr\": 0.014507646418130085\n",
    "},\n",
    "\"mmlu:_average\": {\n",
    "\"acc\": 0.2512092518462898,\n",
    "\"acc_stderr\": 0.032068208511054845,\n",
    "\"acc_norm\": 0.26916706843781046,\n",
    "\"acc_norm_stderr\": 0.033085515170478697\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "438805b3-332d-43aa-9cce-e3e3845cb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_DOREMI_1B_110K = compute_avg_eval(DOREMI_1B_110K)\n",
    "AVG_REFERENCE_1B_110K_TOKEN_RATIO = compute_avg_eval(REFERENCE_1B_110K_TOKEN_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca75f63-de09-4625-84b1-0053751d4a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tasks that DoReMi outperforms the reference model: 3 = 37.50%\n",
      "The number of tasks that DoReMi underperforms the reference model: 5 = 62.50%\n",
      "The number of tasks that DoReMi performs the same as the reference model: 0 = 0.00%\n",
      "On average, DoReMi outperforms the reference model per task by: 0.83%\n",
      "On average, DoReMi underperforms the reference model per task by: 0.23%\n",
      "-------- Average of all tasks (8 tasks) --------\n",
      "The average accuracy of DoReMi: 34.78%\n",
      "The average accuracy of the reference model: 34.61%\n",
      "-------- Eval Per Task (DoReMi vs Reference) --------\n",
      "mmlu:average: 25.12% vs 25.17%\n",
      "arc:average: 25.43% vs 25.52%\n",
      "commonsense_qa: 26.30% vs 26.50%\n",
      "hellaswag: 32.90% vs 32.20%\n",
      "openbookqa: 15.20% vs 14.00%\n",
      "piqa: 64.30% vs 64.90%\n",
      "siqa: 38.20% vs 38.40%\n",
      "winogrande: 50.80% vs 50.20%\n"
     ]
    }
   ],
   "source": [
    "compute_eval_stats(AVG_DOREMI_1B_110K, AVG_REFERENCE_1B_110K_TOKEN_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487049a-8ffc-4cbc-b6e9-edbee362b206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
