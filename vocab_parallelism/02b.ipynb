{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608d4e1-13e4-4fdd-996b-c59d5dce492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_parallel_1f1b(model, data_loader, tensor_shapes, device):\n",
    "    num_warmup_microbatches = min(process_group_manager.pp_world_size - process_group_manager.pp_rank - 1, data_loader.num_local_micro_batches)\n",
    "    num_microbatches_remaining = data_loader.num_local_micro_batches - num_warmup_microbatches\n",
    "    logging_loss, input_tensors, output_tensors  = 0.0, [], []\n",
    "    \n",
    "    def _forward_step(input_tensor):\n",
    "        batch = next(iter(data_loader))\n",
    "        batch[\"hidden_states\"] = input_tensor\n",
    "        output_tensor = model.forward(batch, device)\n",
    "        if process_group_manager.is_pipeline_last_stage:\n",
    "            output_tensor = F.cross_entropy(output_tensor.transpose(1, 2), batch[\"target_ids\"].to(device), reduction='mean')\n",
    "            nonlocal logging_loss\n",
    "            logging_loss += output_tensor.item()\n",
    "        return output_tensor\n",
    "\n",
    "    for _ in range(num_warmup_microbatches): # Warmup forward passes\n",
    "        input_tensor = communicate(shapes=tensor_shapes, dtype=torch.float32, operation='recv_forward')\n",
    "        output_tensor = _forward_step(input_tensor)\n",
    "        communicate(tensor=output_tensor, operation='send_forward')\n",
    "        input_tensors.append(input_tensor)\n",
    "        output_tensors.append(output_tensor)\n",
    "\n",
    "    if num_microbatches_remaining > 0:\n",
    "        input_tensor = communicate(shapes=tensor_shapes, dtype=torch.float32, operation='recv_forward')\n",
    "    \n",
    "    for i in range(num_microbatches_remaining):  # 1F1B steady state\n",
    "        output_tensor = _forward_step(input_tensor)\n",
    "        output_tensor_grad = bidirectional_communicate('send_fwd_recv_bwd', output_tensor, tensor_shapes, torch.float32, device)\n",
    "        input_tensors.append(input_tensor)\n",
    "        output_tensors.append(output_tensor)\n",
    "        input_tensor, output_tensor = input_tensors.pop(0), output_tensors.pop(0)\n",
    "        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)\n",
    "        if i == num_microbatches_remaining - 1: # last iteration\n",
    "            input_tensor = None\n",
    "            communicate(tensor=input_tensor_grad, operation='send_backward')\n",
    "        else:\n",
    "            input_tensor = bidirectional_communicate('send_bwd_recv_fwd', input_tensor_grad, tensor_shapes, torch.float32, device)\n",
    "\n",
    "    for _ in range(num_warmup_microbatches): # Cooldown backward passes\n",
    "        input_tensor, output_tensor = input_tensors.pop(0), output_tensors.pop(0)\n",
    "        output_tensor_grad = communicate(shapes=tensor_shapes, dtype=torch.float32, operation='recv_backward')\n",
    "        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)\n",
    "        communicate(tensor=input_tensor_grad, operation='send_backward')\n",
    "    return logging_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3167686-fa9b-4b10-9533-e03331712fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Timeline (4 stages, 8 microbatches):\n",
      "Time\tStage 0\tStage 1\tStage 2\tStage 3\n",
      "----------------------------------------------\n",
      "0\tF(1)\t-\t-\t-\n",
      "1\tF(2)\tF(1)\t-\t-\n",
      "2\tF(3)\tF(2)\tF(1)\t-\n",
      "3\tF(4)\tF(3)\tF(2)\tF(1)\n",
      "4\tF(5)\tF(4)\tF(3)\tF(2)\n",
      "5\tF(6)\tF(5)\tF(4)\tF(3)\n",
      "6\tF(7)\tF(6)\tF(5)\tF(4)\n",
      "7\tF(8)\tF(7)\tF(6)\tF(5)\n",
      "8\tB(2)\tF(8)\tF(7)\tF(6)\n",
      "9\tB(3)\tB(4)\tF(8)\tF(7)\n",
      "10\tB(4)\tB(5)\tB(6)\tF(8)\n",
      "11\tB(5)\tB(6)\tB(7)\tB(8)\n",
      "12\tB(6)\tB(7)\tB(8)\t-\n",
      "13\tB(7)\tB(8)\t-\t-\n",
      "14\tB(8)\t-\t-\t-\n",
      "15\t-\t-\t-\t-\n",
      "16\t-\t-\t-\t-\n",
      "17\t-\t-\t-\t-\n",
      "18\t-\t-\t-\t-\n",
      "19\t-\t-\t-\t-\n",
      "20\t-\t-\t-\t-\n",
      "21\t-\t-\t-\t-\n"
     ]
    }
   ],
   "source": [
    "def print_pipeline_timeline(S, M):\n",
    "    # Total steps for 1F1B schedule\n",
    "    T = 2 * M + 2 * (S - 1)\n",
    "    \n",
    "    # Prepare a matrix to hold the schedule\n",
    "    # schedule[time][stage] = string describing what's happening\n",
    "    schedule = [[\"-\" for _ in range(S)] for _ in range(T)]\n",
    "    \n",
    "    # Fill in forward (F) and backward (B) operations\n",
    "    for t in range(T):\n",
    "        for s in range(S):\n",
    "            # Check for forward pass:\n",
    "            # f = t - s + 1\n",
    "            f = t - s + 1\n",
    "            # Forward is valid if 1 <= f <= M and t <= M + S - 2\n",
    "            if 1 <= f <= M and t <= M + S - 2:\n",
    "                schedule[t][s] = f\"F({f})\"\n",
    "            else:\n",
    "                # Check for backward pass:\n",
    "                # Backward microbatch at stage s:\n",
    "                # Derived formula: t = 2S + b - s - 2  => b = t + s + 2 - 2S\n",
    "                b = t + s + 2 - 2 * S\n",
    "                # Backward starts after pipeline is initially filled, so t >= S\n",
    "                # Also need b in range [1..M]\n",
    "                if 1 <= b <= M and t >= S:\n",
    "                    schedule[t][s] = f\"B({b})\"\n",
    "                # else it remains \"-\"\n",
    "    \n",
    "    # Print the timeline\n",
    "    print(f\"Pipeline Timeline ({S} stages, {M} microbatches):\")\n",
    "    # Header\n",
    "    header = \"Time\\t\" + \"\\t\".join([f\"Stage {s}\" for s in range(S)])\n",
    "    print(header)\n",
    "    print(\"-\" * (len(header) + 10))\n",
    "    \n",
    "    for t in range(T):\n",
    "        row_str = f\"{t}\\t\" + \"\\t\".join(schedule[t])\n",
    "        print(row_str)\n",
    "\n",
    "# Example usage:\n",
    "# S = number of stages, M = number of microbatches\n",
    "# The example in the prompt mentioned 4 stages and 8 microbatches.\n",
    "print_pipeline_timeline(S=4, M=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d6a5f-5f5e-4512-8cdd-bf67f76a0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = PipelineTrainBatchState()\n",
    "\n",
    "        outputs = []\n",
    "        batch = iter(batch)\n",
    "\n",
    "        current_pp_rank = dist.get_rank(pg)\n",
    "\n",
    "        with attach_pipeline_state_to_model(model=model, pipeline_state=state):\n",
    "            # Init\n",
    "            for _ in range(pg.size() - current_pp_rank - 1):\n",
    "                micro_batch = next(batch)\n",
    "                context = self._get_fwd_context(model=model)\n",
    "                output = self.forward(context=context, state=state, micro_batch=micro_batch, model=model)\n",
    "\n",
    "                # TODO @thomasw21: Somehow this needs to be done somewhere else to support interleaving. Somewhere right after a \"stage\"\n",
    "                for _ in range(len(state.microbatches_activations_to_send)):\n",
    "                    send_activation = state.microbatches_activations_to_send.popleft()\n",
    "                    # Execute\n",
    "                    send_activation()\n",
    "\n",
    "                # We make `output` a dict\n",
    "                if not isinstance(output, dict):\n",
    "                    output = {\"loss\": output}\n",
    "\n",
    "                # Send tensors\n",
    "                # TODO @thomasw21: Somehow this needs to be done somewhere else to support interleaving. Somewhere right after a \"stage\"\n",
    "                for _ in range(len(state.microbatches_activations_to_send)):\n",
    "                    send_activation = state.microbatches_activations_to_send.popleft()\n",
    "                    # Execute\n",
    "                    send_activation()\n",
    "\n",
    "                # Store the loss for each microbatch\n",
    "                if not isinstance(output[\"loss\"], TensorPointer):\n",
    "                    output = {k: v.detach() for k, v in output.items()}\n",
    "                outputs.append(output)\n",
    "\n",
    "            for micro_batch in batch:\n",
    "                context = self._get_fwd_context(model=model)\n",
    "                output = self.forward(context=context, state=state, micro_batch=micro_batch, model=model)\n",
    "\n",
    "                # We make `output` a dict\n",
    "                if not isinstance(output, dict):\n",
    "                    output = {\"loss\": output}\n",
    "\n",
    "                # Store the loss for each microbatch\n",
    "                if not isinstance(output[\"loss\"], TensorPointer):\n",
    "                    output = {k: v.detach() for k, v in output.items()}\n",
    "                outputs.append(output)\n",
    "\n",
    "                # One backward\n",
    "                context = self._get_bwd_context(\n",
    "                    model=model,\n",
    "                    nb_backwards=state.nb_backwards,\n",
    "                    grad_accumulator=grad_accumulator,\n",
    "                )\n",
    "                self.backward(context=context, state=state, grad_accumulator=grad_accumulator)\n",
    "\n",
    "            # Check figure in paper: The remain blocks are all backward and there is only `pg.size() - current_pp_rank - 1` blocks left\n",
    "            assert len(state.microbatches_activations_requiring_backward) == pg.size() - current_pp_rank - 1\n",
    "            # No more activation to send/recv\n",
    "            assert (\n",
    "                len(state.microbatches_activations_to_send) == 0\n",
    "            ), f\"There are activations left for me to send still: {len(state.microbatches_activations_to_send)}\"\n",
    "            assert (\n",
    "                len(state.microbatches_activations_to_recv) == 0\n",
    "            ), f\"There are activations left for me to recv still: {len(state.microbatches_activations_to_recv)}\"\n",
    "\n",
    "            # Close: compute backward for the rest\n",
    "            # TODO @thomasw21: Somehow this needs to be done somewhere else to support interleaving. Somewhere right after a \"stage\"\n",
    "            for _ in range(len(state.microbatches_grads_to_send)):\n",
    "                send_grads = state.microbatches_grads_to_send.popleft()\n",
    "                # Execute\n",
    "                send_grads()\n",
    "            for _ in range(len(state.microbatches_activations_requiring_backward)):\n",
    "                context = self._get_bwd_context(\n",
    "                    model=model,\n",
    "                    nb_backwards=state.nb_backwards,\n",
    "                    grad_accumulator=grad_accumulator,\n",
    "                )\n",
    "                self.backward(context=context, state=state, grad_accumulator=grad_accumulator)\n",
    "\n",
    "                # TODO @thomasw21: Somehow this needs to be done somewhere else to support interleaving. Somewhere right after a \"stage\"\n",
    "                for _ in range(len(state.microbatches_grads_to_send)):\n",
    "                    send_grads = state.microbatches_grads_to_send.popleft()\n",
    "                    # Execute\n",
    "                    send_grads()\n",
    "\n",
    "            # Make sure that micro batches are all fully consumed\n",
    "            state.check_buffers_empty()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
