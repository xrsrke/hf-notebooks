{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e47cbb-4314-4afb-9630-30c1af8e9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    get_dataset_size_from_model_size, calculate_total_steps, calculate_total_flops, calculate_flops_per_step,\n",
    "    calculate_num_h100s_per_step, calculate_total_time_to_train_a_model,\n",
    "    compute_minimum_latency_between_clusters, calculate_total_minimum_comm_latency_to_train_a_model,\n",
    "    get_time_per_step, get_training_cost\n",
    ")\n",
    "from utils import (\n",
    "    convert_to_petaflops, convert_to_exaflops, convert_seconds_to_days,\n",
    "    convert_to_xt_format, convert_to_million_format, convert_to_billion_format, convert_seconds_to_years\n",
    ")\n",
    "from constants import UTILIZED_BFLOAT16_FLOPS, H100_COST_PER_HOUR, H100_COST_PER_GPU\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a590a17-2d8b-4534-ae6c-351c97b613fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_model_sizes = [70*10**9, 100*10**9, 500*10**9, 1000*10**9, 10000*10**9, 100000*10**9]\n",
    "target_model_sizes = [70*10**9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834849fb-cabb-45fd-9bdf-c66efdba4e7d",
   "metadata": {},
   "source": [
    "#### Critical batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b372b86-abea-4a51-a2b8-5c4a8ecb7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_critical_batch_size, convert_to_million_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59dde003-8f1c-485d-af88-5617b80cb34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_batch_sizes = [get_critical_batch_size(x) * 4096 for x in target_model_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35783b3-f5d9-45d3-8bff-8d12ab47e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gbs = pd.DataFrame({\n",
    "    'Model Size (B params)': [convert_to_xt_format(x) for x in target_model_sizes],\n",
    "    'Critical Batch Size (tokens)': [f'{x/1e6:1f}M' for x in critical_batch_sizes]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1430d89-1c0c-4a2d-97c6-2370e159545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (B params)</th>\n",
       "      <th>Critical Batch Size (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>4.218658M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (B params) Critical Batch Size (tokens)\n",
       "0                70.00B                    4.218658M"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11fc98-3b28-48b1-a575-45a71cf58a02",
   "metadata": {},
   "source": [
    "#### Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76e02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import CTX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "789c4b71-6311-421f-9da8-7c837b031e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100b to 100T\n",
    "# global_batch_sizes = [x*10**6 for x in [2, 10, 40]]\n",
    "global_batch_sizes = [x*10**6 for x in [1, 2, 4, 8, 16, 32]]\n",
    "global_batch_sizes.insert(0, CTX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e8c4bf-ee6b-4c0b-bb1e-5e04ce1e029f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8192, 1000000, 2000000, 4000000, 8000000, 16000000, 32000000]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f02f48e-3d76-4f34-9fd4-b06a038ddc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_step = 1 # the total time of a fwd, and bwd pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb228d04-838d-487d-ae32-99d0a9a76451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import VANILA_TRAINING_CONFIG, LLAMA3_70B_CONFIG, H100_MEMORY\n",
    "from copy import deepcopy\n",
    "from transformer_mem_functional import calculate_memory_requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1553384-116a-4d25-b733-6462692bf453",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_compute = []\n",
    "\n",
    "for is_train_for_20T in [False, True]:\n",
    "    for global_batch_size in global_batch_sizes:\n",
    "        data_compute = {\n",
    "            \"Model Size (Params)\": [],\n",
    "            \"Dataset Size (Tokens)\": [],\n",
    "            \"gbs\": [],\n",
    "            \"Total Steps\": [],\n",
    "            \"Total FLOPs\": [],\n",
    "            \"FLOPs per Step\": [],\n",
    "            \"H100 GPUs Needed\": [],\n",
    "            \"Total H100s cost\": [],\n",
    "            \"training_time_and_mem_with_0_grad_accum\": [],\n",
    "            \"training_time_and_mem_with_5_grad_accum\": [],\n",
    "            \"training_time_and_mem_with_10_grad_accum\": [],\n",
    "            \"training_time_and_mem_with_100_grad_accum\": [],\n",
    "            # \"Total Training Time with 1000 grad accum\": []\n",
    "        }\n",
    "        \n",
    "        for model_size in target_model_sizes:\n",
    "            if is_train_for_20T is True:\n",
    "                if model_size > 100*10**9:\n",
    "                    # NOTE: only check 20T tokens for model that is less than 10B\n",
    "                    continue\n",
    "                \n",
    "                dataset_size = 20000*10**9\n",
    "            else:\n",
    "                dataset_size = get_dataset_size_from_model_size(model_size)\n",
    "\n",
    "            def compute_training_memory_with_grad_accum(gbs, num_grad_accum):\n",
    "                training_config = deepcopy(VANILA_TRAINING_CONFIG)\n",
    "                training_config.batch_size_per_replicas = gbs // num_grad_accum\n",
    "                training_config.checkpoint_activations = True\n",
    "\n",
    "                training_memory = calculate_memory_requirements(\n",
    "                    transformer=LLAMA3_70B_CONFIG,\n",
    "                    config=training_config\n",
    "                )[\"total_training_mem\"][0]\n",
    "                return training_memory\n",
    "            \n",
    "            training_mem_with_0_grad_accum = compute_training_memory_with_grad_accum(global_batch_size, 1)\n",
    "            training_mem_with_5_grad_accum = compute_training_memory_with_grad_accum(global_batch_size, 5)\n",
    "            training_mem_with_10_grad_accum = compute_training_memory_with_grad_accum(global_batch_size, 10)\n",
    "            training_mem_with_100_grad_accum = compute_training_memory_with_grad_accum(global_batch_size, 100)\n",
    "\n",
    "            num_gpus_with_5_grad_accum = training_mem_with_5_grad_accum / H100_MEMORY\n",
    "            num_gpus_with_10_grad_accum = training_mem_with_10_grad_accum / H100_MEMORY\n",
    "            num_gpus_with_100_grad_accum = training_mem_with_100_grad_accum / H100_MEMORY\n",
    "                \n",
    "            total_steps = calculate_total_steps(model_size, dataset_size, global_batch_size)\n",
    "            total_flops = calculate_total_flops(model_size, dataset_size)\n",
    "            flops_per_step = calculate_flops_per_step(model_size, global_batch_size)\n",
    "            h100s_per_step = calculate_num_h100s_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "    \n",
    "            time_per_step = get_time_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "            total_time = calculate_total_time_to_train_a_model(model_size, dataset_size, global_batch_size, time_per_step)\n",
    "            total_training_cost = get_training_cost(model_size, dataset_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS, H100_COST_PER_HOUR)\n",
    "            \n",
    "            data_compute[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "            data_compute[\"Dataset Size (Tokens)\"].append(convert_to_xt_format(dataset_size))\n",
    "            data_compute[\"gbs\"].append(f'{global_batch_size/1e6}M')\n",
    "            data_compute[\"Total Steps\"].append(\"{:,}\".format(total_steps))\n",
    "            data_compute[\"Total FLOPs\"].append(convert_to_exaflops(total_flops))\n",
    "            data_compute[\"FLOPs per Step\"].append(convert_to_petaflops(flops_per_step))\n",
    "            data_compute[\"H100 GPUs Needed\"].append(h100s_per_step)\n",
    "            # data_compute[\"Total H100s cost\"].append(convert_to_billion_format(h100s_per_step * H100_COST_PER_GPU))\n",
    "            data_compute[\"Total H100s cost\"].append(convert_to_million_format(total_training_cost))\n",
    "            data_compute[\"training_time_and_mem_with_0_grad_accum\"].append(convert_seconds_to_days(total_time))\n",
    "            # data_compute[\"training_time_and_mem_with_5_grad_accum\"].append(f\"{convert_seconds_to_years(total_time*5)} | {h100s_per_step/5} gpus | {num_gpus_with_5_grad_accum} gpus\")\n",
    "            data_compute[\"training_time_and_mem_with_5_grad_accum\"].append(f\"{convert_seconds_to_years(total_time*5)} | {h100s_per_step/5} gpus\")\n",
    "            data_compute[\"training_time_and_mem_with_10_grad_accum\"].append(f\"{convert_seconds_to_years(total_time*10)} | {h100s_per_step/10} gpus\")\n",
    "            data_compute[\"training_time_and_mem_with_100_grad_accum\"].append(f\"{convert_seconds_to_years(total_time*100)} | {h100s_per_step/100} gpus\")\n",
    "            # data_compute[\"Total Training Time with 1000 grad accum\"].append(f\"{convert_seconds_to_years(total_time*1000)} - {h100s_per_step/1000} gpus\")\n",
    "        \n",
    "        df = pd.DataFrame(data_compute)\n",
    "        # Add batch size information\n",
    "        # df['Global Batch Size'] = f'{global_batch_size/1e6}M'\n",
    "        dataframes_compute.append(df)\n",
    "    \n",
    "    final_df_compute = pd.DataFrame()\n",
    "    for i, df in enumerate(dataframes_compute):\n",
    "        final_df_compute = pd.concat([final_df_compute, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6188ca7e-09c0-4dee-b71f-75c4d0f56f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Dataset Size (Tokens)</th>\n",
       "      <th>gbs</th>\n",
       "      <th>Total Steps</th>\n",
       "      <th>Total FLOPs</th>\n",
       "      <th>FLOPs per Step</th>\n",
       "      <th>H100 GPUs Needed</th>\n",
       "      <th>Total H100s cost</th>\n",
       "      <th>training_time_and_mem_with_0_grad_accum</th>\n",
       "      <th>training_time_and_mem_with_5_grad_accum</th>\n",
       "      <th>training_time_and_mem_with_10_grad_accum</th>\n",
       "      <th>training_time_and_mem_with_100_grad_accum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>1.4T</td>\n",
       "      <td>0.008192M</td>\n",
       "      <td>170,898,437</td>\n",
       "      <td>588,000.0 EFLOPs</td>\n",
       "      <td>3.44064 PFLOPs</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.8m</td>\n",
       "      <td>2150.4 days</td>\n",
       "      <td>29.44 years | 1.6 gpus</td>\n",
       "      <td>58.87 years | 0.8 gpus</td>\n",
       "      <td>588.74 years | 0.08 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>1.4T</td>\n",
       "      <td>1.0M</td>\n",
       "      <td>1,400,000</td>\n",
       "      <td>588,000.0 EFLOPs</td>\n",
       "      <td>420.0 PFLOPs</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>0.8m</td>\n",
       "      <td>16.2 days</td>\n",
       "      <td>0.22 years | 212.2 gpus</td>\n",
       "      <td>0.44 years | 106.1 gpus</td>\n",
       "      <td>4.44 years | 10.61 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>1.4T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>700,000</td>\n",
       "      <td>588,000.0 EFLOPs</td>\n",
       "      <td>840.0 PFLOPs</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>0.8m</td>\n",
       "      <td>8.1 days</td>\n",
       "      <td>0.11 years | 424.6 gpus</td>\n",
       "      <td>0.22 years | 212.3 gpus</td>\n",
       "      <td>2.22 years | 21.23 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>1.4T</td>\n",
       "      <td>4.0M</td>\n",
       "      <td>350,000</td>\n",
       "      <td>588,000.0 EFLOPs</td>\n",
       "      <td>1,680.0 PFLOPs</td>\n",
       "      <td>4246.0</td>\n",
       "      <td>0.8m</td>\n",
       "      <td>4.1 days</td>\n",
       "      <td>0.06 years | 849.2 gpus</td>\n",
       "      <td>0.11 years | 424.6 gpus</td>\n",
       "      <td>1.11 years | 42.46 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>1.4T</td>\n",
       "      <td>8.0M</td>\n",
       "      <td>175,000</td>\n",
       "      <td>588,000.0 EFLOPs</td>\n",
       "      <td>3,360.0 PFLOPs</td>\n",
       "      <td>8493.0</td>\n",
       "      <td>0.8m</td>\n",
       "      <td>2.0 days</td>\n",
       "      <td>0.03 years | 1698.6 gpus</td>\n",
       "      <td>0.06 years | 849.3 gpus</td>\n",
       "      <td>0.55 years | 84.93 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>1.4T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>87,500</td>\n",
       "      <td>588,000.0 EFLOPs</td>\n",
       "      <td>6,720.0 PFLOPs</td>\n",
       "      <td>16986.0</td>\n",
       "      <td>0.8m</td>\n",
       "      <td>1.0 days</td>\n",
       "      <td>0.01 years | 3397.2 gpus</td>\n",
       "      <td>0.03 years | 1698.6 gpus</td>\n",
       "      <td>0.28 years | 169.86 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>1.4T</td>\n",
       "      <td>32.0M</td>\n",
       "      <td>43,750</td>\n",
       "      <td>588,000.0 EFLOPs</td>\n",
       "      <td>13,440.0 PFLOPs</td>\n",
       "      <td>33973.0</td>\n",
       "      <td>0.8m</td>\n",
       "      <td>0.5 days</td>\n",
       "      <td>0.01 years | 6794.6 gpus</td>\n",
       "      <td>0.01 years | 3397.3 gpus</td>\n",
       "      <td>0.14 years | 339.73 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>0.008192M</td>\n",
       "      <td>2,441,406,250</td>\n",
       "      <td>8,400,000.0 EFLOPs</td>\n",
       "      <td>3.44064 PFLOPs</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.8m</td>\n",
       "      <td>30719.9 days</td>\n",
       "      <td>420.53 years | 1.6 gpus</td>\n",
       "      <td>841.06 years | 0.8 gpus</td>\n",
       "      <td>8410.64 years | 0.08 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>1.0M</td>\n",
       "      <td>20,000,000</td>\n",
       "      <td>8,400,000.0 EFLOPs</td>\n",
       "      <td>420.0 PFLOPs</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>11.8m</td>\n",
       "      <td>231.6 days</td>\n",
       "      <td>3.17 years | 212.2 gpus</td>\n",
       "      <td>6.34 years | 106.1 gpus</td>\n",
       "      <td>63.42 years | 10.61 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>10,000,000</td>\n",
       "      <td>8,400,000.0 EFLOPs</td>\n",
       "      <td>840.0 PFLOPs</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>11.8m</td>\n",
       "      <td>115.8 days</td>\n",
       "      <td>1.58 years | 424.6 gpus</td>\n",
       "      <td>3.17 years | 212.3 gpus</td>\n",
       "      <td>31.69 years | 21.23 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>4.0M</td>\n",
       "      <td>5,000,000</td>\n",
       "      <td>8,400,000.0 EFLOPs</td>\n",
       "      <td>1,680.0 PFLOPs</td>\n",
       "      <td>4246.0</td>\n",
       "      <td>11.8m</td>\n",
       "      <td>57.9 days</td>\n",
       "      <td>0.79 years | 849.2 gpus</td>\n",
       "      <td>1.58 years | 424.6 gpus</td>\n",
       "      <td>15.85 years | 42.46 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>8.0M</td>\n",
       "      <td>2,500,000</td>\n",
       "      <td>8,400,000.0 EFLOPs</td>\n",
       "      <td>3,360.0 PFLOPs</td>\n",
       "      <td>8493.0</td>\n",
       "      <td>11.8m</td>\n",
       "      <td>28.9 days</td>\n",
       "      <td>0.40 years | 1698.6 gpus</td>\n",
       "      <td>0.79 years | 849.3 gpus</td>\n",
       "      <td>7.92 years | 84.93 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>1,250,000</td>\n",
       "      <td>8,400,000.0 EFLOPs</td>\n",
       "      <td>6,720.0 PFLOPs</td>\n",
       "      <td>16986.0</td>\n",
       "      <td>11.8m</td>\n",
       "      <td>14.5 days</td>\n",
       "      <td>0.20 years | 3397.2 gpus</td>\n",
       "      <td>0.40 years | 1698.6 gpus</td>\n",
       "      <td>3.96 years | 169.86 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>32.0M</td>\n",
       "      <td>625,000</td>\n",
       "      <td>8,400,000.0 EFLOPs</td>\n",
       "      <td>13,440.0 PFLOPs</td>\n",
       "      <td>33973.0</td>\n",
       "      <td>11.8m</td>\n",
       "      <td>7.2 days</td>\n",
       "      <td>0.10 years | 6794.6 gpus</td>\n",
       "      <td>0.20 years | 3397.3 gpus</td>\n",
       "      <td>1.98 years | 339.73 gpus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (Params) Dataset Size (Tokens)        gbs    Total Steps  \\\n",
       "0              70.00B                  1.4T  0.008192M    170,898,437   \n",
       "0              70.00B                  1.4T       1.0M      1,400,000   \n",
       "0              70.00B                  1.4T       2.0M        700,000   \n",
       "0              70.00B                  1.4T       4.0M        350,000   \n",
       "0              70.00B                  1.4T       8.0M        175,000   \n",
       "0              70.00B                  1.4T      16.0M         87,500   \n",
       "0              70.00B                  1.4T      32.0M         43,750   \n",
       "0              70.00B                 20.0T  0.008192M  2,441,406,250   \n",
       "0              70.00B                 20.0T       1.0M     20,000,000   \n",
       "0              70.00B                 20.0T       2.0M     10,000,000   \n",
       "0              70.00B                 20.0T       4.0M      5,000,000   \n",
       "0              70.00B                 20.0T       8.0M      2,500,000   \n",
       "0              70.00B                 20.0T      16.0M      1,250,000   \n",
       "0              70.00B                 20.0T      32.0M        625,000   \n",
       "\n",
       "          Total FLOPs   FLOPs per Step  H100 GPUs Needed Total H100s cost  \\\n",
       "0    588,000.0 EFLOPs   3.44064 PFLOPs               8.0             0.8m   \n",
       "0    588,000.0 EFLOPs     420.0 PFLOPs            1061.0             0.8m   \n",
       "0    588,000.0 EFLOPs     840.0 PFLOPs            2123.0             0.8m   \n",
       "0    588,000.0 EFLOPs   1,680.0 PFLOPs            4246.0             0.8m   \n",
       "0    588,000.0 EFLOPs   3,360.0 PFLOPs            8493.0             0.8m   \n",
       "0    588,000.0 EFLOPs   6,720.0 PFLOPs           16986.0             0.8m   \n",
       "0    588,000.0 EFLOPs  13,440.0 PFLOPs           33973.0             0.8m   \n",
       "0  8,400,000.0 EFLOPs   3.44064 PFLOPs               8.0            11.8m   \n",
       "0  8,400,000.0 EFLOPs     420.0 PFLOPs            1061.0            11.8m   \n",
       "0  8,400,000.0 EFLOPs     840.0 PFLOPs            2123.0            11.8m   \n",
       "0  8,400,000.0 EFLOPs   1,680.0 PFLOPs            4246.0            11.8m   \n",
       "0  8,400,000.0 EFLOPs   3,360.0 PFLOPs            8493.0            11.8m   \n",
       "0  8,400,000.0 EFLOPs   6,720.0 PFLOPs           16986.0            11.8m   \n",
       "0  8,400,000.0 EFLOPs  13,440.0 PFLOPs           33973.0            11.8m   \n",
       "\n",
       "  training_time_and_mem_with_0_grad_accum  \\\n",
       "0                             2150.4 days   \n",
       "0                               16.2 days   \n",
       "0                                8.1 days   \n",
       "0                                4.1 days   \n",
       "0                                2.0 days   \n",
       "0                                1.0 days   \n",
       "0                                0.5 days   \n",
       "0                            30719.9 days   \n",
       "0                              231.6 days   \n",
       "0                              115.8 days   \n",
       "0                               57.9 days   \n",
       "0                               28.9 days   \n",
       "0                               14.5 days   \n",
       "0                                7.2 days   \n",
       "\n",
       "  training_time_and_mem_with_5_grad_accum  \\\n",
       "0                  29.44 years | 1.6 gpus   \n",
       "0                 0.22 years | 212.2 gpus   \n",
       "0                 0.11 years | 424.6 gpus   \n",
       "0                 0.06 years | 849.2 gpus   \n",
       "0                0.03 years | 1698.6 gpus   \n",
       "0                0.01 years | 3397.2 gpus   \n",
       "0                0.01 years | 6794.6 gpus   \n",
       "0                 420.53 years | 1.6 gpus   \n",
       "0                 3.17 years | 212.2 gpus   \n",
       "0                 1.58 years | 424.6 gpus   \n",
       "0                 0.79 years | 849.2 gpus   \n",
       "0                0.40 years | 1698.6 gpus   \n",
       "0                0.20 years | 3397.2 gpus   \n",
       "0                0.10 years | 6794.6 gpus   \n",
       "\n",
       "  training_time_and_mem_with_10_grad_accum  \\\n",
       "0                   58.87 years | 0.8 gpus   \n",
       "0                  0.44 years | 106.1 gpus   \n",
       "0                  0.22 years | 212.3 gpus   \n",
       "0                  0.11 years | 424.6 gpus   \n",
       "0                  0.06 years | 849.3 gpus   \n",
       "0                 0.03 years | 1698.6 gpus   \n",
       "0                 0.01 years | 3397.3 gpus   \n",
       "0                  841.06 years | 0.8 gpus   \n",
       "0                  6.34 years | 106.1 gpus   \n",
       "0                  3.17 years | 212.3 gpus   \n",
       "0                  1.58 years | 424.6 gpus   \n",
       "0                  0.79 years | 849.3 gpus   \n",
       "0                 0.40 years | 1698.6 gpus   \n",
       "0                 0.20 years | 3397.3 gpus   \n",
       "\n",
       "  training_time_and_mem_with_100_grad_accum  \n",
       "0                  588.74 years | 0.08 gpus  \n",
       "0                   4.44 years | 10.61 gpus  \n",
       "0                   2.22 years | 21.23 gpus  \n",
       "0                   1.11 years | 42.46 gpus  \n",
       "0                   0.55 years | 84.93 gpus  \n",
       "0                  0.28 years | 169.86 gpus  \n",
       "0                  0.14 years | 339.73 gpus  \n",
       "0                 8410.64 years | 0.08 gpus  \n",
       "0                  63.42 years | 10.61 gpus  \n",
       "0                  31.69 years | 21.23 gpus  \n",
       "0                  15.85 years | 42.46 gpus  \n",
       "0                   7.92 years | 84.93 gpus  \n",
       "0                  3.96 years | 169.86 gpus  \n",
       "0                  1.98 years | 339.73 gpus  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032de16-00f5-4881-8746-97ae7026ecda",
   "metadata": {},
   "source": [
    "##### Communication time of data parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54207cd-a0b5-4bed-aacd-ff3ee447f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import FP8_BYTES, BFLOAT16_BYTES\n",
    "from utils import convert_bytes_to_terabytes\n",
    "from utils import calculate_comm_time_given_comm_volume, convert_bytes_to_gigabytes\n",
    "from constants import NVLINK_MAX_TOTAL_BANDWIDTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7dd7e3-632b-44d1-bc85-e6ee59495f85",
   "metadata": {},
   "source": [
    "Assume that fwd+bwd pass of a single replicas takes 1 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b702221-3a61-4cf9-9470-9e74aed3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comm_bandwidths = [0.5*1024**3, 1*1024**3, 4*1024**3] # bytes/sec\n",
    "# comm_bandwidths = [40*1024**3, NVLINK_MAX_TOTAL_BANDWIDTH] # bytes/sec\n",
    "comm_bandwidths = [10*1024**3] # bytes/sec\n",
    "# divide by 10 data centers\n",
    "# comm_bandwidths = [(4/10)*1024**3] # bytes/sec\n",
    "# cluster_sizes = [1024, 10240, 102400]\n",
    "NUM_DATA_CENTERS = 10\n",
    "\n",
    "data_mem = {\n",
    "    \"Model Size (Params)\": [],\n",
    "    \"Global batch size\": [],\n",
    "    # \"Number of datacenters\": [],\n",
    "    \"Total bfloat16 gradient storage\": [],\n",
    "    \"Total fp8 gradient storage\": [],\n",
    "    \"Bandwidth\": [],\n",
    "    \"Total communication time in bfloat16 - comm/compute ratio\": [],\n",
    "    \"Total communication time in fp8 - comm/compute ratio\": [],\n",
    "    \"Total GPU idle cost for bfloat16 comm\": [],\n",
    "    \"Total GPU idle cost for fp8 comm\": [],\n",
    "    \"DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio\": []\n",
    "}\n",
    "# for cluster_size in cluster_sizes:\n",
    "for bandwidth in comm_bandwidths:\n",
    "    for global_batch_size in global_batch_sizes:\n",
    "        for model_size in target_model_sizes:\n",
    "            _ds_size = 20_000*10**9\n",
    "            # _ds_size = 100*10**9\n",
    "            total_steps = calculate_total_steps(model_size, dataset_size=_ds_size, global_batch_size=global_batch_size)\n",
    "            time_per_step = get_time_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "            total_time = calculate_total_time_to_train_a_model(model_size, _ds_size, global_batch_size, time_per_step)\n",
    "            \n",
    "            # h100s_per_step = calculate_num_h100s_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "            # num_clusters = h100s_per_step // cluster_size\n",
    "\n",
    "            bfloat16_grad_comm_volume = model_size * BFLOAT16_BYTES\n",
    "            fp8_grad_comm_volume = model_size * FP8_BYTES\n",
    "            \n",
    "            bfloat16_total_comm_time = calculate_comm_time_given_comm_volume(bfloat16_grad_comm_volume, bandwidth, NUM_DATA_CENTERS) * total_steps\n",
    "            fp8_total_comm_time = calculate_comm_time_given_comm_volume(fp8_grad_comm_volume, bandwidth, NUM_DATA_CENTERS) * total_steps\n",
    "            bfloat16_diloco_total_comm_time = calculate_comm_time_given_comm_volume(fp8_grad_comm_volume, bandwidth, NUM_DATA_CENTERS) * (total_steps / 500)\n",
    "\n",
    "            bfloat16_comm_compute_ratio = (bfloat16_total_comm_time / (total_time + bfloat16_total_comm_time)) * 100\n",
    "            fp8_comm_compute_ratio = (fp8_total_comm_time / (total_time + fp8_total_comm_time)) * 100\n",
    "            bfloat16_diloco_comm_compute_ratio = (bfloat16_diloco_total_comm_time / (total_time + bfloat16_diloco_total_comm_time)) * 100\n",
    "            \n",
    "            bfloat16_total_gpu_idle_cost_comm = ((bfloat16_total_comm_time * h100s_per_step) / (60*60)) / H100_COST_PER_HOUR\n",
    "            fp8_total_gpu_idle_cost_comm = ((fp8_total_comm_time * h100s_per_step) / (60*60)) / H100_COST_PER_HOUR\n",
    "            \n",
    "            data_mem[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "            data_mem[\"Global batch size\"].append(f'{global_batch_size/1e6}M')\n",
    "            # data_mem[\"Number of datacenters\"].append(num_clusters)\n",
    "            data_mem[\"Total bfloat16 gradient storage\"].append(convert_bytes_to_terabytes(model_size * BFLOAT16_BYTES))\n",
    "            data_mem[\"Total fp8 gradient storage\"].append(convert_bytes_to_terabytes(model_size * FP8_BYTES))\n",
    "            data_mem[\"Bandwidth\"].append(f\"{convert_bytes_to_gigabytes(bandwidth)}/s\")\n",
    "            data_mem[\"Total communication time in bfloat16 - comm/compute ratio\"].append(f\"{convert_seconds_to_days(bfloat16_total_comm_time)} / {convert_seconds_to_years(bfloat16_total_comm_time)} - {bfloat16_comm_compute_ratio:.2f}%\")\n",
    "            data_mem[\"Total communication time in fp8 - comm/compute ratio\"].append(f\"{convert_seconds_to_days(bfloat16_total_comm_time)} / {convert_seconds_to_years(fp8_total_comm_time)} - {fp8_comm_compute_ratio:.2f}%\")\n",
    "            data_mem[\"Total GPU idle cost for bfloat16 comm\"].append(convert_to_billion_format(bfloat16_total_gpu_idle_cost_comm))\n",
    "            data_mem[\"Total GPU idle cost for fp8 comm\"].append(convert_to_billion_format(fp8_total_gpu_idle_cost_comm))\n",
    "            if global_batch_size == 2*10**6:\n",
    "                assert 1 == 1\n",
    "\n",
    "            data_mem[\"DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio\"].append(f\"{convert_seconds_to_days(bfloat16_diloco_total_comm_time)}  - {bfloat16_diloco_comm_compute_ratio:.2f}%\")\n",
    "    \n",
    "    df_mem = pd.DataFrame(data_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b92fc56-67f2-4502-a4dd-36e6f4c612fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Global batch size</th>\n",
       "      <th>Total bfloat16 gradient storage</th>\n",
       "      <th>Total fp8 gradient storage</th>\n",
       "      <th>Bandwidth</th>\n",
       "      <th>Total communication time in bfloat16 - comm/compute ratio</th>\n",
       "      <th>Total communication time in fp8 - comm/compute ratio</th>\n",
       "      <th>Total GPU idle cost for bfloat16 comm</th>\n",
       "      <th>Total GPU idle cost for fp8 comm</th>\n",
       "      <th>DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>0.008192M</td>\n",
       "      <td>0.140 TB</td>\n",
       "      <td>0.070 TB</td>\n",
       "      <td>10.737 GB/s</td>\n",
       "      <td>3684295.7 days / 10087.05 years - 99.17%</td>\n",
       "      <td>3684295.7 days / 5043.53 years - 98.36%</td>\n",
       "      <td>1,502.00B</td>\n",
       "      <td>751.00B</td>\n",
       "      <td>3684.3 days  - 10.71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>1.0M</td>\n",
       "      <td>0.140 TB</td>\n",
       "      <td>0.070 TB</td>\n",
       "      <td>10.737 GB/s</td>\n",
       "      <td>30181.8 days / 82.63 years - 99.24%</td>\n",
       "      <td>30181.8 days / 41.32 years - 98.49%</td>\n",
       "      <td>12.30B</td>\n",
       "      <td>6.15B</td>\n",
       "      <td>30.2 days  - 11.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>0.140 TB</td>\n",
       "      <td>0.070 TB</td>\n",
       "      <td>10.737 GB/s</td>\n",
       "      <td>15090.9 days / 41.32 years - 99.24%</td>\n",
       "      <td>15090.9 days / 20.66 years - 98.49%</td>\n",
       "      <td>6.15B</td>\n",
       "      <td>3.08B</td>\n",
       "      <td>15.1 days  - 11.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>4.0M</td>\n",
       "      <td>0.140 TB</td>\n",
       "      <td>0.070 TB</td>\n",
       "      <td>10.737 GB/s</td>\n",
       "      <td>7545.4 days / 20.66 years - 99.24%</td>\n",
       "      <td>7545.4 days / 10.33 years - 98.49%</td>\n",
       "      <td>3.08B</td>\n",
       "      <td>1.54B</td>\n",
       "      <td>7.5 days  - 11.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>8.0M</td>\n",
       "      <td>0.140 TB</td>\n",
       "      <td>0.070 TB</td>\n",
       "      <td>10.737 GB/s</td>\n",
       "      <td>3772.7 days / 10.33 years - 99.24%</td>\n",
       "      <td>3772.7 days / 5.16 years - 98.49%</td>\n",
       "      <td>1.54B</td>\n",
       "      <td>0.77B</td>\n",
       "      <td>3.8 days  - 11.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>0.140 TB</td>\n",
       "      <td>0.070 TB</td>\n",
       "      <td>10.737 GB/s</td>\n",
       "      <td>1886.4 days / 5.16 years - 99.24%</td>\n",
       "      <td>1886.4 days / 2.58 years - 98.49%</td>\n",
       "      <td>0.77B</td>\n",
       "      <td>0.38B</td>\n",
       "      <td>1.9 days  - 11.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70.00B</td>\n",
       "      <td>32.0M</td>\n",
       "      <td>0.140 TB</td>\n",
       "      <td>0.070 TB</td>\n",
       "      <td>10.737 GB/s</td>\n",
       "      <td>943.2 days / 2.58 years - 99.24%</td>\n",
       "      <td>943.2 days / 1.29 years - 98.49%</td>\n",
       "      <td>0.38B</td>\n",
       "      <td>0.19B</td>\n",
       "      <td>0.9 days  - 11.53%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (Params) Global batch size Total bfloat16 gradient storage  \\\n",
       "0              70.00B         0.008192M                        0.140 TB   \n",
       "1              70.00B              1.0M                        0.140 TB   \n",
       "2              70.00B              2.0M                        0.140 TB   \n",
       "3              70.00B              4.0M                        0.140 TB   \n",
       "4              70.00B              8.0M                        0.140 TB   \n",
       "5              70.00B             16.0M                        0.140 TB   \n",
       "6              70.00B             32.0M                        0.140 TB   \n",
       "\n",
       "  Total fp8 gradient storage    Bandwidth  \\\n",
       "0                   0.070 TB  10.737 GB/s   \n",
       "1                   0.070 TB  10.737 GB/s   \n",
       "2                   0.070 TB  10.737 GB/s   \n",
       "3                   0.070 TB  10.737 GB/s   \n",
       "4                   0.070 TB  10.737 GB/s   \n",
       "5                   0.070 TB  10.737 GB/s   \n",
       "6                   0.070 TB  10.737 GB/s   \n",
       "\n",
       "  Total communication time in bfloat16 - comm/compute ratio  \\\n",
       "0           3684295.7 days / 10087.05 years - 99.17%          \n",
       "1                30181.8 days / 82.63 years - 99.24%          \n",
       "2                15090.9 days / 41.32 years - 99.24%          \n",
       "3                 7545.4 days / 20.66 years - 99.24%          \n",
       "4                 3772.7 days / 10.33 years - 99.24%          \n",
       "5                  1886.4 days / 5.16 years - 99.24%          \n",
       "6                   943.2 days / 2.58 years - 99.24%          \n",
       "\n",
       "  Total communication time in fp8 - comm/compute ratio  \\\n",
       "0            3684295.7 days / 5043.53 years - 98.36%     \n",
       "1                30181.8 days / 41.32 years - 98.49%     \n",
       "2                15090.9 days / 20.66 years - 98.49%     \n",
       "3                 7545.4 days / 10.33 years - 98.49%     \n",
       "4                  3772.7 days / 5.16 years - 98.49%     \n",
       "5                  1886.4 days / 2.58 years - 98.49%     \n",
       "6                   943.2 days / 1.29 years - 98.49%     \n",
       "\n",
       "  Total GPU idle cost for bfloat16 comm Total GPU idle cost for fp8 comm  \\\n",
       "0                             1,502.00B                          751.00B   \n",
       "1                                12.30B                            6.15B   \n",
       "2                                 6.15B                            3.08B   \n",
       "3                                 3.08B                            1.54B   \n",
       "4                                 1.54B                            0.77B   \n",
       "5                                 0.77B                            0.38B   \n",
       "6                                 0.38B                            0.19B   \n",
       "\n",
       "  DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio  \n",
       "0                              3684.3 days  - 10.71%                                    \n",
       "1                                30.2 days  - 11.53%                                    \n",
       "2                                15.1 days  - 11.53%                                    \n",
       "3                                 7.5 days  - 11.53%                                    \n",
       "4                                 3.8 days  - 11.53%                                    \n",
       "5                                 1.9 days  - 11.53%                                    \n",
       "6                                 0.9 days  - 11.53%                                    "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97153b-7c80-45de-bffc-31dd0caa676d",
   "metadata": {},
   "source": [
    "#### Communication latency (theoretical minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ba983-2882-48d4-9541-1bd8faabfddb",
   "metadata": {},
   "source": [
    "Assumptions on communication\n",
    "- No limit on banwidth\n",
    "- Achieve speed of light\n",
    "- Clostest surface distance between two points on the earth surface (assume you don't dig a crazy hole to go a straight line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24b40c02-2ac3-4dce-947e-a4915b7518f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_latency_between_jz_and_jc = compute_minimum_latency_between_clusters(\"JEAN_ZAY\", \"JOLIOT_CURIE\")\n",
    "minimum_latency_between_jz_and_ec = compute_minimum_latency_between_clusters(\"JEAN_ZAY\", \"EL_CAPITAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ba98b7a-e713-446d-9dad-613b76d447e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "calculate_total_steps() missing 1 required positional argument: 'global_batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m data_comm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Size (Tokens)\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(convert_to_xt_format(dataset_size))\n\u001b[1;32m     23\u001b[0m data_comm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal Batch Size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_batch_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m data_comm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal minimum communication latency between JZ and JC\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcalculate_total_minimum_comm_latency_to_train_a_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminimum_latency_between_jz_and_jc\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m data_comm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal GPU idle time for minimum comm between JZ and JC\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(convert_seconds_to_days(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc) \u001b[38;5;241m*\u001b[39m h100s_per_step))\n\u001b[1;32m     27\u001b[0m data_comm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU idle cost during JZ-JC minimum communication latency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(convert_to_million_format((calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)) \u001b[38;5;241m*\u001b[39m h100s_per_step \u001b[38;5;241m*\u001b[39m H100_COST_PER_HOUR))\n",
      "File \u001b[0;32m~/DATA/hf-notebooks/diloco/utils.py:159\u001b[0m, in \u001b[0;36mcalculate_total_minimum_comm_latency_to_train_a_model\u001b[0;34m(model_size, global_batch_size, minimum_latency)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_total_minimum_comm_latency_to_train_a_model\u001b[39m(model_size, global_batch_size, minimum_latency):\n\u001b[0;32m--> 159\u001b[0m     total_steps \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_total_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_steps \u001b[38;5;241m*\u001b[39m minimum_latency\n",
      "\u001b[0;31mTypeError\u001b[0m: calculate_total_steps() missing 1 required positional argument: 'global_batch_size'"
     ]
    }
   ],
   "source": [
    "dataframes_comm = []\n",
    "\n",
    "for global_batch_size in global_batch_sizes:\n",
    "    data_comm = {\n",
    "        \"Model Size (Params)\": [],\n",
    "        \"Dataset Size (Tokens)\": [],\n",
    "        \"Global Batch Size\": [],\n",
    "        \n",
    "        \"Total minimum communication latency between JZ and JC\": [],\n",
    "        \"Total GPU idle time for minimum comm between JZ and JC\": [],\n",
    "        \"GPU idle cost during JZ-JC minimum communication latency\": [],\n",
    "        \n",
    "        \"Total minimum communication latency between JZ and EC\": [],\n",
    "        \"Total GPU idle time for minimum comm between JZ and EC\": [],\n",
    "        \"GPU idle cost during JZ-EC minimum communication latency\": [],\n",
    "    }\n",
    "    \n",
    "    for model_size in target_model_sizes:\n",
    "        dataset_size = get_dataset_size_from_model_size(model_size)\n",
    "        # Append to dictionary\n",
    "        data_comm[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "        data_comm[\"Dataset Size (Tokens)\"].append(convert_to_xt_format(dataset_size))\n",
    "        data_comm[\"Global Batch Size\"].append(f'{global_batch_size/1e6}M')\n",
    "        \n",
    "        data_comm[\"Total minimum communication latency between JZ and JC\"].append(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc))\n",
    "        data_comm[\"Total GPU idle time for minimum comm between JZ and JC\"].append(convert_seconds_to_days(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc) * h100s_per_step))\n",
    "        data_comm[\"GPU idle cost during JZ-JC minimum communication latency\"].append(convert_to_million_format((calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc) / (60 * 60)) * h100s_per_step * H100_COST_PER_HOUR))\n",
    "        \n",
    "        data_comm[\"Total minimum communication latency between JZ and EC\"].append(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_ec))\n",
    "        data_comm[\"Total GPU idle time for minimum comm between JZ and EC\"].append(convert_seconds_to_years(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_ec) * h100s_per_step))\n",
    "        data_comm[\"GPU idle cost during JZ-EC minimum communication latency\"].append(convert_to_million_format((calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_ec) / (60 * 60)) * h100s_per_step * H100_COST_PER_HOUR))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_comm = pd.DataFrame(data_comm)\n",
    "    # df_comm['Global Batch Size'] = f'{global_batch_size/1e6}M'\n",
    "    dataframes_comm.append(df_comm)\n",
    "\n",
    "final_df_comm = pd.DataFrame()\n",
    "for i, df in enumerate(dataframes_comm):\n",
    "    final_df_comm = pd.concat([final_df_comm, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e897a27-98f8-43a8-aec6-55310d7da1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a547c9e-def0-4c82-a541-517ec6d127c6",
   "metadata": {},
   "source": [
    "#### Electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2735f7c7-3210-414b-9590-b2c3a0620355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import TOTAL_H100_WATT\n",
    "from utils import convert_watts_to_megawatts, convert_watts_to_terawatts, calculate_electricity_consumption_of_an_h100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8857f303-4c68-4db4-9912-9f1fdd3afa8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataframes_elec = []\n",
    "\n",
    "for global_batch_size in global_batch_sizes:\n",
    "    data_elec = {\n",
    "        \"Model Size (Params)\": [],\n",
    "        \"Dataset Size (Tokens)\": [],\n",
    "        \"Global batch size\": [],\n",
    "        \"Number of GPUs\": [],\n",
    "        \"Total electricity per step (without grad accum)\": [],\n",
    "        \"Total electricity for the entire training (without grad accum)\": []\n",
    "    }\n",
    "    \n",
    "    for model_size in target_model_sizes:\n",
    "        dataset_size = get_dataset_size_from_model_size(model_size)\n",
    "        h100s_per_step = calculate_num_h100s_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "        time_per_step = get_time_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "        total_time = calculate_total_time_to_train_a_model(model_size, global_batch_size, time_per_step)\n",
    "        total_electricity_consumption = calculate_electricity_consumption_of_an_h100(TOTAL_H100_WATT, total_time) * h100s_per_step\n",
    "        \n",
    "        data_elec[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "        data_elec[\"Dataset Size (Tokens)\"].append(convert_to_xt_format(dataset_size))\n",
    "        data_elec[\"Global batch size\"].append(f'{global_batch_size/1e6}M')\n",
    "        data_elec[\"Number of GPUs\"].append(h100s_per_step)\n",
    "        data_elec[\"Total electricity per step (without grad accum)\"].append(convert_watts_to_megawatts(h100s_per_step * TOTAL_H100_WATT))\n",
    "        data_elec[\"Total electricity for the entire training (without grad accum)\"].append(f\"{convert_watts_to_terawatts(total_electricity_consumption)}\")\n",
    "    \n",
    "    df = pd.DataFrame(data_elec)\n",
    "    # df['Global Batch Size'] = f'{global_batch_size/1e6}M'\n",
    "    dataframes_elec.append(df)\n",
    "\n",
    "final_df_elec = pd.DataFrame()\n",
    "for i, df in enumerate(dataframes_elec):\n",
    "    final_df_elec = pd.concat([final_df_elec, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96921a0d-6fc4-471b-a0d7-47e1c5ecd808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my calculation closes to 100k gpu cluster's electricity: https://semianalysis.com/2024/06/17/100000-h100-clusters-power-network/#power-challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8152cae-4337-4743-a9ba-e9fe58fa946a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e422830-704f-4af3-9c81-3b6ee3a8eb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c797a-2914-4eb3-bdae-ce7f0026be1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e1305-4452-4fd0-8f4a-b8d8d860058e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aaf18c-6c78-4d21-82c5-15e55063b1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
