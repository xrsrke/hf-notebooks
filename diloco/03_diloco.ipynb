{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e47cbb-4314-4afb-9630-30c1af8e9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    get_dataset_size_from_model_size, calculate_total_steps, calculate_total_flops, calculate_flops_per_step,\n",
    "    calculate_num_h100s_per_step, calculate_total_time_to_train_a_model,\n",
    "    compute_minimum_latency_between_clusters, calculate_total_minimum_comm_latency_to_train_a_model\n",
    ")\n",
    "from utils import (\n",
    "    convert_to_petaflops, convert_to_exaflops, convert_seconds_to_days,\n",
    "    convert_to_xt_format, convert_to_million_format, convert_to_billion_format, convert_seconds_to_years\n",
    ")\n",
    "from constants import UTILIZED_BFLOAT16_FLOPS, H100_COST_PER_HOUR, H100_COST_PER_GPU\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11fc98-3b28-48b1-a575-45a71cf58a02",
   "metadata": {},
   "source": [
    "#### Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd23e6e-342a-4bb9-9df5-8eae0a1925b7",
   "metadata": {},
   "source": [
    "Assumption: one fwd+bwd pass takes a second (if it take x more seconds, then you scale the numbers linearly), 20k per h100 (wholesale price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789c4b71-6311-421f-9da8-7c837b031e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100b to 100T\n",
    "target_model_sizes = [100*10**9, 500*10**9, 1000*10**9, 10000*10**9, 100000*10**9]\n",
    "global_batch_sizes = [x*10**6 for x in [2, 16, 40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f02f48e-3d76-4f34-9fd4-b06a038ddc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_step = 1 # the total time of a fwd, and bwd pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1553384-116a-4d25-b733-6462692bf453",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_compute = []\n",
    "\n",
    "for global_batch_size in global_batch_sizes:\n",
    "    data_compute = {\n",
    "        \"Model Size (Params)\": [],\n",
    "        \"Dataset Size (Tokens)\": [],\n",
    "        \"Global Batch Size\": [],\n",
    "        \"Total Steps\": [],\n",
    "        \"Total FLOPs\": [],\n",
    "        \"FLOPs per Step\": [],\n",
    "        \"H100 GPUs Needed\": [],\n",
    "        \"Total H100s cost\": [],\n",
    "        \"Total Training Time without grad accum\": [],\n",
    "        \"Total Training Time with 10 grad accum\": [],\n",
    "        \"Total Training Time with 100 grad accum\": [],\n",
    "        \"Total Training Time with 1000 grad accum\": []\n",
    "    }\n",
    "    \n",
    "    for model_size in target_model_sizes:\n",
    "        dataset_size = get_dataset_size_from_model_size(model_size)\n",
    "        total_steps = calculate_total_steps(model_size, global_batch_size)\n",
    "        total_flops = calculate_total_flops(model_size)\n",
    "        flops_per_step = calculate_flops_per_step(model_size, global_batch_size)\n",
    "        h100s_per_step = calculate_num_h100s_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "        total_time = calculate_total_time_to_train_a_model(model_size, global_batch_size, time_per_step)\n",
    "        \n",
    "        data_compute[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "        data_compute[\"Dataset Size (Tokens)\"].append(convert_to_xt_format(dataset_size))\n",
    "        data_compute[\"Global Batch Size\"].append(f'{global_batch_size/1e6}M')\n",
    "        data_compute[\"Total Steps\"].append(\"{:,}\".format(total_steps))\n",
    "        data_compute[\"Total FLOPs\"].append(convert_to_exaflops(total_flops))\n",
    "        data_compute[\"FLOPs per Step\"].append(convert_to_petaflops(flops_per_step))\n",
    "        data_compute[\"H100 GPUs Needed\"].append(h100s_per_step)\n",
    "        data_compute[\"Total H100s cost\"].append(convert_to_billion_format(h100s_per_step * H100_COST_PER_GPU))\n",
    "        data_compute[\"Total Training Time without grad accum\"].append(convert_seconds_to_days(total_time))\n",
    "        data_compute[\"Total Training Time with 10 grad accum\"].append(f\"{convert_seconds_to_years(total_time*10)} - {h100s_per_step/10} gpus\")\n",
    "        data_compute[\"Total Training Time with 100 grad accum\"].append(f\"{convert_seconds_to_years(total_time*100)} - {h100s_per_step/100} gpus\")\n",
    "        data_compute[\"Total Training Time with 1000 grad accum\"].append(f\"{convert_seconds_to_years(total_time*1000)} - {h100s_per_step/1000} gpus\")\n",
    "    \n",
    "    df = pd.DataFrame(data_compute)\n",
    "    # Add batch size information\n",
    "    # df['Global Batch Size'] = f'{global_batch_size/1e6}M'\n",
    "    dataframes_compute.append(df)\n",
    "\n",
    "final_df_compute = pd.DataFrame()\n",
    "for i, df in enumerate(dataframes_compute):\n",
    "    final_df_compute = pd.concat([final_df_compute, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6188ca7e-09c0-4dee-b71f-75c4d0f56f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Dataset Size (Tokens)</th>\n",
       "      <th>Global Batch Size</th>\n",
       "      <th>Total Steps</th>\n",
       "      <th>Total FLOPs</th>\n",
       "      <th>FLOPs per Step</th>\n",
       "      <th>H100 GPUs Needed</th>\n",
       "      <th>Total H100s cost</th>\n",
       "      <th>Total Training Time without grad accum</th>\n",
       "      <th>Total Training Time with 10 grad accum</th>\n",
       "      <th>Total Training Time with 100 grad accum</th>\n",
       "      <th>Total Training Time with 1000 grad accum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1,000,000</td>\n",
       "      <td>1,200,000.0 EFLOPs</td>\n",
       "      <td>1,200.0 PFLOPs</td>\n",
       "      <td>2206.0</td>\n",
       "      <td>0.07B</td>\n",
       "      <td>11.6 days</td>\n",
       "      <td>0.3 years - 220.6 gpus</td>\n",
       "      <td>3.2 years - 22.06 gpus</td>\n",
       "      <td>31.7 years - 2.206 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>5,000,000</td>\n",
       "      <td>30,000,000.0 EFLOPs</td>\n",
       "      <td>6,000.0 PFLOPs</td>\n",
       "      <td>11030.0</td>\n",
       "      <td>0.33B</td>\n",
       "      <td>57.9 days</td>\n",
       "      <td>1.6 years - 1103.0 gpus</td>\n",
       "      <td>15.8 years - 110.3 gpus</td>\n",
       "      <td>158.4 years - 11.03 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>10,000,000</td>\n",
       "      <td>120,000,000.0 EFLOPs</td>\n",
       "      <td>12,000.0 PFLOPs</td>\n",
       "      <td>22060.0</td>\n",
       "      <td>0.66B</td>\n",
       "      <td>115.7 days</td>\n",
       "      <td>3.2 years - 2206.0 gpus</td>\n",
       "      <td>31.7 years - 220.6 gpus</td>\n",
       "      <td>316.9 years - 22.06 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>100,000,000</td>\n",
       "      <td>12,000,000,000.0 EFLOPs</td>\n",
       "      <td>120,000.0 PFLOPs</td>\n",
       "      <td>220608.0</td>\n",
       "      <td>6.62B</td>\n",
       "      <td>1157.4 days</td>\n",
       "      <td>31.7 years - 22060.8 gpus</td>\n",
       "      <td>316.9 years - 2206.08 gpus</td>\n",
       "      <td>3168.8 years - 220.608 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1,000,000,000</td>\n",
       "      <td>1,200,000,000,000.0 EFLOPs</td>\n",
       "      <td>1,200,000.0 PFLOPs</td>\n",
       "      <td>2206085.0</td>\n",
       "      <td>66.18B</td>\n",
       "      <td>11574.1 days</td>\n",
       "      <td>316.9 years - 220608.5 gpus</td>\n",
       "      <td>3168.8 years - 22060.85 gpus</td>\n",
       "      <td>31688.1 years - 2206.085 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>125,000</td>\n",
       "      <td>1,200,000.0 EFLOPs</td>\n",
       "      <td>9,600.0 PFLOPs</td>\n",
       "      <td>17648.0</td>\n",
       "      <td>0.53B</td>\n",
       "      <td>1.4 days</td>\n",
       "      <td>0.0 years - 1764.8 gpus</td>\n",
       "      <td>0.4 years - 176.48 gpus</td>\n",
       "      <td>4.0 years - 17.648 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>625,000</td>\n",
       "      <td>30,000,000.0 EFLOPs</td>\n",
       "      <td>48,000.0 PFLOPs</td>\n",
       "      <td>88243.0</td>\n",
       "      <td>2.65B</td>\n",
       "      <td>7.2 days</td>\n",
       "      <td>0.2 years - 8824.3 gpus</td>\n",
       "      <td>2.0 years - 882.43 gpus</td>\n",
       "      <td>19.8 years - 88.243 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>1,250,000</td>\n",
       "      <td>120,000,000.0 EFLOPs</td>\n",
       "      <td>96,000.0 PFLOPs</td>\n",
       "      <td>176486.0</td>\n",
       "      <td>5.29B</td>\n",
       "      <td>14.5 days</td>\n",
       "      <td>0.4 years - 17648.6 gpus</td>\n",
       "      <td>4.0 years - 1764.86 gpus</td>\n",
       "      <td>39.6 years - 176.486 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>12,500,000</td>\n",
       "      <td>12,000,000,000.0 EFLOPs</td>\n",
       "      <td>960,000.0 PFLOPs</td>\n",
       "      <td>1764868.0</td>\n",
       "      <td>52.95B</td>\n",
       "      <td>144.7 days</td>\n",
       "      <td>4.0 years - 176486.8 gpus</td>\n",
       "      <td>39.6 years - 17648.68 gpus</td>\n",
       "      <td>396.1 years - 1764.868 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>125,000,000</td>\n",
       "      <td>1,200,000,000,000.0 EFLOPs</td>\n",
       "      <td>9,600,000.0 PFLOPs</td>\n",
       "      <td>17648680.0</td>\n",
       "      <td>529.46B</td>\n",
       "      <td>1446.8 days</td>\n",
       "      <td>39.6 years - 1764868.0 gpus</td>\n",
       "      <td>396.1 years - 176486.8 gpus</td>\n",
       "      <td>3961.0 years - 17648.68 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>50,000</td>\n",
       "      <td>1,200,000.0 EFLOPs</td>\n",
       "      <td>24,000.0 PFLOPs</td>\n",
       "      <td>44121.0</td>\n",
       "      <td>1.32B</td>\n",
       "      <td>0.6 days</td>\n",
       "      <td>0.0 years - 4412.1 gpus</td>\n",
       "      <td>0.2 years - 441.21 gpus</td>\n",
       "      <td>1.6 years - 44.121 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>250,000</td>\n",
       "      <td>30,000,000.0 EFLOPs</td>\n",
       "      <td>120,000.0 PFLOPs</td>\n",
       "      <td>220608.0</td>\n",
       "      <td>6.62B</td>\n",
       "      <td>2.9 days</td>\n",
       "      <td>0.1 years - 22060.8 gpus</td>\n",
       "      <td>0.8 years - 2206.08 gpus</td>\n",
       "      <td>7.9 years - 220.608 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>500,000</td>\n",
       "      <td>120,000,000.0 EFLOPs</td>\n",
       "      <td>240,000.0 PFLOPs</td>\n",
       "      <td>441217.0</td>\n",
       "      <td>13.24B</td>\n",
       "      <td>5.8 days</td>\n",
       "      <td>0.2 years - 44121.7 gpus</td>\n",
       "      <td>1.6 years - 4412.17 gpus</td>\n",
       "      <td>15.8 years - 441.217 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>5,000,000</td>\n",
       "      <td>12,000,000,000.0 EFLOPs</td>\n",
       "      <td>2,400,000.0 PFLOPs</td>\n",
       "      <td>4412170.0</td>\n",
       "      <td>132.37B</td>\n",
       "      <td>57.9 days</td>\n",
       "      <td>1.6 years - 441217.0 gpus</td>\n",
       "      <td>15.8 years - 44121.7 gpus</td>\n",
       "      <td>158.4 years - 4412.17 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>50,000,000</td>\n",
       "      <td>1,200,000,000,000.0 EFLOPs</td>\n",
       "      <td>24,000,000.0 PFLOPs</td>\n",
       "      <td>44121702.0</td>\n",
       "      <td>1323.65B</td>\n",
       "      <td>578.7 days</td>\n",
       "      <td>15.8 years - 4412170.2 gpus</td>\n",
       "      <td>158.4 years - 441217.02 gpus</td>\n",
       "      <td>1584.4 years - 44121.702 gpus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (Params) Dataset Size (Tokens) Global Batch Size    Total Steps  \\\n",
       "0                0.1T                  2.0T              2.0M      1,000,000   \n",
       "1                0.5T                 10.0T              2.0M      5,000,000   \n",
       "2                1.0T                 20.0T              2.0M     10,000,000   \n",
       "3               10.0T                200.0T              2.0M    100,000,000   \n",
       "4              100.0T               2000.0T              2.0M  1,000,000,000   \n",
       "0                0.1T                  2.0T             16.0M        125,000   \n",
       "1                0.5T                 10.0T             16.0M        625,000   \n",
       "2                1.0T                 20.0T             16.0M      1,250,000   \n",
       "3               10.0T                200.0T             16.0M     12,500,000   \n",
       "4              100.0T               2000.0T             16.0M    125,000,000   \n",
       "0                0.1T                  2.0T             40.0M         50,000   \n",
       "1                0.5T                 10.0T             40.0M        250,000   \n",
       "2                1.0T                 20.0T             40.0M        500,000   \n",
       "3               10.0T                200.0T             40.0M      5,000,000   \n",
       "4              100.0T               2000.0T             40.0M     50,000,000   \n",
       "\n",
       "                  Total FLOPs       FLOPs per Step  H100 GPUs Needed  \\\n",
       "0          1,200,000.0 EFLOPs       1,200.0 PFLOPs            2206.0   \n",
       "1         30,000,000.0 EFLOPs       6,000.0 PFLOPs           11030.0   \n",
       "2        120,000,000.0 EFLOPs      12,000.0 PFLOPs           22060.0   \n",
       "3     12,000,000,000.0 EFLOPs     120,000.0 PFLOPs          220608.0   \n",
       "4  1,200,000,000,000.0 EFLOPs   1,200,000.0 PFLOPs         2206085.0   \n",
       "0          1,200,000.0 EFLOPs       9,600.0 PFLOPs           17648.0   \n",
       "1         30,000,000.0 EFLOPs      48,000.0 PFLOPs           88243.0   \n",
       "2        120,000,000.0 EFLOPs      96,000.0 PFLOPs          176486.0   \n",
       "3     12,000,000,000.0 EFLOPs     960,000.0 PFLOPs         1764868.0   \n",
       "4  1,200,000,000,000.0 EFLOPs   9,600,000.0 PFLOPs        17648680.0   \n",
       "0          1,200,000.0 EFLOPs      24,000.0 PFLOPs           44121.0   \n",
       "1         30,000,000.0 EFLOPs     120,000.0 PFLOPs          220608.0   \n",
       "2        120,000,000.0 EFLOPs     240,000.0 PFLOPs          441217.0   \n",
       "3     12,000,000,000.0 EFLOPs   2,400,000.0 PFLOPs         4412170.0   \n",
       "4  1,200,000,000,000.0 EFLOPs  24,000,000.0 PFLOPs        44121702.0   \n",
       "\n",
       "  Total H100s cost Total Training Time without grad accum  \\\n",
       "0            0.07B                              11.6 days   \n",
       "1            0.33B                              57.9 days   \n",
       "2            0.66B                             115.7 days   \n",
       "3            6.62B                            1157.4 days   \n",
       "4           66.18B                           11574.1 days   \n",
       "0            0.53B                               1.4 days   \n",
       "1            2.65B                               7.2 days   \n",
       "2            5.29B                              14.5 days   \n",
       "3           52.95B                             144.7 days   \n",
       "4          529.46B                            1446.8 days   \n",
       "0            1.32B                               0.6 days   \n",
       "1            6.62B                               2.9 days   \n",
       "2           13.24B                               5.8 days   \n",
       "3          132.37B                              57.9 days   \n",
       "4         1323.65B                             578.7 days   \n",
       "\n",
       "  Total Training Time with 10 grad accum  \\\n",
       "0                 0.3 years - 220.6 gpus   \n",
       "1                1.6 years - 1103.0 gpus   \n",
       "2                3.2 years - 2206.0 gpus   \n",
       "3              31.7 years - 22060.8 gpus   \n",
       "4            316.9 years - 220608.5 gpus   \n",
       "0                0.0 years - 1764.8 gpus   \n",
       "1                0.2 years - 8824.3 gpus   \n",
       "2               0.4 years - 17648.6 gpus   \n",
       "3              4.0 years - 176486.8 gpus   \n",
       "4            39.6 years - 1764868.0 gpus   \n",
       "0                0.0 years - 4412.1 gpus   \n",
       "1               0.1 years - 22060.8 gpus   \n",
       "2               0.2 years - 44121.7 gpus   \n",
       "3              1.6 years - 441217.0 gpus   \n",
       "4            15.8 years - 4412170.2 gpus   \n",
       "\n",
       "  Total Training Time with 100 grad accum  \\\n",
       "0                  3.2 years - 22.06 gpus   \n",
       "1                 15.8 years - 110.3 gpus   \n",
       "2                 31.7 years - 220.6 gpus   \n",
       "3              316.9 years - 2206.08 gpus   \n",
       "4            3168.8 years - 22060.85 gpus   \n",
       "0                 0.4 years - 176.48 gpus   \n",
       "1                 2.0 years - 882.43 gpus   \n",
       "2                4.0 years - 1764.86 gpus   \n",
       "3              39.6 years - 17648.68 gpus   \n",
       "4             396.1 years - 176486.8 gpus   \n",
       "0                 0.2 years - 441.21 gpus   \n",
       "1                0.8 years - 2206.08 gpus   \n",
       "2                1.6 years - 4412.17 gpus   \n",
       "3               15.8 years - 44121.7 gpus   \n",
       "4            158.4 years - 441217.02 gpus   \n",
       "\n",
       "  Total Training Time with 1000 grad accum  \n",
       "0                  31.7 years - 2.206 gpus  \n",
       "1                 158.4 years - 11.03 gpus  \n",
       "2                 316.9 years - 22.06 gpus  \n",
       "3              3168.8 years - 220.608 gpus  \n",
       "4            31688.1 years - 2206.085 gpus  \n",
       "0                  4.0 years - 17.648 gpus  \n",
       "1                 19.8 years - 88.243 gpus  \n",
       "2                39.6 years - 176.486 gpus  \n",
       "3              396.1 years - 1764.868 gpus  \n",
       "4             3961.0 years - 17648.68 gpus  \n",
       "0                  1.6 years - 44.121 gpus  \n",
       "1                 7.9 years - 220.608 gpus  \n",
       "2                15.8 years - 441.217 gpus  \n",
       "3               158.4 years - 4412.17 gpus  \n",
       "4            1584.4 years - 44121.702 gpus  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032de16-00f5-4881-8746-97ae7026ecda",
   "metadata": {},
   "source": [
    "##### Communication time of data parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54207cd-a0b5-4bed-aacd-ff3ee447f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import FP8_BYTES, BFLOAT16_BYTES\n",
    "from utils import convert_bytes_to_terabytes\n",
    "from utils import calculate_comm_time_given_comm_volume, convert_bytes_to_gigabytes\n",
    "from constants import NVLINK_MAX_TOTAL_BANDWIDTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7dd7e3-632b-44d1-bc85-e6ee59495f85",
   "metadata": {},
   "source": [
    "Assume that fwd+bwd pass of a single replicas takes 1 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b702221-3a61-4cf9-9470-9e74aed3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comm_bandwidths = [0.5*1024**3, 1*1024**3, 4*1024**3] # bytes/sec\n",
    "# comm_bandwidths = [40*1024**3, NVLINK_MAX_TOTAL_BANDWIDTH] # bytes/sec\n",
    "comm_bandwidths = [40*1024**3] # bytes/sec\n",
    "cluster_sizes = [1024, 10240, 102400]\n",
    "\n",
    "data_mem = {\n",
    "    \"Model Size (Params)\": [],\n",
    "    \"Global batch size\": [],\n",
    "    # \"Number of datacenters\": [],\n",
    "    \"Total bfloat16 gradient storage\": [],\n",
    "    \"Total fp8 gradient storage\": [],\n",
    "    \"Bandwidth\": [],\n",
    "    \"Total communication time in bfloat16 - comm/compute ratio\": [],\n",
    "    \"Total communication time in fp8 - comm/compute ratio\": [],\n",
    "    \"Total GPU idle cost for bfloat16 comm\": [],\n",
    "    \"Total GPU idle cost for fp8 comm\": [],\n",
    "    \"DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio\": []\n",
    "}\n",
    "# for cluster_size in cluster_sizes:\n",
    "for bandwidth in comm_bandwidths:\n",
    "    for global_batch_size in global_batch_sizes:\n",
    "        for model_size in target_model_sizes:\n",
    "            total_steps = calculate_total_steps(model_size, global_batch_size)\n",
    "            total_time = calculate_total_time_to_train_a_model(model_size, global_batch_size, time_per_step)\n",
    "            \n",
    "            # h100s_per_step = calculate_num_h100s_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "            # num_clusters = h100s_per_step // cluster_size\n",
    "\n",
    "            bfloat16_grad_comm_volume = model_size * BFLOAT16_BYTES\n",
    "            fp8_grad_comm_volume = model_size * FP8_BYTES\n",
    "            \n",
    "            bfloat16_total_comm_time = calculate_comm_time_given_comm_volume(bfloat16_grad_comm_volume, bandwidth) * total_steps\n",
    "            fp8_total_comm_time = calculate_comm_time_given_comm_volume(fp8_grad_comm_volume, bandwidth) * total_steps\n",
    "            bfloat16_diloco_total_comm_time = calculate_comm_time_given_comm_volume(fp8_grad_comm_volume, bandwidth) * (total_steps / 500)\n",
    "\n",
    "            bfloat16_comm_compute_ratio = (bfloat16_total_comm_time / (total_time + bfloat16_total_comm_time)) * 100\n",
    "            fp8_comm_compute_ratio = (fp8_total_comm_time / (total_time + fp8_total_comm_time)) * 100\n",
    "            bfloat16_diloco_comm_compute_ratio = (bfloat16_diloco_total_comm_time / (total_time + bfloat16_diloco_total_comm_time)) * 100\n",
    "            \n",
    "            bfloat16_total_gpu_idle_cost_comm = ((bfloat16_total_comm_time * h100s_per_step) / (60*60)) / H100_COST_PER_HOUR\n",
    "            fp8_total_gpu_idle_cost_comm = ((fp8_total_comm_time * h100s_per_step) / (60*60)) / H100_COST_PER_HOUR\n",
    "            \n",
    "            data_mem[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "            data_mem[\"Global batch size\"].append(f'{global_batch_size/1e6}M')\n",
    "            # data_mem[\"Number of datacenters\"].append(num_clusters)\n",
    "            data_mem[\"Total bfloat16 gradient storage\"].append(convert_bytes_to_terabytes(model_size * BFLOAT16_BYTES))\n",
    "            data_mem[\"Total fp8 gradient storage\"].append(convert_bytes_to_terabytes(model_size * FP8_BYTES))\n",
    "            data_mem[\"Bandwidth\"].append(f\"{convert_bytes_to_gigabytes(bandwidth)}/s\")\n",
    "            data_mem[\"Total communication time in bfloat16 - comm/compute ratio\"].append(f\"{convert_seconds_to_days(bfloat16_total_comm_time)} / {convert_seconds_to_years(bfloat16_total_comm_time)} - {bfloat16_comm_compute_ratio:.2f}%\")\n",
    "            data_mem[\"Total communication time in fp8 - comm/compute ratio\"].append(f\"{convert_seconds_to_days(bfloat16_total_comm_time)} / {convert_seconds_to_years(fp8_total_comm_time)} - {fp8_comm_compute_ratio:.2f}%\")\n",
    "            data_mem[\"Total GPU idle cost for bfloat16 comm\"].append(convert_to_billion_format(bfloat16_total_gpu_idle_cost_comm))\n",
    "            data_mem[\"Total GPU idle cost for fp8 comm\"].append(convert_to_billion_format(fp8_total_gpu_idle_cost_comm))\n",
    "            data_mem[\"DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio\"].append(f\"{convert_seconds_to_days(bfloat16_diloco_total_comm_time)}  - {bfloat16_diloco_comm_compute_ratio:.2f}%\")\n",
    "    \n",
    "    df_mem = pd.DataFrame(data_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d602d39-1e4e-4f90-a574-509e46b83733",
   "metadata": {},
   "source": [
    "Add cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b92fc56-67f2-4502-a4dd-36e6f4c612fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Global batch size</th>\n",
       "      <th>Total bfloat16 gradient storage</th>\n",
       "      <th>Total fp8 gradient storage</th>\n",
       "      <th>Bandwidth</th>\n",
       "      <th>Total communication time in bfloat16 - comm/compute ratio</th>\n",
       "      <th>Total communication time in fp8 - comm/compute ratio</th>\n",
       "      <th>Total GPU idle cost for bfloat16 comm</th>\n",
       "      <th>Total GPU idle cost for fp8 comm</th>\n",
       "      <th>DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>0.200 TB</td>\n",
       "      <td>0.100 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>53.9 days / 0.1 years - 82.32%</td>\n",
       "      <td>53.9 days / 0.1 years - 69.95%</td>\n",
       "      <td>28.54B</td>\n",
       "      <td>14.27B</td>\n",
       "      <td>0.1 days  - 0.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>0.500 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>1347.4 days / 3.7 years - 95.88%</td>\n",
       "      <td>1347.4 days / 1.8 years - 92.09%</td>\n",
       "      <td>713.39B</td>\n",
       "      <td>356.70B</td>\n",
       "      <td>1.3 days  - 2.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>2.000 TB</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>5389.6 days / 14.8 years - 97.90%</td>\n",
       "      <td>5389.6 days / 7.4 years - 95.88%</td>\n",
       "      <td>2853.58B</td>\n",
       "      <td>1426.79B</td>\n",
       "      <td>5.4 days  - 4.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>20.000 TB</td>\n",
       "      <td>10.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>538959.8 days / 1475.6 years - 99.79%</td>\n",
       "      <td>538959.8 days / 737.8 years - 99.57%</td>\n",
       "      <td>285357.90B</td>\n",
       "      <td>142678.95B</td>\n",
       "      <td>539.0 days  - 31.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>200.000 TB</td>\n",
       "      <td>100.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>53895982.3 days / 147559.2 years - 99.98%</td>\n",
       "      <td>53895982.3 days / 73779.6 years - 99.96%</td>\n",
       "      <td>28535789.65B</td>\n",
       "      <td>14267894.83B</td>\n",
       "      <td>53896.0 days  - 82.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>0.200 TB</td>\n",
       "      <td>0.100 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>6.7 days / 0.0 years - 82.32%</td>\n",
       "      <td>6.7 days / 0.0 years - 69.95%</td>\n",
       "      <td>3.57B</td>\n",
       "      <td>1.78B</td>\n",
       "      <td>0.0 days  - 0.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>0.500 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>168.4 days / 0.5 years - 95.88%</td>\n",
       "      <td>168.4 days / 0.2 years - 92.09%</td>\n",
       "      <td>89.17B</td>\n",
       "      <td>44.59B</td>\n",
       "      <td>0.2 days  - 2.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>2.000 TB</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>673.7 days / 1.8 years - 97.90%</td>\n",
       "      <td>673.7 days / 0.9 years - 95.88%</td>\n",
       "      <td>356.70B</td>\n",
       "      <td>178.35B</td>\n",
       "      <td>0.7 days  - 4.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>20.000 TB</td>\n",
       "      <td>10.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>67370.0 days / 184.4 years - 99.79%</td>\n",
       "      <td>67370.0 days / 92.2 years - 99.57%</td>\n",
       "      <td>35669.74B</td>\n",
       "      <td>17834.87B</td>\n",
       "      <td>67.4 days  - 31.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>200.000 TB</td>\n",
       "      <td>100.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>6736997.8 days / 18444.9 years - 99.98%</td>\n",
       "      <td>6736997.8 days / 9222.4 years - 99.96%</td>\n",
       "      <td>3566973.71B</td>\n",
       "      <td>1783486.85B</td>\n",
       "      <td>6737.0 days  - 82.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>0.200 TB</td>\n",
       "      <td>0.100 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>2.7 days / 0.0 years - 82.32%</td>\n",
       "      <td>2.7 days / 0.0 years - 69.95%</td>\n",
       "      <td>1.43B</td>\n",
       "      <td>0.71B</td>\n",
       "      <td>0.0 days  - 0.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>0.500 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>67.4 days / 0.2 years - 95.88%</td>\n",
       "      <td>67.4 days / 0.1 years - 92.09%</td>\n",
       "      <td>35.67B</td>\n",
       "      <td>17.83B</td>\n",
       "      <td>0.1 days  - 2.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>2.000 TB</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>269.5 days / 0.7 years - 97.90%</td>\n",
       "      <td>269.5 days / 0.4 years - 95.88%</td>\n",
       "      <td>142.68B</td>\n",
       "      <td>71.34B</td>\n",
       "      <td>0.3 days  - 4.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>20.000 TB</td>\n",
       "      <td>10.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>26948.0 days / 73.8 years - 99.79%</td>\n",
       "      <td>26948.0 days / 36.9 years - 99.57%</td>\n",
       "      <td>14267.89B</td>\n",
       "      <td>7133.95B</td>\n",
       "      <td>26.9 days  - 31.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>200.000 TB</td>\n",
       "      <td>100.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>2694799.1 days / 7378.0 years - 99.98%</td>\n",
       "      <td>2694799.1 days / 3689.0 years - 99.96%</td>\n",
       "      <td>1426789.48B</td>\n",
       "      <td>713394.74B</td>\n",
       "      <td>2694.8 days  - 82.32%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Size (Params) Global batch size Total bfloat16 gradient storage  \\\n",
       "0                 0.1T              2.0M                        0.200 TB   \n",
       "1                 0.5T              2.0M                        1.000 TB   \n",
       "2                 1.0T              2.0M                        2.000 TB   \n",
       "3                10.0T              2.0M                       20.000 TB   \n",
       "4               100.0T              2.0M                      200.000 TB   \n",
       "5                 0.1T             16.0M                        0.200 TB   \n",
       "6                 0.5T             16.0M                        1.000 TB   \n",
       "7                 1.0T             16.0M                        2.000 TB   \n",
       "8                10.0T             16.0M                       20.000 TB   \n",
       "9               100.0T             16.0M                      200.000 TB   \n",
       "10                0.1T             40.0M                        0.200 TB   \n",
       "11                0.5T             40.0M                        1.000 TB   \n",
       "12                1.0T             40.0M                        2.000 TB   \n",
       "13               10.0T             40.0M                       20.000 TB   \n",
       "14              100.0T             40.0M                      200.000 TB   \n",
       "\n",
       "   Total fp8 gradient storage    Bandwidth  \\\n",
       "0                    0.100 TB  42.950 GB/s   \n",
       "1                    0.500 TB  42.950 GB/s   \n",
       "2                    1.000 TB  42.950 GB/s   \n",
       "3                   10.000 TB  42.950 GB/s   \n",
       "4                  100.000 TB  42.950 GB/s   \n",
       "5                    0.100 TB  42.950 GB/s   \n",
       "6                    0.500 TB  42.950 GB/s   \n",
       "7                    1.000 TB  42.950 GB/s   \n",
       "8                   10.000 TB  42.950 GB/s   \n",
       "9                  100.000 TB  42.950 GB/s   \n",
       "10                   0.100 TB  42.950 GB/s   \n",
       "11                   0.500 TB  42.950 GB/s   \n",
       "12                   1.000 TB  42.950 GB/s   \n",
       "13                  10.000 TB  42.950 GB/s   \n",
       "14                 100.000 TB  42.950 GB/s   \n",
       "\n",
       "   Total communication time in bfloat16 - comm/compute ratio  \\\n",
       "0                      53.9 days / 0.1 years - 82.32%          \n",
       "1                    1347.4 days / 3.7 years - 95.88%          \n",
       "2                   5389.6 days / 14.8 years - 97.90%          \n",
       "3               538959.8 days / 1475.6 years - 99.79%          \n",
       "4           53895982.3 days / 147559.2 years - 99.98%          \n",
       "5                       6.7 days / 0.0 years - 82.32%          \n",
       "6                     168.4 days / 0.5 years - 95.88%          \n",
       "7                     673.7 days / 1.8 years - 97.90%          \n",
       "8                 67370.0 days / 184.4 years - 99.79%          \n",
       "9             6736997.8 days / 18444.9 years - 99.98%          \n",
       "10                      2.7 days / 0.0 years - 82.32%          \n",
       "11                     67.4 days / 0.2 years - 95.88%          \n",
       "12                    269.5 days / 0.7 years - 97.90%          \n",
       "13                 26948.0 days / 73.8 years - 99.79%          \n",
       "14             2694799.1 days / 7378.0 years - 99.98%          \n",
       "\n",
       "   Total communication time in fp8 - comm/compute ratio  \\\n",
       "0                      53.9 days / 0.1 years - 69.95%     \n",
       "1                    1347.4 days / 1.8 years - 92.09%     \n",
       "2                    5389.6 days / 7.4 years - 95.88%     \n",
       "3                538959.8 days / 737.8 years - 99.57%     \n",
       "4            53895982.3 days / 73779.6 years - 99.96%     \n",
       "5                       6.7 days / 0.0 years - 69.95%     \n",
       "6                     168.4 days / 0.2 years - 92.09%     \n",
       "7                     673.7 days / 0.9 years - 95.88%     \n",
       "8                  67370.0 days / 92.2 years - 99.57%     \n",
       "9              6736997.8 days / 9222.4 years - 99.96%     \n",
       "10                      2.7 days / 0.0 years - 69.95%     \n",
       "11                     67.4 days / 0.1 years - 92.09%     \n",
       "12                    269.5 days / 0.4 years - 95.88%     \n",
       "13                 26948.0 days / 36.9 years - 99.57%     \n",
       "14             2694799.1 days / 3689.0 years - 99.96%     \n",
       "\n",
       "   Total GPU idle cost for bfloat16 comm Total GPU idle cost for fp8 comm  \\\n",
       "0                                 28.54B                           14.27B   \n",
       "1                                713.39B                          356.70B   \n",
       "2                               2853.58B                         1426.79B   \n",
       "3                             285357.90B                       142678.95B   \n",
       "4                           28535789.65B                     14267894.83B   \n",
       "5                                  3.57B                            1.78B   \n",
       "6                                 89.17B                           44.59B   \n",
       "7                                356.70B                          178.35B   \n",
       "8                              35669.74B                        17834.87B   \n",
       "9                            3566973.71B                      1783486.85B   \n",
       "10                                 1.43B                            0.71B   \n",
       "11                                35.67B                           17.83B   \n",
       "12                               142.68B                           71.34B   \n",
       "13                             14267.89B                         7133.95B   \n",
       "14                           1426789.48B                       713394.74B   \n",
       "\n",
       "   DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio  \n",
       "0                                   0.1 days  - 0.46%                                    \n",
       "1                                   1.3 days  - 2.28%                                    \n",
       "2                                   5.4 days  - 4.45%                                    \n",
       "3                                539.0 days  - 31.77%                                    \n",
       "4                              53896.0 days  - 82.32%                                    \n",
       "5                                   0.0 days  - 0.46%                                    \n",
       "6                                   0.2 days  - 2.28%                                    \n",
       "7                                   0.7 days  - 4.45%                                    \n",
       "8                                 67.4 days  - 31.77%                                    \n",
       "9                               6737.0 days  - 82.32%                                    \n",
       "10                                  0.0 days  - 0.46%                                    \n",
       "11                                  0.1 days  - 2.28%                                    \n",
       "12                                  0.3 days  - 4.45%                                    \n",
       "13                                26.9 days  - 31.77%                                    \n",
       "14                              2694.8 days  - 82.32%                                    "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97153b-7c80-45de-bffc-31dd0caa676d",
   "metadata": {},
   "source": [
    "#### Communication latency (theoretical minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ba983-2882-48d4-9541-1bd8faabfddb",
   "metadata": {},
   "source": [
    "Assumptions on communication\n",
    "- No limit on banwidth\n",
    "- Achieve speed of light\n",
    "- Clostest surface distance between two points on the earth surface (assume you don't dig a crazy hole to go a straight line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b40c02-2ac3-4dce-947e-a4915b7518f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_latency_between_jz_and_jc = compute_minimum_latency_between_clusters(\"JEAN_ZAY\", \"JOLIOT_CURIE\")\n",
    "minimum_latency_between_jz_and_ec = compute_minimum_latency_between_clusters(\"JEAN_ZAY\", \"EL_CAPITAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba98b7a-e713-446d-9dad-613b76d447e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_comm = []\n",
    "\n",
    "for global_batch_size in global_batch_sizes:\n",
    "    data_comm = {\n",
    "        \"Model Size (Params)\": [],\n",
    "        \"Dataset Size (Tokens)\": [],\n",
    "        \"Global Batch Size\": [],\n",
    "        \n",
    "        \"Total minimum communication latency between JZ and JC\": [],\n",
    "        \"Total GPU idle time for minimum comm between JZ and JC\": [],\n",
    "        \"GPU idle cost during JZ-JC minimum communication latency\": [],\n",
    "        \n",
    "        \"Total minimum communication latency between JZ and EC\": [],\n",
    "        \"Total GPU idle time for minimum comm between JZ and EC\": [],\n",
    "        \"GPU idle cost during JZ-EC minimum communication latency\": [],\n",
    "    }\n",
    "    \n",
    "    for model_size in target_model_sizes:\n",
    "        dataset_size = get_dataset_size_from_model_size(model_size)\n",
    "        # Append to dictionary\n",
    "        data_comm[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "        data_comm[\"Dataset Size (Tokens)\"].append(convert_to_xt_format(dataset_size))\n",
    "        data_comm[\"Global Batch Size\"].append(f'{global_batch_size/1e6}M')\n",
    "        \n",
    "        data_comm[\"Total minimum communication latency between JZ and JC\"].append(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc))\n",
    "        data_comm[\"Total GPU idle time for minimum comm between JZ and JC\"].append(convert_seconds_to_days(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc) * h100s_per_step))\n",
    "        data_comm[\"GPU idle cost during JZ-JC minimum communication latency\"].append(convert_to_million_format((calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc) / (60 * 60)) * h100s_per_step * H100_COST_PER_HOUR))\n",
    "        \n",
    "        data_comm[\"Total minimum communication latency between JZ and EC\"].append(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_ec))\n",
    "        data_comm[\"Total GPU idle time for minimum comm between JZ and EC\"].append(convert_seconds_to_years(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_ec) * h100s_per_step))\n",
    "        data_comm[\"GPU idle cost during JZ-EC minimum communication latency\"].append(convert_to_million_format((calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_ec) / (60 * 60)) * h100s_per_step * H100_COST_PER_HOUR))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_comm = pd.DataFrame(data_comm)\n",
    "    # df_comm['Global Batch Size'] = f'{global_batch_size/1e6}M'\n",
    "    dataframes_comm.append(df_comm)\n",
    "\n",
    "final_df_comm = pd.DataFrame()\n",
    "for i, df in enumerate(dataframes_comm):\n",
    "    final_df_comm = pd.concat([final_df_comm, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e897a27-98f8-43a8-aec6-55310d7da1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Dataset Size (Tokens)</th>\n",
       "      <th>Global Batch Size</th>\n",
       "      <th>Total minimum communication latency between JZ and JC</th>\n",
       "      <th>Total GPU idle time for minimum comm between JZ and JC</th>\n",
       "      <th>GPU idle cost during JZ-JC minimum communication latency</th>\n",
       "      <th>Total minimum communication latency between JZ and EC</th>\n",
       "      <th>Total GPU idle time for minimum comm between JZ and EC</th>\n",
       "      <th>GPU idle cost during JZ-EC minimum communication latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1.278270</td>\n",
       "      <td>652.8 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>29.790336</td>\n",
       "      <td>41.7 years</td>\n",
       "      <td>0.7m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>6.391352</td>\n",
       "      <td>3263.9 days</td>\n",
       "      <td>0.2m</td>\n",
       "      <td>148.951681</td>\n",
       "      <td>208.3 years</td>\n",
       "      <td>3.7m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>12.782705</td>\n",
       "      <td>6527.7 days</td>\n",
       "      <td>0.3m</td>\n",
       "      <td>297.903362</td>\n",
       "      <td>416.5 years</td>\n",
       "      <td>7.3m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>127.827046</td>\n",
       "      <td>65277.2 days</td>\n",
       "      <td>3.1m</td>\n",
       "      <td>2979.033618</td>\n",
       "      <td>4165.1 years</td>\n",
       "      <td>73.0m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1278.270459</td>\n",
       "      <td>652771.6 days</td>\n",
       "      <td>31.3m</td>\n",
       "      <td>29790.336181</td>\n",
       "      <td>41650.8 years</td>\n",
       "      <td>730.2m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>0.159784</td>\n",
       "      <td>81.6 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>3.723792</td>\n",
       "      <td>5.2 years</td>\n",
       "      <td>0.1m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>0.798919</td>\n",
       "      <td>408.0 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>18.618960</td>\n",
       "      <td>26.0 years</td>\n",
       "      <td>0.5m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>1.597838</td>\n",
       "      <td>816.0 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>37.237920</td>\n",
       "      <td>52.1 years</td>\n",
       "      <td>0.9m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>15.978381</td>\n",
       "      <td>8159.6 days</td>\n",
       "      <td>0.4m</td>\n",
       "      <td>372.379202</td>\n",
       "      <td>520.6 years</td>\n",
       "      <td>9.1m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>159.783807</td>\n",
       "      <td>81596.5 days</td>\n",
       "      <td>3.9m</td>\n",
       "      <td>3723.792023</td>\n",
       "      <td>5206.4 years</td>\n",
       "      <td>91.3m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>0.063914</td>\n",
       "      <td>32.6 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>1.489517</td>\n",
       "      <td>2.1 years</td>\n",
       "      <td>0.0m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>0.319568</td>\n",
       "      <td>163.2 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>7.447584</td>\n",
       "      <td>10.4 years</td>\n",
       "      <td>0.2m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>0.639135</td>\n",
       "      <td>326.4 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>14.895168</td>\n",
       "      <td>20.8 years</td>\n",
       "      <td>0.4m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>6.391352</td>\n",
       "      <td>3263.9 days</td>\n",
       "      <td>0.2m</td>\n",
       "      <td>148.951681</td>\n",
       "      <td>208.3 years</td>\n",
       "      <td>3.7m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>63.913523</td>\n",
       "      <td>32638.6 days</td>\n",
       "      <td>1.6m</td>\n",
       "      <td>1489.516809</td>\n",
       "      <td>2082.5 years</td>\n",
       "      <td>36.5m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (Params) Dataset Size (Tokens) Global Batch Size  \\\n",
       "0                0.1T                  2.0T              2.0M   \n",
       "1                0.5T                 10.0T              2.0M   \n",
       "2                1.0T                 20.0T              2.0M   \n",
       "3               10.0T                200.0T              2.0M   \n",
       "4              100.0T               2000.0T              2.0M   \n",
       "0                0.1T                  2.0T             16.0M   \n",
       "1                0.5T                 10.0T             16.0M   \n",
       "2                1.0T                 20.0T             16.0M   \n",
       "3               10.0T                200.0T             16.0M   \n",
       "4              100.0T               2000.0T             16.0M   \n",
       "0                0.1T                  2.0T             40.0M   \n",
       "1                0.5T                 10.0T             40.0M   \n",
       "2                1.0T                 20.0T             40.0M   \n",
       "3               10.0T                200.0T             40.0M   \n",
       "4              100.0T               2000.0T             40.0M   \n",
       "\n",
       "   Total minimum communication latency between JZ and JC  \\\n",
       "0                                           1.278270       \n",
       "1                                           6.391352       \n",
       "2                                          12.782705       \n",
       "3                                         127.827046       \n",
       "4                                        1278.270459       \n",
       "0                                           0.159784       \n",
       "1                                           0.798919       \n",
       "2                                           1.597838       \n",
       "3                                          15.978381       \n",
       "4                                         159.783807       \n",
       "0                                           0.063914       \n",
       "1                                           0.319568       \n",
       "2                                           0.639135       \n",
       "3                                           6.391352       \n",
       "4                                          63.913523       \n",
       "\n",
       "  Total GPU idle time for minimum comm between JZ and JC  \\\n",
       "0                                         652.8 days       \n",
       "1                                        3263.9 days       \n",
       "2                                        6527.7 days       \n",
       "3                                       65277.2 days       \n",
       "4                                      652771.6 days       \n",
       "0                                          81.6 days       \n",
       "1                                         408.0 days       \n",
       "2                                         816.0 days       \n",
       "3                                        8159.6 days       \n",
       "4                                       81596.5 days       \n",
       "0                                          32.6 days       \n",
       "1                                         163.2 days       \n",
       "2                                         326.4 days       \n",
       "3                                        3263.9 days       \n",
       "4                                       32638.6 days       \n",
       "\n",
       "  GPU idle cost during JZ-JC minimum communication latency  \\\n",
       "0                                               0.0m         \n",
       "1                                               0.2m         \n",
       "2                                               0.3m         \n",
       "3                                               3.1m         \n",
       "4                                              31.3m         \n",
       "0                                               0.0m         \n",
       "1                                               0.0m         \n",
       "2                                               0.0m         \n",
       "3                                               0.4m         \n",
       "4                                               3.9m         \n",
       "0                                               0.0m         \n",
       "1                                               0.0m         \n",
       "2                                               0.0m         \n",
       "3                                               0.2m         \n",
       "4                                               1.6m         \n",
       "\n",
       "   Total minimum communication latency between JZ and EC  \\\n",
       "0                                          29.790336       \n",
       "1                                         148.951681       \n",
       "2                                         297.903362       \n",
       "3                                        2979.033618       \n",
       "4                                       29790.336181       \n",
       "0                                           3.723792       \n",
       "1                                          18.618960       \n",
       "2                                          37.237920       \n",
       "3                                         372.379202       \n",
       "4                                        3723.792023       \n",
       "0                                           1.489517       \n",
       "1                                           7.447584       \n",
       "2                                          14.895168       \n",
       "3                                         148.951681       \n",
       "4                                        1489.516809       \n",
       "\n",
       "  Total GPU idle time for minimum comm between JZ and EC  \\\n",
       "0                                         41.7 years       \n",
       "1                                        208.3 years       \n",
       "2                                        416.5 years       \n",
       "3                                       4165.1 years       \n",
       "4                                      41650.8 years       \n",
       "0                                          5.2 years       \n",
       "1                                         26.0 years       \n",
       "2                                         52.1 years       \n",
       "3                                        520.6 years       \n",
       "4                                       5206.4 years       \n",
       "0                                          2.1 years       \n",
       "1                                         10.4 years       \n",
       "2                                         20.8 years       \n",
       "3                                        208.3 years       \n",
       "4                                       2082.5 years       \n",
       "\n",
       "  GPU idle cost during JZ-EC minimum communication latency  \n",
       "0                                               0.7m        \n",
       "1                                               3.7m        \n",
       "2                                               7.3m        \n",
       "3                                              73.0m        \n",
       "4                                             730.2m        \n",
       "0                                               0.1m        \n",
       "1                                               0.5m        \n",
       "2                                               0.9m        \n",
       "3                                               9.1m        \n",
       "4                                              91.3m        \n",
       "0                                               0.0m        \n",
       "1                                               0.2m        \n",
       "2                                               0.4m        \n",
       "3                                               3.7m        \n",
       "4                                              36.5m        "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a547c9e-def0-4c82-a541-517ec6d127c6",
   "metadata": {},
   "source": [
    "#### Electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2735f7c7-3210-414b-9590-b2c3a0620355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import TOTAL_H100_WATT\n",
    "from utils import convert_watts_to_megawatts, convert_watts_to_terawatts, calculate_electricity_consumption_of_an_h100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8857f303-4c68-4db4-9912-9f1fdd3afa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_elec = []\n",
    "\n",
    "for global_batch_size in global_batch_sizes:\n",
    "    data_elec = {\n",
    "        \"Model Size (Params)\": [],\n",
    "        \"Dataset Size (Tokens)\": [],\n",
    "        \"Global batch size\": [],\n",
    "        \"Number of GPUs\": [],\n",
    "        \"Total electricity per step (without grad accum)\": [],\n",
    "        \"Total electricity for the entire training (without grad accum)\": []\n",
    "    }\n",
    "    \n",
    "    for model_size in target_model_sizes:\n",
    "        dataset_size = get_dataset_size_from_model_size(model_size)\n",
    "        h100s_per_step = calculate_num_h100s_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "        total_time = calculate_total_time_to_train_a_model(model_size, global_batch_size, time_per_step)\n",
    "        total_electricity_consumption = calculate_electricity_consumption_of_an_h100(TOTAL_H100_WATT, total_time) * h100s_per_step\n",
    "        \n",
    "        data_elec[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "        data_elec[\"Dataset Size (Tokens)\"].append(convert_to_xt_format(dataset_size))\n",
    "        data_elec[\"Global batch size\"].append(f'{global_batch_size/1e6}M')\n",
    "        data_elec[\"Number of GPUs\"].append(h100s_per_step)\n",
    "        data_elec[\"Total electricity per step (without grad accum)\"].append(convert_watts_to_megawatts(h100s_per_step * TOTAL_H100_WATT))\n",
    "        data_elec[\"Total electricity for the entire training (without grad accum)\"].append(f\"{convert_watts_to_terawatts(total_electricity_consumption)}\")\n",
    "    \n",
    "    df = pd.DataFrame(data_elec)\n",
    "    # df['Global Batch Size'] = f'{global_batch_size/1e6}M'\n",
    "    dataframes_elec.append(df)\n",
    "\n",
    "final_df_elec = pd.DataFrame()\n",
    "for i, df in enumerate(dataframes_elec):\n",
    "    final_df_elec = pd.concat([final_df_elec, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96921a0d-6fc4-471b-a0d7-47e1c5ecd808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my calculation closes to 100k gpu cluster's electricity: https://semianalysis.com/2024/06/17/100000-h100-clusters-power-network/#power-challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8152cae-4337-4743-a9ba-e9fe58fa946a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Dataset Size (Tokens)</th>\n",
       "      <th>Global batch size</th>\n",
       "      <th>Number of GPUs</th>\n",
       "      <th>Total electricity per step (without grad accum)</th>\n",
       "      <th>Total electricity for the entire training (without grad accum)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>2206.0</td>\n",
       "      <td>2.813 MW</td>\n",
       "      <td>2.812650000000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>11030.0</td>\n",
       "      <td>14.063 MW</td>\n",
       "      <td>70.316250000000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>22060.0</td>\n",
       "      <td>28.127 MW</td>\n",
       "      <td>281.265000000000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>220608.0</td>\n",
       "      <td>281.275 MW</td>\n",
       "      <td>28127.520000000000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>2206085.0</td>\n",
       "      <td>2812.758 MW</td>\n",
       "      <td>2812758.375000000000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>17648.0</td>\n",
       "      <td>22.501 MW</td>\n",
       "      <td>2.812650000000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>88243.0</td>\n",
       "      <td>112.510 MW</td>\n",
       "      <td>70.318640625000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>176486.0</td>\n",
       "      <td>225.020 MW</td>\n",
       "      <td>281.274562500000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>1764868.0</td>\n",
       "      <td>2250.207 MW</td>\n",
       "      <td>28127.583750000002 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>16.0M</td>\n",
       "      <td>17648680.0</td>\n",
       "      <td>22502.067 MW</td>\n",
       "      <td>2812758.375000000000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>44121.0</td>\n",
       "      <td>56.254 MW</td>\n",
       "      <td>2.812713750000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>220608.0</td>\n",
       "      <td>281.275 MW</td>\n",
       "      <td>70.318800000000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>441217.0</td>\n",
       "      <td>562.552 MW</td>\n",
       "      <td>281.275837500000 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>4412170.0</td>\n",
       "      <td>5625.517 MW</td>\n",
       "      <td>28127.583750000002 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>44121702.0</td>\n",
       "      <td>56255.170 MW</td>\n",
       "      <td>2812758.502499999944 TW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (Params) Dataset Size (Tokens) Global batch size  Number of GPUs  \\\n",
       "0                0.1T                  2.0T              2.0M          2206.0   \n",
       "1                0.5T                 10.0T              2.0M         11030.0   \n",
       "2                1.0T                 20.0T              2.0M         22060.0   \n",
       "3               10.0T                200.0T              2.0M        220608.0   \n",
       "4              100.0T               2000.0T              2.0M       2206085.0   \n",
       "0                0.1T                  2.0T             16.0M         17648.0   \n",
       "1                0.5T                 10.0T             16.0M         88243.0   \n",
       "2                1.0T                 20.0T             16.0M        176486.0   \n",
       "3               10.0T                200.0T             16.0M       1764868.0   \n",
       "4              100.0T               2000.0T             16.0M      17648680.0   \n",
       "0                0.1T                  2.0T             40.0M         44121.0   \n",
       "1                0.5T                 10.0T             40.0M        220608.0   \n",
       "2                1.0T                 20.0T             40.0M        441217.0   \n",
       "3               10.0T                200.0T             40.0M       4412170.0   \n",
       "4              100.0T               2000.0T             40.0M      44121702.0   \n",
       "\n",
       "  Total electricity per step (without grad accum)  \\\n",
       "0                                        2.813 MW   \n",
       "1                                       14.063 MW   \n",
       "2                                       28.127 MW   \n",
       "3                                      281.275 MW   \n",
       "4                                     2812.758 MW   \n",
       "0                                       22.501 MW   \n",
       "1                                      112.510 MW   \n",
       "2                                      225.020 MW   \n",
       "3                                     2250.207 MW   \n",
       "4                                    22502.067 MW   \n",
       "0                                       56.254 MW   \n",
       "1                                      281.275 MW   \n",
       "2                                      562.552 MW   \n",
       "3                                     5625.517 MW   \n",
       "4                                    56255.170 MW   \n",
       "\n",
       "  Total electricity for the entire training (without grad accum)  \n",
       "0                                  2.812650000000 TW              \n",
       "1                                 70.316250000000 TW              \n",
       "2                                281.265000000000 TW              \n",
       "3                              28127.520000000000 TW              \n",
       "4                            2812758.375000000000 TW              \n",
       "0                                  2.812650000000 TW              \n",
       "1                                 70.318640625000 TW              \n",
       "2                                281.274562500000 TW              \n",
       "3                              28127.583750000002 TW              \n",
       "4                            2812758.375000000000 TW              \n",
       "0                                  2.812713750000 TW              \n",
       "1                                 70.318800000000 TW              \n",
       "2                                281.275837500000 TW              \n",
       "3                              28127.583750000002 TW              \n",
       "4                            2812758.502499999944 TW              "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e422830-704f-4af3-9c81-3b6ee3a8eb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c797a-2914-4eb3-bdae-ce7f0026be1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e1305-4452-4fd0-8f4a-b8d8d860058e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aaf18c-6c78-4d21-82c5-15e55063b1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
