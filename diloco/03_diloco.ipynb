{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e47cbb-4314-4afb-9630-30c1af8e9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    get_dataset_size_from_model_size, calculate_total_steps, calculate_total_flops, calculate_flops_per_step,\n",
    "    calculate_num_h100s_per_step, calculate_total_time_to_train_a_model,\n",
    "    compute_minimum_latency_between_clusters, calculate_total_minimum_comm_latency_to_train_a_model,\n",
    "    get_time_per_step, get_training_cost\n",
    ")\n",
    "from utils import (\n",
    "    convert_to_petaflops, convert_to_exaflops, convert_seconds_to_days,\n",
    "    convert_to_xt_format, convert_to_million_format, convert_to_billion_format, convert_seconds_to_years\n",
    ")\n",
    "from constants import UTILIZED_BFLOAT16_FLOPS, H100_COST_PER_HOUR, H100_COST_PER_GPU\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a590a17-2d8b-4534-ae6c-351c97b613fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_sizes = [100*10**9, 500*10**9, 1000*10**9, 10000*10**9, 100000*10**9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834849fb-cabb-45fd-9bdf-c66efdba4e7d",
   "metadata": {},
   "source": [
    "#### Critical batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b372b86-abea-4a51-a2b8-5c4a8ecb7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_critical_batch_size, convert_to_million_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59dde003-8f1c-485d-af88-5617b80cb34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_batch_sizes = [get_critical_batch_size(x) * 4096 for x in target_model_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35783b3-f5d9-45d3-8bff-8d12ab47e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gbs = pd.DataFrame({\n",
    "    'Model Size (B params)': [convert_to_xt_format(x) for x in target_model_sizes],\n",
    "    'Critical Batch Size (tokens)': [f'{x/1e6:1f}M' for x in critical_batch_sizes]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1430d89-1c0c-4a2d-97c6-2370e159545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (B params)</th>\n",
       "      <th>Critical Batch Size (tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>4.740704M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>8.025570M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>10.067991M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>21.381728M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>45.409090M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (B params) Critical Batch Size (tokens)\n",
       "0                  0.1T                    4.740704M\n",
       "1                  0.5T                    8.025570M\n",
       "2                  1.0T                   10.067991M\n",
       "3                 10.0T                   21.381728M\n",
       "4                100.0T                   45.409090M"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11fc98-3b28-48b1-a575-45a71cf58a02",
   "metadata": {},
   "source": [
    "#### Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "789c4b71-6311-421f-9da8-7c837b031e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100b to 100T\n",
    "global_batch_sizes = [x*10**6 for x in [2, 10, 40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f02f48e-3d76-4f34-9fd4-b06a038ddc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_step = 1 # the total time of a fwd, and bwd pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1553384-116a-4d25-b733-6462692bf453",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_compute = []\n",
    "\n",
    "for global_batch_size in global_batch_sizes:\n",
    "    data_compute = {\n",
    "        \"Model Size (Params)\": [],\n",
    "        \"Dataset Size (Tokens)\": [],\n",
    "        \"Global Batch Size\": [],\n",
    "        \"Total Steps\": [],\n",
    "        \"Total FLOPs\": [],\n",
    "        \"FLOPs per Step\": [],\n",
    "        \"H100 GPUs Needed\": [],\n",
    "        \"Total H100s cost\": [],\n",
    "        \"Total Training Time without grad accum\": [],\n",
    "        \"Total Training Time with 10 grad accum\": [],\n",
    "        \"Total Training Time with 100 grad accum\": [],\n",
    "        \"Total Training Time with 1000 grad accum\": []\n",
    "    }\n",
    "    \n",
    "    for model_size in target_model_sizes:\n",
    "        dataset_size = get_dataset_size_from_model_size(model_size)\n",
    "        total_steps = calculate_total_steps(model_size, global_batch_size)\n",
    "        total_flops = calculate_total_flops(model_size)\n",
    "        flops_per_step = calculate_flops_per_step(model_size, global_batch_size)\n",
    "        h100s_per_step = calculate_num_h100s_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "\n",
    "        time_per_step = get_time_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "        total_time = calculate_total_time_to_train_a_model(model_size, global_batch_size, time_per_step)\n",
    "        total_training_cost = get_training_cost(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS, H100_COST_PER_HOUR)\n",
    "        \n",
    "        data_compute[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "        data_compute[\"Dataset Size (Tokens)\"].append(convert_to_xt_format(dataset_size))\n",
    "        data_compute[\"Global Batch Size\"].append(f'{global_batch_size/1e6}M')\n",
    "        data_compute[\"Total Steps\"].append(\"{:,}\".format(total_steps))\n",
    "        data_compute[\"Total FLOPs\"].append(convert_to_exaflops(total_flops))\n",
    "        data_compute[\"FLOPs per Step\"].append(convert_to_petaflops(flops_per_step))\n",
    "        data_compute[\"H100 GPUs Needed\"].append(h100s_per_step)\n",
    "        # data_compute[\"Total H100s cost\"].append(convert_to_billion_format(h100s_per_step * H100_COST_PER_GPU))\n",
    "        data_compute[\"Total H100s cost\"].append(convert_to_million_format(total_training_cost))\n",
    "        data_compute[\"Total Training Time without grad accum\"].append(convert_seconds_to_days(total_time))\n",
    "        data_compute[\"Total Training Time with 10 grad accum\"].append(f\"{convert_seconds_to_years(total_time*10)} - {h100s_per_step/10} gpus\")\n",
    "        data_compute[\"Total Training Time with 100 grad accum\"].append(f\"{convert_seconds_to_years(total_time*100)} - {h100s_per_step/100} gpus\")\n",
    "        data_compute[\"Total Training Time with 1000 grad accum\"].append(f\"{convert_seconds_to_years(total_time*1000)} - {h100s_per_step/1000} gpus\")\n",
    "    \n",
    "    df = pd.DataFrame(data_compute)\n",
    "    # Add batch size information\n",
    "    # df['Global Batch Size'] = f'{global_batch_size/1e6}M'\n",
    "    dataframes_compute.append(df)\n",
    "\n",
    "final_df_compute = pd.DataFrame()\n",
    "for i, df in enumerate(dataframes_compute):\n",
    "    final_df_compute = pd.concat([final_df_compute, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6188ca7e-09c0-4dee-b71f-75c4d0f56f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Dataset Size (Tokens)</th>\n",
       "      <th>Global Batch Size</th>\n",
       "      <th>Total Steps</th>\n",
       "      <th>Total FLOPs</th>\n",
       "      <th>FLOPs per Step</th>\n",
       "      <th>H100 GPUs Needed</th>\n",
       "      <th>Total H100s cost</th>\n",
       "      <th>Total Training Time without grad accum</th>\n",
       "      <th>Total Training Time with 10 grad accum</th>\n",
       "      <th>Total Training Time with 100 grad accum</th>\n",
       "      <th>Total Training Time with 1000 grad accum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1,000,000</td>\n",
       "      <td>1,200,000.0 EFLOPs</td>\n",
       "      <td>1,200.0 PFLOPs</td>\n",
       "      <td>2206.0</td>\n",
       "      <td>1.2m</td>\n",
       "      <td>11.6 days</td>\n",
       "      <td>0.32 years - 220.6 gpus</td>\n",
       "      <td>3.17 years - 22.06 gpus</td>\n",
       "      <td>31.69 years - 2.206 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>5,000,000</td>\n",
       "      <td>30,000,000.0 EFLOPs</td>\n",
       "      <td>6,000.0 PFLOPs</td>\n",
       "      <td>11030.0</td>\n",
       "      <td>30.6m</td>\n",
       "      <td>57.9 days</td>\n",
       "      <td>1.58 years - 1103.0 gpus</td>\n",
       "      <td>15.84 years - 110.3 gpus</td>\n",
       "      <td>158.45 years - 11.03 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>10,000,000</td>\n",
       "      <td>120,000,000.0 EFLOPs</td>\n",
       "      <td>12,000.0 PFLOPs</td>\n",
       "      <td>22060.0</td>\n",
       "      <td>122.6m</td>\n",
       "      <td>115.7 days</td>\n",
       "      <td>3.17 years - 2206.0 gpus</td>\n",
       "      <td>31.69 years - 220.6 gpus</td>\n",
       "      <td>316.89 years - 22.06 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>100,000,000</td>\n",
       "      <td>12,000,000,000.0 EFLOPs</td>\n",
       "      <td>120,000.0 PFLOPs</td>\n",
       "      <td>220608.0</td>\n",
       "      <td>12,256.0m</td>\n",
       "      <td>1157.4 days</td>\n",
       "      <td>31.69 years - 22060.8 gpus</td>\n",
       "      <td>316.88 years - 2206.08 gpus</td>\n",
       "      <td>3168.82 years - 220.608 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1,000,000,000</td>\n",
       "      <td>1,200,000,000,000.0 EFLOPs</td>\n",
       "      <td>1,200,000.0 PFLOPs</td>\n",
       "      <td>2206085.0</td>\n",
       "      <td>1,225,602.8m</td>\n",
       "      <td>11574.1 days</td>\n",
       "      <td>316.88 years - 220608.5 gpus</td>\n",
       "      <td>3168.81 years - 22060.85 gpus</td>\n",
       "      <td>31688.09 years - 2206.085 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>200,000</td>\n",
       "      <td>1,200,000.0 EFLOPs</td>\n",
       "      <td>6,000.0 PFLOPs</td>\n",
       "      <td>11030.0</td>\n",
       "      <td>1.2m</td>\n",
       "      <td>2.3 days</td>\n",
       "      <td>0.06 years - 1103.0 gpus</td>\n",
       "      <td>0.63 years - 110.3 gpus</td>\n",
       "      <td>6.34 years - 11.03 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>1,000,000</td>\n",
       "      <td>30,000,000.0 EFLOPs</td>\n",
       "      <td>30,000.0 PFLOPs</td>\n",
       "      <td>55152.0</td>\n",
       "      <td>30.6m</td>\n",
       "      <td>11.6 days</td>\n",
       "      <td>0.32 years - 5515.2 gpus</td>\n",
       "      <td>3.17 years - 551.52 gpus</td>\n",
       "      <td>31.69 years - 55.152 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>2,000,000</td>\n",
       "      <td>120,000,000.0 EFLOPs</td>\n",
       "      <td>60,000.0 PFLOPs</td>\n",
       "      <td>110304.0</td>\n",
       "      <td>122.6m</td>\n",
       "      <td>23.1 days</td>\n",
       "      <td>0.63 years - 11030.4 gpus</td>\n",
       "      <td>6.34 years - 1103.04 gpus</td>\n",
       "      <td>63.38 years - 110.304 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>20,000,000</td>\n",
       "      <td>12,000,000,000.0 EFLOPs</td>\n",
       "      <td>600,000.0 PFLOPs</td>\n",
       "      <td>1103042.0</td>\n",
       "      <td>12,256.0m</td>\n",
       "      <td>231.5 days</td>\n",
       "      <td>6.34 years - 110304.2 gpus</td>\n",
       "      <td>63.38 years - 11030.42 gpus</td>\n",
       "      <td>633.76 years - 1103.042 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>200,000,000</td>\n",
       "      <td>1,200,000,000,000.0 EFLOPs</td>\n",
       "      <td>6,000,000.0 PFLOPs</td>\n",
       "      <td>11030425.0</td>\n",
       "      <td>1,225,602.8m</td>\n",
       "      <td>2314.8 days</td>\n",
       "      <td>63.38 years - 1103042.5 gpus</td>\n",
       "      <td>633.76 years - 110304.25 gpus</td>\n",
       "      <td>6337.62 years - 11030.425 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>50,000</td>\n",
       "      <td>1,200,000.0 EFLOPs</td>\n",
       "      <td>24,000.0 PFLOPs</td>\n",
       "      <td>44121.0</td>\n",
       "      <td>1.2m</td>\n",
       "      <td>0.6 days</td>\n",
       "      <td>0.02 years - 4412.1 gpus</td>\n",
       "      <td>0.16 years - 441.21 gpus</td>\n",
       "      <td>1.58 years - 44.121 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>250,000</td>\n",
       "      <td>30,000,000.0 EFLOPs</td>\n",
       "      <td>120,000.0 PFLOPs</td>\n",
       "      <td>220608.0</td>\n",
       "      <td>30.6m</td>\n",
       "      <td>2.9 days</td>\n",
       "      <td>0.08 years - 22060.8 gpus</td>\n",
       "      <td>0.79 years - 2206.08 gpus</td>\n",
       "      <td>7.92 years - 220.608 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>500,000</td>\n",
       "      <td>120,000,000.0 EFLOPs</td>\n",
       "      <td>240,000.0 PFLOPs</td>\n",
       "      <td>441217.0</td>\n",
       "      <td>122.6m</td>\n",
       "      <td>5.8 days</td>\n",
       "      <td>0.16 years - 44121.7 gpus</td>\n",
       "      <td>1.58 years - 4412.17 gpus</td>\n",
       "      <td>15.84 years - 441.217 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>5,000,000</td>\n",
       "      <td>12,000,000,000.0 EFLOPs</td>\n",
       "      <td>2,400,000.0 PFLOPs</td>\n",
       "      <td>4412170.0</td>\n",
       "      <td>12,256.0m</td>\n",
       "      <td>57.9 days</td>\n",
       "      <td>1.58 years - 441217.0 gpus</td>\n",
       "      <td>15.84 years - 44121.7 gpus</td>\n",
       "      <td>158.44 years - 4412.17 gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>50,000,000</td>\n",
       "      <td>1,200,000,000,000.0 EFLOPs</td>\n",
       "      <td>24,000,000.0 PFLOPs</td>\n",
       "      <td>44121702.0</td>\n",
       "      <td>1,225,602.8m</td>\n",
       "      <td>578.7 days</td>\n",
       "      <td>15.84 years - 4412170.2 gpus</td>\n",
       "      <td>158.44 years - 441217.02 gpus</td>\n",
       "      <td>1584.40 years - 44121.702 gpus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (Params) Dataset Size (Tokens) Global Batch Size    Total Steps  \\\n",
       "0                0.1T                  2.0T              2.0M      1,000,000   \n",
       "1                0.5T                 10.0T              2.0M      5,000,000   \n",
       "2                1.0T                 20.0T              2.0M     10,000,000   \n",
       "3               10.0T                200.0T              2.0M    100,000,000   \n",
       "4              100.0T               2000.0T              2.0M  1,000,000,000   \n",
       "0                0.1T                  2.0T             10.0M        200,000   \n",
       "1                0.5T                 10.0T             10.0M      1,000,000   \n",
       "2                1.0T                 20.0T             10.0M      2,000,000   \n",
       "3               10.0T                200.0T             10.0M     20,000,000   \n",
       "4              100.0T               2000.0T             10.0M    200,000,000   \n",
       "0                0.1T                  2.0T             40.0M         50,000   \n",
       "1                0.5T                 10.0T             40.0M        250,000   \n",
       "2                1.0T                 20.0T             40.0M        500,000   \n",
       "3               10.0T                200.0T             40.0M      5,000,000   \n",
       "4              100.0T               2000.0T             40.0M     50,000,000   \n",
       "\n",
       "                  Total FLOPs       FLOPs per Step  H100 GPUs Needed  \\\n",
       "0          1,200,000.0 EFLOPs       1,200.0 PFLOPs            2206.0   \n",
       "1         30,000,000.0 EFLOPs       6,000.0 PFLOPs           11030.0   \n",
       "2        120,000,000.0 EFLOPs      12,000.0 PFLOPs           22060.0   \n",
       "3     12,000,000,000.0 EFLOPs     120,000.0 PFLOPs          220608.0   \n",
       "4  1,200,000,000,000.0 EFLOPs   1,200,000.0 PFLOPs         2206085.0   \n",
       "0          1,200,000.0 EFLOPs       6,000.0 PFLOPs           11030.0   \n",
       "1         30,000,000.0 EFLOPs      30,000.0 PFLOPs           55152.0   \n",
       "2        120,000,000.0 EFLOPs      60,000.0 PFLOPs          110304.0   \n",
       "3     12,000,000,000.0 EFLOPs     600,000.0 PFLOPs         1103042.0   \n",
       "4  1,200,000,000,000.0 EFLOPs   6,000,000.0 PFLOPs        11030425.0   \n",
       "0          1,200,000.0 EFLOPs      24,000.0 PFLOPs           44121.0   \n",
       "1         30,000,000.0 EFLOPs     120,000.0 PFLOPs          220608.0   \n",
       "2        120,000,000.0 EFLOPs     240,000.0 PFLOPs          441217.0   \n",
       "3     12,000,000,000.0 EFLOPs   2,400,000.0 PFLOPs         4412170.0   \n",
       "4  1,200,000,000,000.0 EFLOPs  24,000,000.0 PFLOPs        44121702.0   \n",
       "\n",
       "  Total H100s cost Total Training Time without grad accum  \\\n",
       "0             1.2m                              11.6 days   \n",
       "1            30.6m                              57.9 days   \n",
       "2           122.6m                             115.7 days   \n",
       "3        12,256.0m                            1157.4 days   \n",
       "4     1,225,602.8m                           11574.1 days   \n",
       "0             1.2m                               2.3 days   \n",
       "1            30.6m                              11.6 days   \n",
       "2           122.6m                              23.1 days   \n",
       "3        12,256.0m                             231.5 days   \n",
       "4     1,225,602.8m                            2314.8 days   \n",
       "0             1.2m                               0.6 days   \n",
       "1            30.6m                               2.9 days   \n",
       "2           122.6m                               5.8 days   \n",
       "3        12,256.0m                              57.9 days   \n",
       "4     1,225,602.8m                             578.7 days   \n",
       "\n",
       "  Total Training Time with 10 grad accum  \\\n",
       "0                0.32 years - 220.6 gpus   \n",
       "1               1.58 years - 1103.0 gpus   \n",
       "2               3.17 years - 2206.0 gpus   \n",
       "3             31.69 years - 22060.8 gpus   \n",
       "4           316.88 years - 220608.5 gpus   \n",
       "0               0.06 years - 1103.0 gpus   \n",
       "1               0.32 years - 5515.2 gpus   \n",
       "2              0.63 years - 11030.4 gpus   \n",
       "3             6.34 years - 110304.2 gpus   \n",
       "4           63.38 years - 1103042.5 gpus   \n",
       "0               0.02 years - 4412.1 gpus   \n",
       "1              0.08 years - 22060.8 gpus   \n",
       "2              0.16 years - 44121.7 gpus   \n",
       "3             1.58 years - 441217.0 gpus   \n",
       "4           15.84 years - 4412170.2 gpus   \n",
       "\n",
       "  Total Training Time with 100 grad accum  \\\n",
       "0                 3.17 years - 22.06 gpus   \n",
       "1                15.84 years - 110.3 gpus   \n",
       "2                31.69 years - 220.6 gpus   \n",
       "3             316.88 years - 2206.08 gpus   \n",
       "4           3168.81 years - 22060.85 gpus   \n",
       "0                 0.63 years - 110.3 gpus   \n",
       "1                3.17 years - 551.52 gpus   \n",
       "2               6.34 years - 1103.04 gpus   \n",
       "3             63.38 years - 11030.42 gpus   \n",
       "4           633.76 years - 110304.25 gpus   \n",
       "0                0.16 years - 441.21 gpus   \n",
       "1               0.79 years - 2206.08 gpus   \n",
       "2               1.58 years - 4412.17 gpus   \n",
       "3              15.84 years - 44121.7 gpus   \n",
       "4           158.44 years - 441217.02 gpus   \n",
       "\n",
       "  Total Training Time with 1000 grad accum  \n",
       "0                 31.69 years - 2.206 gpus  \n",
       "1                158.45 years - 11.03 gpus  \n",
       "2                316.89 years - 22.06 gpus  \n",
       "3             3168.82 years - 220.608 gpus  \n",
       "4           31688.09 years - 2206.085 gpus  \n",
       "0                  6.34 years - 11.03 gpus  \n",
       "1                31.69 years - 55.152 gpus  \n",
       "2               63.38 years - 110.304 gpus  \n",
       "3             633.76 years - 1103.042 gpus  \n",
       "4           6337.62 years - 11030.425 gpus  \n",
       "0                 1.58 years - 44.121 gpus  \n",
       "1                7.92 years - 220.608 gpus  \n",
       "2               15.84 years - 441.217 gpus  \n",
       "3              158.44 years - 4412.17 gpus  \n",
       "4           1584.40 years - 44121.702 gpus  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032de16-00f5-4881-8746-97ae7026ecda",
   "metadata": {},
   "source": [
    "##### Communication time of data parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a54207cd-a0b5-4bed-aacd-ff3ee447f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import FP8_BYTES, BFLOAT16_BYTES\n",
    "from utils import convert_bytes_to_terabytes\n",
    "from utils import calculate_comm_time_given_comm_volume, convert_bytes_to_gigabytes\n",
    "from constants import NVLINK_MAX_TOTAL_BANDWIDTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7dd7e3-632b-44d1-bc85-e6ee59495f85",
   "metadata": {},
   "source": [
    "Assume that fwd+bwd pass of a single replicas takes 1 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b702221-3a61-4cf9-9470-9e74aed3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comm_bandwidths = [0.5*1024**3, 1*1024**3, 4*1024**3] # bytes/sec\n",
    "# comm_bandwidths = [40*1024**3, NVLINK_MAX_TOTAL_BANDWIDTH] # bytes/sec\n",
    "comm_bandwidths = [40*1024**3] # bytes/sec\n",
    "cluster_sizes = [1024, 10240, 102400]\n",
    "\n",
    "data_mem = {\n",
    "    \"Model Size (Params)\": [],\n",
    "    \"Global batch size\": [],\n",
    "    # \"Number of datacenters\": [],\n",
    "    \"Total bfloat16 gradient storage\": [],\n",
    "    \"Total fp8 gradient storage\": [],\n",
    "    \"Bandwidth\": [],\n",
    "    \"Total communication time in bfloat16 - comm/compute ratio\": [],\n",
    "    \"Total communication time in fp8 - comm/compute ratio\": [],\n",
    "    \"Total GPU idle cost for bfloat16 comm\": [],\n",
    "    \"Total GPU idle cost for fp8 comm\": [],\n",
    "    \"DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio\": []\n",
    "}\n",
    "# for cluster_size in cluster_sizes:\n",
    "for bandwidth in comm_bandwidths:\n",
    "    for global_batch_size in global_batch_sizes:\n",
    "        for model_size in target_model_sizes:\n",
    "            total_steps = calculate_total_steps(model_size, global_batch_size)\n",
    "            time_per_step = get_time_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "            total_time = calculate_total_time_to_train_a_model(model_size, global_batch_size, time_per_step)\n",
    "            \n",
    "            # h100s_per_step = calculate_num_h100s_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "            # num_clusters = h100s_per_step // cluster_size\n",
    "\n",
    "            bfloat16_grad_comm_volume = model_size * BFLOAT16_BYTES\n",
    "            fp8_grad_comm_volume = model_size * FP8_BYTES\n",
    "            \n",
    "            bfloat16_total_comm_time = calculate_comm_time_given_comm_volume(bfloat16_grad_comm_volume, bandwidth) * total_steps\n",
    "            fp8_total_comm_time = calculate_comm_time_given_comm_volume(fp8_grad_comm_volume, bandwidth) * total_steps\n",
    "            bfloat16_diloco_total_comm_time = calculate_comm_time_given_comm_volume(fp8_grad_comm_volume, bandwidth) * (total_steps / 500)\n",
    "\n",
    "            bfloat16_comm_compute_ratio = (bfloat16_total_comm_time / (total_time + bfloat16_total_comm_time)) * 100\n",
    "            fp8_comm_compute_ratio = (fp8_total_comm_time / (total_time + fp8_total_comm_time)) * 100\n",
    "            bfloat16_diloco_comm_compute_ratio = (bfloat16_diloco_total_comm_time / (total_time + bfloat16_diloco_total_comm_time)) * 100\n",
    "            \n",
    "            bfloat16_total_gpu_idle_cost_comm = ((bfloat16_total_comm_time * h100s_per_step) / (60*60)) / H100_COST_PER_HOUR\n",
    "            fp8_total_gpu_idle_cost_comm = ((fp8_total_comm_time * h100s_per_step) / (60*60)) / H100_COST_PER_HOUR\n",
    "            \n",
    "            data_mem[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "            data_mem[\"Global batch size\"].append(f'{global_batch_size/1e6}M')\n",
    "            # data_mem[\"Number of datacenters\"].append(num_clusters)\n",
    "            data_mem[\"Total bfloat16 gradient storage\"].append(convert_bytes_to_terabytes(model_size * BFLOAT16_BYTES))\n",
    "            data_mem[\"Total fp8 gradient storage\"].append(convert_bytes_to_terabytes(model_size * FP8_BYTES))\n",
    "            data_mem[\"Bandwidth\"].append(f\"{convert_bytes_to_gigabytes(bandwidth)}/s\")\n",
    "            data_mem[\"Total communication time in bfloat16 - comm/compute ratio\"].append(f\"{convert_seconds_to_days(bfloat16_total_comm_time)} / {convert_seconds_to_years(bfloat16_total_comm_time)} - {bfloat16_comm_compute_ratio:.2f}%\")\n",
    "            data_mem[\"Total communication time in fp8 - comm/compute ratio\"].append(f\"{convert_seconds_to_days(bfloat16_total_comm_time)} / {convert_seconds_to_years(fp8_total_comm_time)} - {fp8_comm_compute_ratio:.2f}%\")\n",
    "            data_mem[\"Total GPU idle cost for bfloat16 comm\"].append(convert_to_billion_format(bfloat16_total_gpu_idle_cost_comm))\n",
    "            data_mem[\"Total GPU idle cost for fp8 comm\"].append(convert_to_billion_format(fp8_total_gpu_idle_cost_comm))\n",
    "            data_mem[\"DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio\"].append(f\"{convert_seconds_to_days(bfloat16_diloco_total_comm_time)}  - {bfloat16_diloco_comm_compute_ratio:.2f}%\")\n",
    "    \n",
    "    df_mem = pd.DataFrame(data_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d602d39-1e4e-4f90-a574-509e46b83733",
   "metadata": {},
   "source": [
    "Add cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b92fc56-67f2-4502-a4dd-36e6f4c612fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Global batch size</th>\n",
       "      <th>Total bfloat16 gradient storage</th>\n",
       "      <th>Total fp8 gradient storage</th>\n",
       "      <th>Bandwidth</th>\n",
       "      <th>Total communication time in bfloat16 - comm/compute ratio</th>\n",
       "      <th>Total communication time in fp8 - comm/compute ratio</th>\n",
       "      <th>Total GPU idle cost for bfloat16 comm</th>\n",
       "      <th>Total GPU idle cost for fp8 comm</th>\n",
       "      <th>DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>0.200 TB</td>\n",
       "      <td>0.100 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>53.9 days / 0.15 years - 82.32%</td>\n",
       "      <td>53.9 days / 0.07 years - 69.95%</td>\n",
       "      <td>28.54B</td>\n",
       "      <td>14.27B</td>\n",
       "      <td>0.1 days  - 0.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>0.500 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>1347.4 days / 3.69 years - 95.88%</td>\n",
       "      <td>1347.4 days / 1.84 years - 92.09%</td>\n",
       "      <td>713.39B</td>\n",
       "      <td>356.70B</td>\n",
       "      <td>1.3 days  - 2.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>2.000 TB</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>5389.6 days / 14.76 years - 97.90%</td>\n",
       "      <td>5389.6 days / 7.38 years - 95.88%</td>\n",
       "      <td>2,853.58B</td>\n",
       "      <td>1,426.79B</td>\n",
       "      <td>5.4 days  - 4.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>20.000 TB</td>\n",
       "      <td>10.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>538959.8 days / 1475.59 years - 99.79%</td>\n",
       "      <td>538959.8 days / 737.80 years - 99.57%</td>\n",
       "      <td>285,357.90B</td>\n",
       "      <td>142,678.95B</td>\n",
       "      <td>539.0 days  - 31.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>200.000 TB</td>\n",
       "      <td>100.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>53895982.3 days / 147559.16 years - 99.98%</td>\n",
       "      <td>53895982.3 days / 73779.58 years - 99.96%</td>\n",
       "      <td>28,535,789.65B</td>\n",
       "      <td>14,267,894.83B</td>\n",
       "      <td>53896.0 days  - 82.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>0.200 TB</td>\n",
       "      <td>0.100 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>10.8 days / 0.03 years - 82.32%</td>\n",
       "      <td>10.8 days / 0.01 years - 69.95%</td>\n",
       "      <td>5.71B</td>\n",
       "      <td>2.85B</td>\n",
       "      <td>0.0 days  - 0.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>0.500 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>269.5 days / 0.74 years - 95.88%</td>\n",
       "      <td>269.5 days / 0.37 years - 92.09%</td>\n",
       "      <td>142.68B</td>\n",
       "      <td>71.34B</td>\n",
       "      <td>0.3 days  - 2.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>2.000 TB</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>1077.9 days / 2.95 years - 97.90%</td>\n",
       "      <td>1077.9 days / 1.48 years - 95.88%</td>\n",
       "      <td>570.72B</td>\n",
       "      <td>285.36B</td>\n",
       "      <td>1.1 days  - 4.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>20.000 TB</td>\n",
       "      <td>10.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>107792.0 days / 295.12 years - 99.79%</td>\n",
       "      <td>107792.0 days / 147.56 years - 99.57%</td>\n",
       "      <td>57,071.58B</td>\n",
       "      <td>28,535.79B</td>\n",
       "      <td>107.8 days  - 31.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>200.000 TB</td>\n",
       "      <td>100.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>10779196.5 days / 29511.83 years - 99.98%</td>\n",
       "      <td>10779196.5 days / 14755.92 years - 99.96%</td>\n",
       "      <td>5,707,157.93B</td>\n",
       "      <td>2,853,578.97B</td>\n",
       "      <td>10779.2 days  - 82.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>0.200 TB</td>\n",
       "      <td>0.100 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>2.7 days / 0.01 years - 82.32%</td>\n",
       "      <td>2.7 days / 0.00 years - 69.95%</td>\n",
       "      <td>1.43B</td>\n",
       "      <td>0.71B</td>\n",
       "      <td>0.0 days  - 0.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>0.500 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>67.4 days / 0.18 years - 95.88%</td>\n",
       "      <td>67.4 days / 0.09 years - 92.09%</td>\n",
       "      <td>35.67B</td>\n",
       "      <td>17.83B</td>\n",
       "      <td>0.1 days  - 2.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>2.000 TB</td>\n",
       "      <td>1.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>269.5 days / 0.74 years - 97.90%</td>\n",
       "      <td>269.5 days / 0.37 years - 95.88%</td>\n",
       "      <td>142.68B</td>\n",
       "      <td>71.34B</td>\n",
       "      <td>0.3 days  - 4.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>20.000 TB</td>\n",
       "      <td>10.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>26948.0 days / 73.78 years - 99.79%</td>\n",
       "      <td>26948.0 days / 36.89 years - 99.57%</td>\n",
       "      <td>14,267.89B</td>\n",
       "      <td>7,133.95B</td>\n",
       "      <td>26.9 days  - 31.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>200.000 TB</td>\n",
       "      <td>100.000 TB</td>\n",
       "      <td>42.950 GB/s</td>\n",
       "      <td>2694799.1 days / 7377.96 years - 99.98%</td>\n",
       "      <td>2694799.1 days / 3688.98 years - 99.96%</td>\n",
       "      <td>1,426,789.48B</td>\n",
       "      <td>713,394.74B</td>\n",
       "      <td>2694.8 days  - 82.32%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Size (Params) Global batch size Total bfloat16 gradient storage  \\\n",
       "0                 0.1T              2.0M                        0.200 TB   \n",
       "1                 0.5T              2.0M                        1.000 TB   \n",
       "2                 1.0T              2.0M                        2.000 TB   \n",
       "3                10.0T              2.0M                       20.000 TB   \n",
       "4               100.0T              2.0M                      200.000 TB   \n",
       "5                 0.1T             10.0M                        0.200 TB   \n",
       "6                 0.5T             10.0M                        1.000 TB   \n",
       "7                 1.0T             10.0M                        2.000 TB   \n",
       "8                10.0T             10.0M                       20.000 TB   \n",
       "9               100.0T             10.0M                      200.000 TB   \n",
       "10                0.1T             40.0M                        0.200 TB   \n",
       "11                0.5T             40.0M                        1.000 TB   \n",
       "12                1.0T             40.0M                        2.000 TB   \n",
       "13               10.0T             40.0M                       20.000 TB   \n",
       "14              100.0T             40.0M                      200.000 TB   \n",
       "\n",
       "   Total fp8 gradient storage    Bandwidth  \\\n",
       "0                    0.100 TB  42.950 GB/s   \n",
       "1                    0.500 TB  42.950 GB/s   \n",
       "2                    1.000 TB  42.950 GB/s   \n",
       "3                   10.000 TB  42.950 GB/s   \n",
       "4                  100.000 TB  42.950 GB/s   \n",
       "5                    0.100 TB  42.950 GB/s   \n",
       "6                    0.500 TB  42.950 GB/s   \n",
       "7                    1.000 TB  42.950 GB/s   \n",
       "8                   10.000 TB  42.950 GB/s   \n",
       "9                  100.000 TB  42.950 GB/s   \n",
       "10                   0.100 TB  42.950 GB/s   \n",
       "11                   0.500 TB  42.950 GB/s   \n",
       "12                   1.000 TB  42.950 GB/s   \n",
       "13                  10.000 TB  42.950 GB/s   \n",
       "14                 100.000 TB  42.950 GB/s   \n",
       "\n",
       "   Total communication time in bfloat16 - comm/compute ratio  \\\n",
       "0                     53.9 days / 0.15 years - 82.32%          \n",
       "1                   1347.4 days / 3.69 years - 95.88%          \n",
       "2                  5389.6 days / 14.76 years - 97.90%          \n",
       "3              538959.8 days / 1475.59 years - 99.79%          \n",
       "4          53895982.3 days / 147559.16 years - 99.98%          \n",
       "5                     10.8 days / 0.03 years - 82.32%          \n",
       "6                    269.5 days / 0.74 years - 95.88%          \n",
       "7                   1077.9 days / 2.95 years - 97.90%          \n",
       "8               107792.0 days / 295.12 years - 99.79%          \n",
       "9           10779196.5 days / 29511.83 years - 99.98%          \n",
       "10                     2.7 days / 0.01 years - 82.32%          \n",
       "11                    67.4 days / 0.18 years - 95.88%          \n",
       "12                   269.5 days / 0.74 years - 97.90%          \n",
       "13                26948.0 days / 73.78 years - 99.79%          \n",
       "14            2694799.1 days / 7377.96 years - 99.98%          \n",
       "\n",
       "   Total communication time in fp8 - comm/compute ratio  \\\n",
       "0                     53.9 days / 0.07 years - 69.95%     \n",
       "1                   1347.4 days / 1.84 years - 92.09%     \n",
       "2                   5389.6 days / 7.38 years - 95.88%     \n",
       "3               538959.8 days / 737.80 years - 99.57%     \n",
       "4           53895982.3 days / 73779.58 years - 99.96%     \n",
       "5                     10.8 days / 0.01 years - 69.95%     \n",
       "6                    269.5 days / 0.37 years - 92.09%     \n",
       "7                   1077.9 days / 1.48 years - 95.88%     \n",
       "8               107792.0 days / 147.56 years - 99.57%     \n",
       "9           10779196.5 days / 14755.92 years - 99.96%     \n",
       "10                     2.7 days / 0.00 years - 69.95%     \n",
       "11                    67.4 days / 0.09 years - 92.09%     \n",
       "12                   269.5 days / 0.37 years - 95.88%     \n",
       "13                26948.0 days / 36.89 years - 99.57%     \n",
       "14            2694799.1 days / 3688.98 years - 99.96%     \n",
       "\n",
       "   Total GPU idle cost for bfloat16 comm Total GPU idle cost for fp8 comm  \\\n",
       "0                                 28.54B                           14.27B   \n",
       "1                                713.39B                          356.70B   \n",
       "2                              2,853.58B                        1,426.79B   \n",
       "3                            285,357.90B                      142,678.95B   \n",
       "4                         28,535,789.65B                   14,267,894.83B   \n",
       "5                                  5.71B                            2.85B   \n",
       "6                                142.68B                           71.34B   \n",
       "7                                570.72B                          285.36B   \n",
       "8                             57,071.58B                       28,535.79B   \n",
       "9                          5,707,157.93B                    2,853,578.97B   \n",
       "10                                 1.43B                            0.71B   \n",
       "11                                35.67B                           17.83B   \n",
       "12                               142.68B                           71.34B   \n",
       "13                            14,267.89B                        7,133.95B   \n",
       "14                         1,426,789.48B                      713,394.74B   \n",
       "\n",
       "   DiLoCo's total communication time in bfloat16 (500 inner steps) - comm/compute ratio  \n",
       "0                                   0.1 days  - 0.46%                                    \n",
       "1                                   1.3 days  - 2.28%                                    \n",
       "2                                   5.4 days  - 4.45%                                    \n",
       "3                                539.0 days  - 31.77%                                    \n",
       "4                              53896.0 days  - 82.32%                                    \n",
       "5                                   0.0 days  - 0.46%                                    \n",
       "6                                   0.3 days  - 2.28%                                    \n",
       "7                                   1.1 days  - 4.45%                                    \n",
       "8                                107.8 days  - 31.77%                                    \n",
       "9                              10779.2 days  - 82.32%                                    \n",
       "10                                  0.0 days  - 0.46%                                    \n",
       "11                                  0.1 days  - 2.28%                                    \n",
       "12                                  0.3 days  - 4.45%                                    \n",
       "13                                26.9 days  - 31.77%                                    \n",
       "14                              2694.8 days  - 82.32%                                    "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97153b-7c80-45de-bffc-31dd0caa676d",
   "metadata": {},
   "source": [
    "#### Communication latency (theoretical minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ba983-2882-48d4-9541-1bd8faabfddb",
   "metadata": {},
   "source": [
    "Assumptions on communication\n",
    "- No limit on banwidth\n",
    "- Achieve speed of light\n",
    "- Clostest surface distance between two points on the earth surface (assume you don't dig a crazy hole to go a straight line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b40c02-2ac3-4dce-947e-a4915b7518f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_latency_between_jz_and_jc = compute_minimum_latency_between_clusters(\"JEAN_ZAY\", \"JOLIOT_CURIE\")\n",
    "minimum_latency_between_jz_and_ec = compute_minimum_latency_between_clusters(\"JEAN_ZAY\", \"EL_CAPITAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ba98b7a-e713-446d-9dad-613b76d447e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_comm = []\n",
    "\n",
    "for global_batch_size in global_batch_sizes:\n",
    "    data_comm = {\n",
    "        \"Model Size (Params)\": [],\n",
    "        \"Dataset Size (Tokens)\": [],\n",
    "        \"Global Batch Size\": [],\n",
    "        \n",
    "        \"Total minimum communication latency between JZ and JC\": [],\n",
    "        \"Total GPU idle time for minimum comm between JZ and JC\": [],\n",
    "        \"GPU idle cost during JZ-JC minimum communication latency\": [],\n",
    "        \n",
    "        \"Total minimum communication latency between JZ and EC\": [],\n",
    "        \"Total GPU idle time for minimum comm between JZ and EC\": [],\n",
    "        \"GPU idle cost during JZ-EC minimum communication latency\": [],\n",
    "    }\n",
    "    \n",
    "    for model_size in target_model_sizes:\n",
    "        dataset_size = get_dataset_size_from_model_size(model_size)\n",
    "        # Append to dictionary\n",
    "        data_comm[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "        data_comm[\"Dataset Size (Tokens)\"].append(convert_to_xt_format(dataset_size))\n",
    "        data_comm[\"Global Batch Size\"].append(f'{global_batch_size/1e6}M')\n",
    "        \n",
    "        data_comm[\"Total minimum communication latency between JZ and JC\"].append(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc))\n",
    "        data_comm[\"Total GPU idle time for minimum comm between JZ and JC\"].append(convert_seconds_to_days(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc) * h100s_per_step))\n",
    "        data_comm[\"GPU idle cost during JZ-JC minimum communication latency\"].append(convert_to_million_format((calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_jc) / (60 * 60)) * h100s_per_step * H100_COST_PER_HOUR))\n",
    "        \n",
    "        data_comm[\"Total minimum communication latency between JZ and EC\"].append(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_ec))\n",
    "        data_comm[\"Total GPU idle time for minimum comm between JZ and EC\"].append(convert_seconds_to_years(calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_ec) * h100s_per_step))\n",
    "        data_comm[\"GPU idle cost during JZ-EC minimum communication latency\"].append(convert_to_million_format((calculate_total_minimum_comm_latency_to_train_a_model(model_size, global_batch_size, minimum_latency_between_jz_and_ec) / (60 * 60)) * h100s_per_step * H100_COST_PER_HOUR))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_comm = pd.DataFrame(data_comm)\n",
    "    # df_comm['Global Batch Size'] = f'{global_batch_size/1e6}M'\n",
    "    dataframes_comm.append(df_comm)\n",
    "\n",
    "final_df_comm = pd.DataFrame()\n",
    "for i, df in enumerate(dataframes_comm):\n",
    "    final_df_comm = pd.concat([final_df_comm, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e897a27-98f8-43a8-aec6-55310d7da1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Dataset Size (Tokens)</th>\n",
       "      <th>Global Batch Size</th>\n",
       "      <th>Total minimum communication latency between JZ and JC</th>\n",
       "      <th>Total GPU idle time for minimum comm between JZ and JC</th>\n",
       "      <th>GPU idle cost during JZ-JC minimum communication latency</th>\n",
       "      <th>Total minimum communication latency between JZ and EC</th>\n",
       "      <th>Total GPU idle time for minimum comm between JZ and EC</th>\n",
       "      <th>GPU idle cost during JZ-EC minimum communication latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1.278270</td>\n",
       "      <td>652.8 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>29.790336</td>\n",
       "      <td>41.65 years</td>\n",
       "      <td>0.7m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>6.391352</td>\n",
       "      <td>3263.9 days</td>\n",
       "      <td>0.2m</td>\n",
       "      <td>148.951681</td>\n",
       "      <td>208.25 years</td>\n",
       "      <td>3.7m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>12.782705</td>\n",
       "      <td>6527.7 days</td>\n",
       "      <td>0.3m</td>\n",
       "      <td>297.903362</td>\n",
       "      <td>416.51 years</td>\n",
       "      <td>7.3m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>127.827046</td>\n",
       "      <td>65277.2 days</td>\n",
       "      <td>3.1m</td>\n",
       "      <td>2979.033618</td>\n",
       "      <td>4165.08 years</td>\n",
       "      <td>73.0m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>1278.270459</td>\n",
       "      <td>652771.6 days</td>\n",
       "      <td>31.3m</td>\n",
       "      <td>29790.336181</td>\n",
       "      <td>41650.83 years</td>\n",
       "      <td>730.2m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>0.255654</td>\n",
       "      <td>130.6 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>5.958067</td>\n",
       "      <td>8.33 years</td>\n",
       "      <td>0.1m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>1.278270</td>\n",
       "      <td>652.8 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>29.790336</td>\n",
       "      <td>41.65 years</td>\n",
       "      <td>0.7m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>2.556541</td>\n",
       "      <td>1305.5 days</td>\n",
       "      <td>0.1m</td>\n",
       "      <td>59.580672</td>\n",
       "      <td>83.30 years</td>\n",
       "      <td>1.5m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>25.565409</td>\n",
       "      <td>13055.4 days</td>\n",
       "      <td>0.6m</td>\n",
       "      <td>595.806724</td>\n",
       "      <td>833.02 years</td>\n",
       "      <td>14.6m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>255.654092</td>\n",
       "      <td>130554.3 days</td>\n",
       "      <td>6.3m</td>\n",
       "      <td>5958.067236</td>\n",
       "      <td>8330.17 years</td>\n",
       "      <td>146.0m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>0.063914</td>\n",
       "      <td>32.6 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>1.489517</td>\n",
       "      <td>2.08 years</td>\n",
       "      <td>0.0m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>0.319568</td>\n",
       "      <td>163.2 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>7.447584</td>\n",
       "      <td>10.41 years</td>\n",
       "      <td>0.2m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>0.639135</td>\n",
       "      <td>326.4 days</td>\n",
       "      <td>0.0m</td>\n",
       "      <td>14.895168</td>\n",
       "      <td>20.83 years</td>\n",
       "      <td>0.4m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>6.391352</td>\n",
       "      <td>3263.9 days</td>\n",
       "      <td>0.2m</td>\n",
       "      <td>148.951681</td>\n",
       "      <td>208.25 years</td>\n",
       "      <td>3.7m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>63.913523</td>\n",
       "      <td>32638.6 days</td>\n",
       "      <td>1.6m</td>\n",
       "      <td>1489.516809</td>\n",
       "      <td>2082.54 years</td>\n",
       "      <td>36.5m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (Params) Dataset Size (Tokens) Global Batch Size  \\\n",
       "0                0.1T                  2.0T              2.0M   \n",
       "1                0.5T                 10.0T              2.0M   \n",
       "2                1.0T                 20.0T              2.0M   \n",
       "3               10.0T                200.0T              2.0M   \n",
       "4              100.0T               2000.0T              2.0M   \n",
       "0                0.1T                  2.0T             10.0M   \n",
       "1                0.5T                 10.0T             10.0M   \n",
       "2                1.0T                 20.0T             10.0M   \n",
       "3               10.0T                200.0T             10.0M   \n",
       "4              100.0T               2000.0T             10.0M   \n",
       "0                0.1T                  2.0T             40.0M   \n",
       "1                0.5T                 10.0T             40.0M   \n",
       "2                1.0T                 20.0T             40.0M   \n",
       "3               10.0T                200.0T             40.0M   \n",
       "4              100.0T               2000.0T             40.0M   \n",
       "\n",
       "   Total minimum communication latency between JZ and JC  \\\n",
       "0                                           1.278270       \n",
       "1                                           6.391352       \n",
       "2                                          12.782705       \n",
       "3                                         127.827046       \n",
       "4                                        1278.270459       \n",
       "0                                           0.255654       \n",
       "1                                           1.278270       \n",
       "2                                           2.556541       \n",
       "3                                          25.565409       \n",
       "4                                         255.654092       \n",
       "0                                           0.063914       \n",
       "1                                           0.319568       \n",
       "2                                           0.639135       \n",
       "3                                           6.391352       \n",
       "4                                          63.913523       \n",
       "\n",
       "  Total GPU idle time for minimum comm between JZ and JC  \\\n",
       "0                                         652.8 days       \n",
       "1                                        3263.9 days       \n",
       "2                                        6527.7 days       \n",
       "3                                       65277.2 days       \n",
       "4                                      652771.6 days       \n",
       "0                                         130.6 days       \n",
       "1                                         652.8 days       \n",
       "2                                        1305.5 days       \n",
       "3                                       13055.4 days       \n",
       "4                                      130554.3 days       \n",
       "0                                          32.6 days       \n",
       "1                                         163.2 days       \n",
       "2                                         326.4 days       \n",
       "3                                        3263.9 days       \n",
       "4                                       32638.6 days       \n",
       "\n",
       "  GPU idle cost during JZ-JC minimum communication latency  \\\n",
       "0                                               0.0m         \n",
       "1                                               0.2m         \n",
       "2                                               0.3m         \n",
       "3                                               3.1m         \n",
       "4                                              31.3m         \n",
       "0                                               0.0m         \n",
       "1                                               0.0m         \n",
       "2                                               0.1m         \n",
       "3                                               0.6m         \n",
       "4                                               6.3m         \n",
       "0                                               0.0m         \n",
       "1                                               0.0m         \n",
       "2                                               0.0m         \n",
       "3                                               0.2m         \n",
       "4                                               1.6m         \n",
       "\n",
       "   Total minimum communication latency between JZ and EC  \\\n",
       "0                                          29.790336       \n",
       "1                                         148.951681       \n",
       "2                                         297.903362       \n",
       "3                                        2979.033618       \n",
       "4                                       29790.336181       \n",
       "0                                           5.958067       \n",
       "1                                          29.790336       \n",
       "2                                          59.580672       \n",
       "3                                         595.806724       \n",
       "4                                        5958.067236       \n",
       "0                                           1.489517       \n",
       "1                                           7.447584       \n",
       "2                                          14.895168       \n",
       "3                                         148.951681       \n",
       "4                                        1489.516809       \n",
       "\n",
       "  Total GPU idle time for minimum comm between JZ and EC  \\\n",
       "0                                        41.65 years       \n",
       "1                                       208.25 years       \n",
       "2                                       416.51 years       \n",
       "3                                      4165.08 years       \n",
       "4                                     41650.83 years       \n",
       "0                                         8.33 years       \n",
       "1                                        41.65 years       \n",
       "2                                        83.30 years       \n",
       "3                                       833.02 years       \n",
       "4                                      8330.17 years       \n",
       "0                                         2.08 years       \n",
       "1                                        10.41 years       \n",
       "2                                        20.83 years       \n",
       "3                                       208.25 years       \n",
       "4                                      2082.54 years       \n",
       "\n",
       "  GPU idle cost during JZ-EC minimum communication latency  \n",
       "0                                               0.7m        \n",
       "1                                               3.7m        \n",
       "2                                               7.3m        \n",
       "3                                              73.0m        \n",
       "4                                             730.2m        \n",
       "0                                               0.1m        \n",
       "1                                               0.7m        \n",
       "2                                               1.5m        \n",
       "3                                              14.6m        \n",
       "4                                             146.0m        \n",
       "0                                               0.0m        \n",
       "1                                               0.2m        \n",
       "2                                               0.4m        \n",
       "3                                               3.7m        \n",
       "4                                              36.5m        "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a547c9e-def0-4c82-a541-517ec6d127c6",
   "metadata": {},
   "source": [
    "#### Electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2735f7c7-3210-414b-9590-b2c3a0620355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import TOTAL_H100_WATT\n",
    "from utils import convert_watts_to_megawatts, convert_watts_to_terawatts, calculate_electricity_consumption_of_an_h100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8857f303-4c68-4db4-9912-9f1fdd3afa8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataframes_elec = []\n",
    "\n",
    "for global_batch_size in global_batch_sizes:\n",
    "    data_elec = {\n",
    "        \"Model Size (Params)\": [],\n",
    "        \"Dataset Size (Tokens)\": [],\n",
    "        \"Global batch size\": [],\n",
    "        \"Number of GPUs\": [],\n",
    "        \"Total electricity per step (without grad accum)\": [],\n",
    "        \"Total electricity for the entire training (without grad accum)\": []\n",
    "    }\n",
    "    \n",
    "    for model_size in target_model_sizes:\n",
    "        dataset_size = get_dataset_size_from_model_size(model_size)\n",
    "        h100s_per_step = calculate_num_h100s_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "        time_per_step = get_time_per_step(model_size, global_batch_size, UTILIZED_BFLOAT16_FLOPS)\n",
    "        total_time = calculate_total_time_to_train_a_model(model_size, global_batch_size, time_per_step)\n",
    "        total_electricity_consumption = calculate_electricity_consumption_of_an_h100(TOTAL_H100_WATT, total_time) * h100s_per_step\n",
    "        \n",
    "        data_elec[\"Model Size (Params)\"].append(convert_to_xt_format(model_size))\n",
    "        data_elec[\"Dataset Size (Tokens)\"].append(convert_to_xt_format(dataset_size))\n",
    "        data_elec[\"Global batch size\"].append(f'{global_batch_size/1e6}M')\n",
    "        data_elec[\"Number of GPUs\"].append(h100s_per_step)\n",
    "        data_elec[\"Total electricity per step (without grad accum)\"].append(convert_watts_to_megawatts(h100s_per_step * TOTAL_H100_WATT))\n",
    "        data_elec[\"Total electricity for the entire training (without grad accum)\"].append(f\"{convert_watts_to_terawatts(total_electricity_consumption)}\")\n",
    "    \n",
    "    df = pd.DataFrame(data_elec)\n",
    "    # df['Global Batch Size'] = f'{global_batch_size/1e6}M'\n",
    "    dataframes_elec.append(df)\n",
    "\n",
    "final_df_elec = pd.DataFrame()\n",
    "for i, df in enumerate(dataframes_elec):\n",
    "    final_df_elec = pd.concat([final_df_elec, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96921a0d-6fc4-471b-a0d7-47e1c5ecd808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my calculation closes to 100k gpu cluster's electricity: https://semianalysis.com/2024/06/17/100000-h100-clusters-power-network/#power-challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8152cae-4337-4743-a9ba-e9fe58fa946a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Size (Params)</th>\n",
       "      <th>Dataset Size (Tokens)</th>\n",
       "      <th>Global batch size</th>\n",
       "      <th>Number of GPUs</th>\n",
       "      <th>Total electricity per step (without grad accum)</th>\n",
       "      <th>Total electricity for the entire training (without grad accum)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>2206.0</td>\n",
       "      <td>2.813 MW</td>\n",
       "      <td>2.813 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>11030.0</td>\n",
       "      <td>14.063 MW</td>\n",
       "      <td>70.319 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>22060.0</td>\n",
       "      <td>28.127 MW</td>\n",
       "      <td>281.276 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>220608.0</td>\n",
       "      <td>281.275 MW</td>\n",
       "      <td>28127.585 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>2.0M</td>\n",
       "      <td>2206085.0</td>\n",
       "      <td>2812.758 MW</td>\n",
       "      <td>2812758.526 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>11030.0</td>\n",
       "      <td>14.063 MW</td>\n",
       "      <td>2.813 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>55152.0</td>\n",
       "      <td>70.319 MW</td>\n",
       "      <td>70.319 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>110304.0</td>\n",
       "      <td>140.638 MW</td>\n",
       "      <td>281.276 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>1103042.0</td>\n",
       "      <td>1406.379 MW</td>\n",
       "      <td>28127.585 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>10.0M</td>\n",
       "      <td>11030425.0</td>\n",
       "      <td>14063.792 MW</td>\n",
       "      <td>2812758.526 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1T</td>\n",
       "      <td>2.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>44121.0</td>\n",
       "      <td>56.254 MW</td>\n",
       "      <td>2.813 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5T</td>\n",
       "      <td>10.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>220608.0</td>\n",
       "      <td>281.275 MW</td>\n",
       "      <td>70.319 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0T</td>\n",
       "      <td>20.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>441217.0</td>\n",
       "      <td>562.552 MW</td>\n",
       "      <td>281.276 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0T</td>\n",
       "      <td>200.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>4412170.0</td>\n",
       "      <td>5625.517 MW</td>\n",
       "      <td>28127.585 TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0T</td>\n",
       "      <td>2000.0T</td>\n",
       "      <td>40.0M</td>\n",
       "      <td>44121702.0</td>\n",
       "      <td>56255.170 MW</td>\n",
       "      <td>2812758.526 TW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Size (Params) Dataset Size (Tokens) Global batch size  Number of GPUs  \\\n",
       "0                0.1T                  2.0T              2.0M          2206.0   \n",
       "1                0.5T                 10.0T              2.0M         11030.0   \n",
       "2                1.0T                 20.0T              2.0M         22060.0   \n",
       "3               10.0T                200.0T              2.0M        220608.0   \n",
       "4              100.0T               2000.0T              2.0M       2206085.0   \n",
       "0                0.1T                  2.0T             10.0M         11030.0   \n",
       "1                0.5T                 10.0T             10.0M         55152.0   \n",
       "2                1.0T                 20.0T             10.0M        110304.0   \n",
       "3               10.0T                200.0T             10.0M       1103042.0   \n",
       "4              100.0T               2000.0T             10.0M      11030425.0   \n",
       "0                0.1T                  2.0T             40.0M         44121.0   \n",
       "1                0.5T                 10.0T             40.0M        220608.0   \n",
       "2                1.0T                 20.0T             40.0M        441217.0   \n",
       "3               10.0T                200.0T             40.0M       4412170.0   \n",
       "4              100.0T               2000.0T             40.0M      44121702.0   \n",
       "\n",
       "  Total electricity per step (without grad accum)  \\\n",
       "0                                        2.813 MW   \n",
       "1                                       14.063 MW   \n",
       "2                                       28.127 MW   \n",
       "3                                      281.275 MW   \n",
       "4                                     2812.758 MW   \n",
       "0                                       14.063 MW   \n",
       "1                                       70.319 MW   \n",
       "2                                      140.638 MW   \n",
       "3                                     1406.379 MW   \n",
       "4                                    14063.792 MW   \n",
       "0                                       56.254 MW   \n",
       "1                                      281.275 MW   \n",
       "2                                      562.552 MW   \n",
       "3                                     5625.517 MW   \n",
       "4                                    56255.170 MW   \n",
       "\n",
       "  Total electricity for the entire training (without grad accum)  \n",
       "0                                           2.813 TW              \n",
       "1                                          70.319 TW              \n",
       "2                                         281.276 TW              \n",
       "3                                       28127.585 TW              \n",
       "4                                     2812758.526 TW              \n",
       "0                                           2.813 TW              \n",
       "1                                          70.319 TW              \n",
       "2                                         281.276 TW              \n",
       "3                                       28127.585 TW              \n",
       "4                                     2812758.526 TW              \n",
       "0                                           2.813 TW              \n",
       "1                                          70.319 TW              \n",
       "2                                         281.276 TW              \n",
       "3                                       28127.585 TW              \n",
       "4                                     2812758.526 TW              "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e422830-704f-4af3-9c81-3b6ee3a8eb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c797a-2914-4eb3-bdae-ce7f0026be1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e1305-4452-4fd0-8f4a-b8d8d860058e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aaf18c-6c78-4d21-82c5-15e55063b1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
