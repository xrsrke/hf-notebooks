{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4773bc69-9c7f-4d32-b13d-bd8d076e3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_weight_memory, calculate_kv_cache\n",
    "from utils import convert_bytes_to_gigabytes, convert_bytes_to_megabytes\n",
    "from name import TrainingConfig, Datatype, Transformer\n",
    "from transformer_mem_functional import calculate_memory_requirements\n",
    "from constants import LLAMA3_70B_CONFIG, LLAMA3_400B_CONFIG, VANILA_TRAINING_CONFIG, H100_MEMORY, A100_MEMORY, V100_MEMORY, MI250X_MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70d5936-fb1f-4442-91f4-ea02b8434b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_batch_sizes = [x*10**6 for x in [2, 4, 10, 40]]\n",
    "global_batch_sizes = [x*10**6 for x in [1, 2, 4, 8, 16, 32]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94849146-58d5-428a-84a5-b969b225f0f9",
   "metadata": {},
   "source": [
    "### Total training memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5b9fc8-a589-4780-8f48-f706ebb02471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from utils import convert_to_million_format, convert_bytes_to_terabytes\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1505f64-84bb-421a-87ab-52da9d809788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tp_size: 1',\n",
       " 'pp_size: 1',\n",
       " 'num_gpus: 1',\n",
       " 'ctx_length: 8192',\n",
       " 'partition_activations: False',\n",
       " 'zero1: 1',\n",
       " 'checkpoint_activations: False',\n",
       " 'batch_size_per_replicas: 1',\n",
       " 'weight_dtype: Datatype.BFLOAT16',\n",
       " 'act_dtype: Datatype.BFLOAT16',\n",
       " 'gradient_dtype: Datatype.BFLOAT16',\n",
       " 'optim_first_state_dtype: Datatype.FLOAT32',\n",
       " 'optim_second_state_dtype: Datatype.FLOAT32',\n",
       " 'master_weight_dtype: Datatype.FLOAT32']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{k}: {v}\" for k, v in asdict(VANILA_TRAINING_CONFIG).items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fd28b-b7a7-465c-abde-cddc6ca11f4b",
   "metadata": {},
   "source": [
    "### The total memory for training/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba13f93-56f0-44cc-b410-96571c8b98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044c114a-32a2-43f3-81ea-9bc3fda407a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = gbs // training_config.ctx_length\n",
    "# for gbs in global_batch_sizes:\n",
    "batch_sizes = [gbs // VANILA_TRAINING_CONFIG.ctx_length for gbs in global_batch_sizes]\n",
    "batch_sizes.insert(0, 1)\n",
    "global_batch_sizes.insert(0, VANILA_TRAINING_CONFIG.ctx_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97f1aac-878f-4876-acc8-21f09a0096b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS_PER_REPLICAS = None\n",
    "for ckp_act in [False, True]:\n",
    "    for batch_size, gbs in zip(batch_sizes, global_batch_sizes):\n",
    "        training_config = deepcopy(VANILA_TRAINING_CONFIG)\n",
    "        training_config.batch_size_per_replicas = batch_size\n",
    "        training_config.checkpoint_activations = ckp_act\n",
    "    \n",
    "        memory_requirements = calculate_memory_requirements(\n",
    "            transformer=LLAMA3_70B_CONFIG,\n",
    "            config=training_config\n",
    "        )\n",
    "        keys_that_convert_to_tb = [\"activation_mem\", \"total_training_mem\"]\n",
    "        \n",
    "        memory_requirements_gb = {}\n",
    "        memory_requirements_gb[\"name\"] = LLAMA3_70B_CONFIG.name\n",
    "        memory_requirements_gb['ckp_act'] = ckp_act\n",
    "        memory_requirements_gb['global_batch_size'] = convert_to_million_format(gbs)\n",
    "        memory_requirements_gb['batch_size'] = batch_size\n",
    "        # memory_requirements_gb = {k: convert_bytes_to_gigabytes(v) if not k in keys_that_convert_to_tb else convert_bytes_to_terabytes(v) for k, v in memory_requirements.items()}\n",
    "        memory_requirements_gb.update(\n",
    "            {k: f\"{(convert_bytes_to_gigabytes(v) if k not in keys_that_convert_to_tb else convert_bytes_to_terabytes(v))} - {percent}%\"\n",
    "             for k, (v, percent) in memory_requirements.items()}\n",
    "        )\n",
    "    \n",
    "        memory_requirements_gb[\"nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)\"] = (\n",
    "            f\"{int(memory_requirements['total_training_mem'][0] // H100_MEMORY):,}\",\n",
    "            f\"{int(memory_requirements['total_training_mem'][0] // A100_MEMORY):,}\",\n",
    "            f\"{int(memory_requirements['total_training_mem'][0] // V100_MEMORY):,}\",\n",
    "            f\"{int(memory_requirements['total_training_mem'][0] // MI250X_MEMORY):,}\",\n",
    "        )\n",
    "        memory_requirements_gb[\"nums_h100_for_inference\"] = int(int(memory_requirements[\"total_inference_mem\"][0]) // H100_MEMORY),\n",
    "        memory_data.append(memory_requirements_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd37f16-2cd2-4458-8c11-4be64fe83256",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_df = pd.DataFrame(memory_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea474f5-b6da-41e9-8f91-7d0fa0d1380b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ckp_act</th>\n",
       "      <th>global_batch_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>model_mem</th>\n",
       "      <th>activation_mem</th>\n",
       "      <th>kv_cache_mem</th>\n",
       "      <th>grad_mem</th>\n",
       "      <th>optim_mem</th>\n",
       "      <th>total_training_mem</th>\n",
       "      <th>total_inference_mem</th>\n",
       "      <th>nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)</th>\n",
       "      <th>nums_h100_for_inference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>False</td>\n",
       "      <td>8192</td>\n",
       "      <td>1</td>\n",
       "      <td>128.854 GB - 4.4%</td>\n",
       "      <td>1.901 TB - 64.83%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 4.4%</td>\n",
       "      <td>773.126 GB - 26.37%</td>\n",
       "      <td>2.931 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(34, 42, 170, 21)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0m</td>\n",
       "      <td>122</td>\n",
       "      <td>128.854 GB - 0.06%</td>\n",
       "      <td>231.864 TB - 99.56%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.06%</td>\n",
       "      <td>773.126 GB - 0.33%</td>\n",
       "      <td>232.895 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(2,711, 3,389, 13,556, 1,694)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0m</td>\n",
       "      <td>244</td>\n",
       "      <td>128.854 GB - 0.03%</td>\n",
       "      <td>463.728 TB - 99.78%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.03%</td>\n",
       "      <td>773.126 GB - 0.17%</td>\n",
       "      <td>464.758 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(5,410, 6,763, 27,052, 3,381)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0m</td>\n",
       "      <td>488</td>\n",
       "      <td>128.854 GB - 0.01%</td>\n",
       "      <td>927.455 TB - 99.89%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.01%</td>\n",
       "      <td>773.126 GB - 0.08%</td>\n",
       "      <td>928.486 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(10,809, 13,511, 54,045, 6,755)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0m</td>\n",
       "      <td>976</td>\n",
       "      <td>128.854 GB - 0.01%</td>\n",
       "      <td>1854.910 TB - 99.94%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.01%</td>\n",
       "      <td>773.126 GB - 0.04%</td>\n",
       "      <td>1855.941 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(21,606, 27,007, 108,030, 13,503)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>False</td>\n",
       "      <td>16.0m</td>\n",
       "      <td>1953</td>\n",
       "      <td>128.854 GB - 0.0%</td>\n",
       "      <td>3711.721 TB - 99.97%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.0%</td>\n",
       "      <td>773.126 GB - 0.02%</td>\n",
       "      <td>3712.752 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(43,222, 54,027, 216,110, 27,013)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>False</td>\n",
       "      <td>32.0m</td>\n",
       "      <td>3906</td>\n",
       "      <td>128.854 GB - 0.0%</td>\n",
       "      <td>7423.443 TB - 99.99%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.0%</td>\n",
       "      <td>773.126 GB - 0.01%</td>\n",
       "      <td>7424.474 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(86,432, 108,040, 432,161, 54,020)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>1</td>\n",
       "      <td>128.854 GB - 10.62%</td>\n",
       "      <td>0.183 TB - 15.04%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 10.62%</td>\n",
       "      <td>773.126 GB - 63.72%</td>\n",
       "      <td>1.213 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(14, 17, 70, 8)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0m</td>\n",
       "      <td>122</td>\n",
       "      <td>128.854 GB - 0.55%</td>\n",
       "      <td>22.269 TB - 95.58%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.55%</td>\n",
       "      <td>773.126 GB - 3.32%</td>\n",
       "      <td>23.300 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(271, 339, 1,356, 169)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0m</td>\n",
       "      <td>244</td>\n",
       "      <td>128.854 GB - 0.28%</td>\n",
       "      <td>44.539 TB - 97.74%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.28%</td>\n",
       "      <td>773.126 GB - 1.7%</td>\n",
       "      <td>45.570 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(530, 663, 2,652, 331)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0m</td>\n",
       "      <td>488</td>\n",
       "      <td>128.854 GB - 0.14%</td>\n",
       "      <td>89.078 TB - 98.86%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.14%</td>\n",
       "      <td>773.126 GB - 0.86%</td>\n",
       "      <td>90.108 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(1,049, 1,311, 5,245, 655)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>True</td>\n",
       "      <td>8.0m</td>\n",
       "      <td>976</td>\n",
       "      <td>128.854 GB - 0.07%</td>\n",
       "      <td>178.155 TB - 99.42%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.07%</td>\n",
       "      <td>773.126 GB - 0.43%</td>\n",
       "      <td>179.186 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(2,086, 2,607, 10,430, 1,303)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>True</td>\n",
       "      <td>16.0m</td>\n",
       "      <td>1953</td>\n",
       "      <td>128.854 GB - 0.04%</td>\n",
       "      <td>356.493 TB - 99.71%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.04%</td>\n",
       "      <td>773.126 GB - 0.22%</td>\n",
       "      <td>357.524 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(4,162, 5,202, 20,810, 2,601)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>True</td>\n",
       "      <td>32.0m</td>\n",
       "      <td>3906</td>\n",
       "      <td>128.854 GB - 0.02%</td>\n",
       "      <td>712.986 TB - 99.86%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.02%</td>\n",
       "      <td>773.126 GB - 0.11%</td>\n",
       "      <td>714.017 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(8,312, 10,390, 41,561, 5,195)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  ckp_act global_batch_size  batch_size            model_mem  \\\n",
       "0   llama3 70b    False              8192           1    128.854 GB - 4.4%   \n",
       "1   llama3 70b    False              1.0m         122   128.854 GB - 0.06%   \n",
       "2   llama3 70b    False              2.0m         244   128.854 GB - 0.03%   \n",
       "3   llama3 70b    False              4.0m         488   128.854 GB - 0.01%   \n",
       "4   llama3 70b    False              8.0m         976   128.854 GB - 0.01%   \n",
       "5   llama3 70b    False             16.0m        1953    128.854 GB - 0.0%   \n",
       "6   llama3 70b    False             32.0m        3906    128.854 GB - 0.0%   \n",
       "7   llama3 70b     True              8192           1  128.854 GB - 10.62%   \n",
       "8   llama3 70b     True              1.0m         122   128.854 GB - 0.55%   \n",
       "9   llama3 70b     True              2.0m         244   128.854 GB - 0.28%   \n",
       "10  llama3 70b     True              4.0m         488   128.854 GB - 0.14%   \n",
       "11  llama3 70b     True              8.0m         976   128.854 GB - 0.07%   \n",
       "12  llama3 70b     True             16.0m        1953   128.854 GB - 0.04%   \n",
       "13  llama3 70b     True             32.0m        3906   128.854 GB - 0.02%   \n",
       "\n",
       "          activation_mem    kv_cache_mem             grad_mem  \\\n",
       "0      1.901 TB - 64.83%  21.475 GB - 0%    128.854 GB - 4.4%   \n",
       "1    231.864 TB - 99.56%  21.475 GB - 0%   128.854 GB - 0.06%   \n",
       "2    463.728 TB - 99.78%  21.475 GB - 0%   128.854 GB - 0.03%   \n",
       "3    927.455 TB - 99.89%  21.475 GB - 0%   128.854 GB - 0.01%   \n",
       "4   1854.910 TB - 99.94%  21.475 GB - 0%   128.854 GB - 0.01%   \n",
       "5   3711.721 TB - 99.97%  21.475 GB - 0%    128.854 GB - 0.0%   \n",
       "6   7423.443 TB - 99.99%  21.475 GB - 0%    128.854 GB - 0.0%   \n",
       "7      0.183 TB - 15.04%  21.475 GB - 0%  128.854 GB - 10.62%   \n",
       "8     22.269 TB - 95.58%  21.475 GB - 0%   128.854 GB - 0.55%   \n",
       "9     44.539 TB - 97.74%  21.475 GB - 0%   128.854 GB - 0.28%   \n",
       "10    89.078 TB - 98.86%  21.475 GB - 0%   128.854 GB - 0.14%   \n",
       "11   178.155 TB - 99.42%  21.475 GB - 0%   128.854 GB - 0.07%   \n",
       "12   356.493 TB - 99.71%  21.475 GB - 0%   128.854 GB - 0.04%   \n",
       "13   712.986 TB - 99.86%  21.475 GB - 0%   128.854 GB - 0.02%   \n",
       "\n",
       "              optim_mem  total_training_mem total_inference_mem  \\\n",
       "0   773.126 GB - 26.37%     2.931 TB - 100%     150.329 GB - 0%   \n",
       "1    773.126 GB - 0.33%   232.895 TB - 100%     150.329 GB - 0%   \n",
       "2    773.126 GB - 0.17%   464.758 TB - 100%     150.329 GB - 0%   \n",
       "3    773.126 GB - 0.08%   928.486 TB - 100%     150.329 GB - 0%   \n",
       "4    773.126 GB - 0.04%  1855.941 TB - 100%     150.329 GB - 0%   \n",
       "5    773.126 GB - 0.02%  3712.752 TB - 100%     150.329 GB - 0%   \n",
       "6    773.126 GB - 0.01%  7424.474 TB - 100%     150.329 GB - 0%   \n",
       "7   773.126 GB - 63.72%     1.213 TB - 100%     150.329 GB - 0%   \n",
       "8    773.126 GB - 3.32%    23.300 TB - 100%     150.329 GB - 0%   \n",
       "9     773.126 GB - 1.7%    45.570 TB - 100%     150.329 GB - 0%   \n",
       "10   773.126 GB - 0.86%    90.108 TB - 100%     150.329 GB - 0%   \n",
       "11   773.126 GB - 0.43%   179.186 TB - 100%     150.329 GB - 0%   \n",
       "12   773.126 GB - 0.22%   357.524 TB - 100%     150.329 GB - 0%   \n",
       "13   773.126 GB - 0.11%   714.017 TB - 100%     150.329 GB - 0%   \n",
       "\n",
       "   nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)  \\\n",
       "0                                   (34, 42, 170, 21)                  \n",
       "1                       (2,711, 3,389, 13,556, 1,694)                  \n",
       "2                       (5,410, 6,763, 27,052, 3,381)                  \n",
       "3                     (10,809, 13,511, 54,045, 6,755)                  \n",
       "4                   (21,606, 27,007, 108,030, 13,503)                  \n",
       "5                   (43,222, 54,027, 216,110, 27,013)                  \n",
       "6                  (86,432, 108,040, 432,161, 54,020)                  \n",
       "7                                     (14, 17, 70, 8)                  \n",
       "8                              (271, 339, 1,356, 169)                  \n",
       "9                              (530, 663, 2,652, 331)                  \n",
       "10                         (1,049, 1,311, 5,245, 655)                  \n",
       "11                      (2,086, 2,607, 10,430, 1,303)                  \n",
       "12                      (4,162, 5,202, 20,810, 2,601)                  \n",
       "13                     (8,312, 10,390, 41,561, 5,195)                  \n",
       "\n",
       "   nums_h100_for_inference  \n",
       "0                     (1,)  \n",
       "1                     (1,)  \n",
       "2                     (1,)  \n",
       "3                     (1,)  \n",
       "4                     (1,)  \n",
       "5                     (1,)  \n",
       "6                     (1,)  \n",
       "7                     (1,)  \n",
       "8                     (1,)  \n",
       "9                     (1,)  \n",
       "10                    (1,)  \n",
       "11                    (1,)  \n",
       "12                    (1,)  \n",
       "13                    (1,)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d569a03-22cb-435e-9246-488265116386",
   "metadata": {},
   "source": [
    "### Activation memory's breakdown (the overall memory consumption, without activation recomputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a66eee49-5d91-4bdf-8022-ddd7f759f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_mem_functional import calculate_activation_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9966ec0-03c4-4efa-b462-a3890553f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mem_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63aebbea-349f-46f6-95ad-dbec337ab679",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size, gbs in zip(batch_sizes, global_batch_sizes):\n",
    "    training_config.batch_size_per_replicas = batch_size\n",
    "\n",
    "    memory_requirements = calculate_activation_memory(\n",
    "        transformer=LLAMA3_70B_CONFIG,\n",
    "        config=training_config\n",
    "    )[1]\n",
    "    \n",
    "    memory_requirements_gb = {}\n",
    "    memory_requirements_gb[\"name\"] = LLAMA3_70B_CONFIG.name\n",
    "    memory_requirements_gb['global_batch_size'] = convert_to_million_format(gbs)\n",
    "    memory_requirements_gb['batch_size'] = batch_size\n",
    "    memory_requirements_gb.update(\n",
    "        {k: f\"{(convert_bytes_to_gigabytes(v) if k not in keys_that_convert_to_tb else convert_bytes_to_terabytes(v))} - {percent}%\"\n",
    "         for k, (v, percent) in memory_requirements.items()}\n",
    "    )\n",
    "    act_mem_data.append(memory_requirements_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec1b64df-ed85-40f3-8c59-a8faac08e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mem_df = pd.DataFrame(act_mem_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50e7df66-8125-4a65-9dec-0ab511076bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>global_batch_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>linear_proj_input</th>\n",
       "      <th>attn_qkv_matmul</th>\n",
       "      <th>attn_qk_scores</th>\n",
       "      <th>attn_softmax</th>\n",
       "      <th>attn_dropout</th>\n",
       "      <th>attn_v</th>\n",
       "      <th>attn_drop_mask</th>\n",
       "      <th>mlp</th>\n",
       "      <th>ln</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>8192</td>\n",
       "      <td>1</td>\n",
       "      <td>0.134 GB - 0.56%</td>\n",
       "      <td>0.134 GB - 0.56%</td>\n",
       "      <td>0.268 GB - 1.13%</td>\n",
       "      <td>8.590 GB - 36.16%</td>\n",
       "      <td>4.295 GB - 18.08%</td>\n",
       "      <td>8.724 GB - 36.72%</td>\n",
       "      <td>0.067 GB - 0.28%</td>\n",
       "      <td>1.275 GB - 5.37%</td>\n",
       "      <td>0.268 GB - 1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>2.0m</td>\n",
       "      <td>244</td>\n",
       "      <td>32.749 GB - 0.56%</td>\n",
       "      <td>32.749 GB - 0.56%</td>\n",
       "      <td>65.498 GB - 1.13%</td>\n",
       "      <td>2095.944 GB - 36.16%</td>\n",
       "      <td>1047.972 GB - 18.08%</td>\n",
       "      <td>2128.693 GB - 36.72%</td>\n",
       "      <td>16.375 GB - 0.28%</td>\n",
       "      <td>311.117 GB - 5.37%</td>\n",
       "      <td>65.498 GB - 1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>4.0m</td>\n",
       "      <td>488</td>\n",
       "      <td>65.498 GB - 0.56%</td>\n",
       "      <td>65.498 GB - 0.56%</td>\n",
       "      <td>130.997 GB - 1.13%</td>\n",
       "      <td>4191.888 GB - 36.16%</td>\n",
       "      <td>2095.944 GB - 18.08%</td>\n",
       "      <td>4257.386 GB - 36.72%</td>\n",
       "      <td>32.749 GB - 0.28%</td>\n",
       "      <td>622.233 GB - 5.37%</td>\n",
       "      <td>130.997 GB - 1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>10.0m</td>\n",
       "      <td>1220</td>\n",
       "      <td>163.746 GB - 0.56%</td>\n",
       "      <td>163.746 GB - 0.56%</td>\n",
       "      <td>327.491 GB - 1.13%</td>\n",
       "      <td>10479.720 GB - 36.16%</td>\n",
       "      <td>5239.860 GB - 18.08%</td>\n",
       "      <td>10643.466 GB - 36.72%</td>\n",
       "      <td>81.873 GB - 0.28%</td>\n",
       "      <td>1555.583 GB - 5.37%</td>\n",
       "      <td>327.491 GB - 1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>40.0m</td>\n",
       "      <td>4882</td>\n",
       "      <td>655.251 GB - 0.56%</td>\n",
       "      <td>655.251 GB - 0.56%</td>\n",
       "      <td>1310.502 GB - 1.13%</td>\n",
       "      <td>41936.061 GB - 36.16%</td>\n",
       "      <td>20968.030 GB - 18.08%</td>\n",
       "      <td>42591.312 GB - 36.72%</td>\n",
       "      <td>327.625 GB - 0.28%</td>\n",
       "      <td>6224.884 GB - 5.37%</td>\n",
       "      <td>1310.502 GB - 1.13%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name global_batch_size  batch_size   linear_proj_input  \\\n",
       "0  llama3 70b              8192           1    0.134 GB - 0.56%   \n",
       "1  llama3 70b              2.0m         244   32.749 GB - 0.56%   \n",
       "2  llama3 70b              4.0m         488   65.498 GB - 0.56%   \n",
       "3  llama3 70b             10.0m        1220  163.746 GB - 0.56%   \n",
       "4  llama3 70b             40.0m        4882  655.251 GB - 0.56%   \n",
       "\n",
       "      attn_qkv_matmul       attn_qk_scores           attn_softmax  \\\n",
       "0    0.134 GB - 0.56%     0.268 GB - 1.13%      8.590 GB - 36.16%   \n",
       "1   32.749 GB - 0.56%    65.498 GB - 1.13%   2095.944 GB - 36.16%   \n",
       "2   65.498 GB - 0.56%   130.997 GB - 1.13%   4191.888 GB - 36.16%   \n",
       "3  163.746 GB - 0.56%   327.491 GB - 1.13%  10479.720 GB - 36.16%   \n",
       "4  655.251 GB - 0.56%  1310.502 GB - 1.13%  41936.061 GB - 36.16%   \n",
       "\n",
       "            attn_dropout                 attn_v      attn_drop_mask  \\\n",
       "0      4.295 GB - 18.08%      8.724 GB - 36.72%    0.067 GB - 0.28%   \n",
       "1   1047.972 GB - 18.08%   2128.693 GB - 36.72%   16.375 GB - 0.28%   \n",
       "2   2095.944 GB - 18.08%   4257.386 GB - 36.72%   32.749 GB - 0.28%   \n",
       "3   5239.860 GB - 18.08%  10643.466 GB - 36.72%   81.873 GB - 0.28%   \n",
       "4  20968.030 GB - 18.08%  42591.312 GB - 36.72%  327.625 GB - 0.28%   \n",
       "\n",
       "                   mlp                   ln  \n",
       "0     1.275 GB - 5.37%     0.268 GB - 1.13%  \n",
       "1   311.117 GB - 5.37%    65.498 GB - 1.13%  \n",
       "2   622.233 GB - 5.37%   130.997 GB - 1.13%  \n",
       "3  1555.583 GB - 5.37%   327.491 GB - 1.13%  \n",
       "4  6224.884 GB - 5.37%  1310.502 GB - 1.13%  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_mem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7f9e2-c270-429e-95c7-fd6854fa0ea7",
   "metadata": {},
   "source": [
    "### Datacenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc69d3fd-323b-4777-8eab-03bdc14503bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract minimal GPUs per cluster for batch_size = 1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m min_gpus_str \u001b[38;5;241m=\u001b[39m \u001b[43mmemory_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmemory_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m min_h100, min_a100, min_v100, min_mi250x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m min_gpus_str]\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.10/site-packages/pandas/core/base.py:418\u001b[0m, in \u001b[0;36mIndexOpsMixin.item\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only convert an array of size 1 to a Python scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "# Extract minimal GPUs per cluster for batch_size = 1\n",
    "min_gpus_str = memory_df[memory_df['batch_size'] == 1][\"nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)\"].item()\n",
    "min_h100, min_a100, min_v100, min_mi250x = [int(x.replace(\",\", \"\")) for x in min_gpus_str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f22783-c5d9-48a0-a518-faa9a588d457",
   "metadata": {},
   "source": [
    "The minimum number of GPUs required for each cluster using the following hardware: H100, A100, V100, and MI250X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2948bfa-4115-427a-98b9-1d8b2f46fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gpus_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2a1d13e-44c9-4c9d-b326-3fc73eece76c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract total GPUs required for global_batch_size = 2.0m\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m total_gpus_str \u001b[38;5;241m=\u001b[39m \u001b[43mmemory_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmemory_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglobal_batch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2.0m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m total_h100, total_a100, total_v100, total_mi250x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m total_gpus_str]\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.10/site-packages/pandas/core/base.py:418\u001b[0m, in \u001b[0;36mIndexOpsMixin.item\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only convert an array of size 1 to a Python scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "# Extract total GPUs required for global_batch_size = 2.0m\n",
    "total_gpus_str = memory_df[memory_df['global_batch_size'] == \"2.0m\"][\"nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)\"].item()\n",
    "total_h100, total_a100, total_v100, total_mi250x = [int(x.replace(\",\", \"\")) for x in total_gpus_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "079a4ab6-dce0-4dee-aa95-de879c5ae417",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'min_h100' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gpu_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH100\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA100\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV100\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMI250X\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m base_gpus_per_cluster \u001b[38;5;241m=\u001b[39m [\u001b[43mmin_h100\u001b[49m, min_a100, min_v100, min_mi250x]\n\u001b[1;32m      3\u001b[0m totals \u001b[38;5;241m=\u001b[39m [total_h100, total_a100, total_v100, total_mi250x]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'min_h100' is not defined"
     ]
    }
   ],
   "source": [
    "gpu_names = ['H100', 'A100', 'V100', 'MI250X']\n",
    "base_gpus_per_cluster = [min_h100, min_a100, min_v100, min_mi250x]\n",
    "totals = [total_h100, total_a100, total_v100, total_mi250x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca8182-eab3-45a9-9c04-fa55b20fa78a",
   "metadata": {},
   "source": [
    "The maximum number of clusters, assuming each cluster trains with a batch size of 1, given a global batch size of 2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fade32ea-bf93-46ec-a948-e315d67038ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_gpus_per_cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m factors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m factors:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Scale the gpus_per_cluster by the factor\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     scaled_gpus_per_cluster \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;241m*\u001b[39m factor \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbase_gpus_per_cluster\u001b[49m]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Compute maximum number of clusters for each GPU type\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     max_clusters \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m g \u001b[38;5;28;01mfor\u001b[39;00m t, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(totals, scaled_gpus_per_cluster)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_gpus_per_cluster' is not defined"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "factors = [1, 5, 10, 15]\n",
    "for factor in factors:\n",
    "    # Scale the gpus_per_cluster by the factor\n",
    "    scaled_gpus_per_cluster = [g * factor for g in base_gpus_per_cluster]\n",
    "    \n",
    "    # Compute maximum number of clusters for each GPU type\n",
    "    max_clusters = [t // g for t, g in zip(totals, scaled_gpus_per_cluster)]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'gpu_name': gpu_names,\n",
    "        # 'factor': factor,\n",
    "        'gpus_per_cluster': scaled_gpus_per_cluster,\n",
    "        'maximum_number_of_datacenters': max_clusters\n",
    "    })\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames for a final result\n",
    "result_df = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1429681-abb9-4575-a40a-ffbbf7d1cd48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f309a-5f50-43f6-93fc-a6b36e96a044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5e8c2-eb0e-4a4f-a2cb-24af213469bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
