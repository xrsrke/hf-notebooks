{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4773bc69-9c7f-4d32-b13d-bd8d076e3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_weight_memory, calculate_kv_cache\n",
    "from utils import convert_bytes_to_gigabytes, convert_bytes_to_megabytes\n",
    "from name import TrainingConfig, Datatype, Transformer\n",
    "from transformer_mem_functional import calculate_memory_requirements\n",
    "from constants import LLAMA3_70B_CONFIG, LLAMA3_400B_CONFIG, VANILA_TRAINING_CONFIG, H100_MEMORY, A100_MEMORY, V100_MEMORY, MI250X_MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70d5936-fb1f-4442-91f4-ea02b8434b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_sizes = [x*10**6 for x in [2, 4, 10, 40]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94849146-58d5-428a-84a5-b969b225f0f9",
   "metadata": {},
   "source": [
    "### Total training memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5b9fc8-a589-4780-8f48-f706ebb02471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from utils import convert_to_million_format, convert_bytes_to_terabytes\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1505f64-84bb-421a-87ab-52da9d809788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tp_size: 1',\n",
       " 'pp_size: 1',\n",
       " 'num_gpus: 1',\n",
       " 'ctx_length: 8192',\n",
       " 'partition_activations: False',\n",
       " 'zero1: 1',\n",
       " 'checkpoint_activations: False',\n",
       " 'batch_size_per_replicas: 1',\n",
       " 'weight_dtype: Datatype.BFLOAT16',\n",
       " 'act_dtype: Datatype.BFLOAT16',\n",
       " 'gradient_dtype: Datatype.BFLOAT16',\n",
       " 'optim_first_state_dtype: Datatype.FLOAT32',\n",
       " 'optim_second_state_dtype: Datatype.FLOAT32',\n",
       " 'master_weight_dtype: Datatype.FLOAT32']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{k}: {v}\" for k, v in asdict(VANILA_TRAINING_CONFIG).items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fd28b-b7a7-465c-abde-cddc6ca11f4b",
   "metadata": {},
   "source": [
    "### The total memory for training/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba13f93-56f0-44cc-b410-96571c8b98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044c114a-32a2-43f3-81ea-9bc3fda407a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = gbs // training_config.ctx_length\n",
    "# for gbs in global_batch_sizes:\n",
    "batch_sizes = [gbs // VANILA_TRAINING_CONFIG.ctx_length for gbs in global_batch_sizes]\n",
    "batch_sizes.insert(0, 1)\n",
    "global_batch_sizes.insert(0, VANILA_TRAINING_CONFIG.ctx_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97f1aac-878f-4876-acc8-21f09a0096b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS_PER_REPLICAS = None\n",
    "for batch_size, gbs in zip(batch_sizes, global_batch_sizes):\n",
    "    training_config = deepcopy(VANILA_TRAINING_CONFIG)\n",
    "    training_config.batch_size_per_replicas = batch_size\n",
    "\n",
    "    memory_requirements = calculate_memory_requirements(\n",
    "        transformer=LLAMA3_70B_CONFIG,\n",
    "        config=training_config\n",
    "    )\n",
    "    keys_that_convert_to_tb = [\"activation_mem\", \"total_training_mem\"]\n",
    "    \n",
    "    memory_requirements_gb = {}\n",
    "    memory_requirements_gb[\"name\"] = LLAMA3_70B_CONFIG.name\n",
    "    memory_requirements_gb['global_batch_size'] = convert_to_million_format(gbs)\n",
    "    memory_requirements_gb['batch_size'] = batch_size\n",
    "    # memory_requirements_gb = {k: convert_bytes_to_gigabytes(v) if not k in keys_that_convert_to_tb else convert_bytes_to_terabytes(v) for k, v in memory_requirements.items()}\n",
    "    memory_requirements_gb.update(\n",
    "        {k: f\"{(convert_bytes_to_gigabytes(v) if k not in keys_that_convert_to_tb else convert_bytes_to_terabytes(v))} - {percent}%\"\n",
    "         for k, (v, percent) in memory_requirements.items()}\n",
    "    )\n",
    "\n",
    "    memory_requirements_gb[\"nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)\"] = (\n",
    "        f\"{int(memory_requirements['total_training_mem'][0] // H100_MEMORY):,}\",\n",
    "        f\"{int(memory_requirements['total_training_mem'][0] // A100_MEMORY):,}\",\n",
    "        f\"{int(memory_requirements['total_training_mem'][0] // V100_MEMORY):,}\",\n",
    "        f\"{int(memory_requirements['total_training_mem'][0] // MI250X_MEMORY):,}\",\n",
    "    )\n",
    "    memory_requirements_gb[\"nums_h100_for_inference\"] = int(int(memory_requirements[\"total_inference_mem\"][0]) // H100_MEMORY),\n",
    "    memory_data.append(memory_requirements_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd37f16-2cd2-4458-8c11-4be64fe83256",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_df = pd.DataFrame(memory_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea474f5-b6da-41e9-8f91-7d0fa0d1380b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>global_batch_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>model_mem</th>\n",
       "      <th>activation_mem</th>\n",
       "      <th>kv_cache_mem</th>\n",
       "      <th>grad_mem</th>\n",
       "      <th>optim_mem</th>\n",
       "      <th>total_training_mem</th>\n",
       "      <th>total_inference_mem</th>\n",
       "      <th>nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)</th>\n",
       "      <th>nums_h100_for_inference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>8192</td>\n",
       "      <td>1</td>\n",
       "      <td>128.854 GB - 4.4%</td>\n",
       "      <td>1.901 TB - 64.83%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 4.4%</td>\n",
       "      <td>773.126 GB - 26.37%</td>\n",
       "      <td>2.931 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(34, 42, 170, 21)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>2.0m</td>\n",
       "      <td>244</td>\n",
       "      <td>128.854 GB - 0.03%</td>\n",
       "      <td>463.728 TB - 99.78%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.03%</td>\n",
       "      <td>773.126 GB - 0.17%</td>\n",
       "      <td>464.758 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(5,410, 6,763, 27,052, 3,381)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>4.0m</td>\n",
       "      <td>488</td>\n",
       "      <td>128.854 GB - 0.01%</td>\n",
       "      <td>927.455 TB - 99.89%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.01%</td>\n",
       "      <td>773.126 GB - 0.08%</td>\n",
       "      <td>928.486 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(10,809, 13,511, 54,045, 6,755)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>10.0m</td>\n",
       "      <td>1220</td>\n",
       "      <td>128.854 GB - 0.01%</td>\n",
       "      <td>2318.638 TB - 99.96%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.01%</td>\n",
       "      <td>773.126 GB - 0.03%</td>\n",
       "      <td>2319.669 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(27,004, 33,755, 135,022, 16,877)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>40.0m</td>\n",
       "      <td>4882</td>\n",
       "      <td>128.854 GB - 0.0%</td>\n",
       "      <td>9278.353 TB - 99.99%</td>\n",
       "      <td>21.475 GB - 0%</td>\n",
       "      <td>128.854 GB - 0.0%</td>\n",
       "      <td>773.126 GB - 0.01%</td>\n",
       "      <td>9279.384 TB - 100%</td>\n",
       "      <td>150.329 GB - 0%</td>\n",
       "      <td>(108,026, 135,032, 540,131, 67,516)</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name global_batch_size  batch_size           model_mem  \\\n",
       "0  llama3 70b              8192           1   128.854 GB - 4.4%   \n",
       "1  llama3 70b              2.0m         244  128.854 GB - 0.03%   \n",
       "2  llama3 70b              4.0m         488  128.854 GB - 0.01%   \n",
       "3  llama3 70b             10.0m        1220  128.854 GB - 0.01%   \n",
       "4  llama3 70b             40.0m        4882   128.854 GB - 0.0%   \n",
       "\n",
       "         activation_mem    kv_cache_mem            grad_mem  \\\n",
       "0     1.901 TB - 64.83%  21.475 GB - 0%   128.854 GB - 4.4%   \n",
       "1   463.728 TB - 99.78%  21.475 GB - 0%  128.854 GB - 0.03%   \n",
       "2   927.455 TB - 99.89%  21.475 GB - 0%  128.854 GB - 0.01%   \n",
       "3  2318.638 TB - 99.96%  21.475 GB - 0%  128.854 GB - 0.01%   \n",
       "4  9278.353 TB - 99.99%  21.475 GB - 0%   128.854 GB - 0.0%   \n",
       "\n",
       "             optim_mem  total_training_mem total_inference_mem  \\\n",
       "0  773.126 GB - 26.37%     2.931 TB - 100%     150.329 GB - 0%   \n",
       "1   773.126 GB - 0.17%   464.758 TB - 100%     150.329 GB - 0%   \n",
       "2   773.126 GB - 0.08%   928.486 TB - 100%     150.329 GB - 0%   \n",
       "3   773.126 GB - 0.03%  2319.669 TB - 100%     150.329 GB - 0%   \n",
       "4   773.126 GB - 0.01%  9279.384 TB - 100%     150.329 GB - 0%   \n",
       "\n",
       "  nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)  \\\n",
       "0                                  (34, 42, 170, 21)                  \n",
       "1                      (5,410, 6,763, 27,052, 3,381)                  \n",
       "2                    (10,809, 13,511, 54,045, 6,755)                  \n",
       "3                  (27,004, 33,755, 135,022, 16,877)                  \n",
       "4                (108,026, 135,032, 540,131, 67,516)                  \n",
       "\n",
       "  nums_h100_for_inference  \n",
       "0                    (1,)  \n",
       "1                    (1,)  \n",
       "2                    (1,)  \n",
       "3                    (1,)  \n",
       "4                    (1,)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d569a03-22cb-435e-9246-488265116386",
   "metadata": {},
   "source": [
    "### Activation memory's breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66eee49-5d91-4bdf-8022-ddd7f759f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_mem_functional import calculate_activation_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9966ec0-03c4-4efa-b462-a3890553f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mem_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63aebbea-349f-46f6-95ad-dbec337ab679",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size, gbs in zip(batch_sizes, global_batch_sizes):\n",
    "    training_config.batch_size_per_replicas = batch_size\n",
    "\n",
    "    memory_requirements = calculate_activation_memory(\n",
    "        transformer=LLAMA3_70B_CONFIG,\n",
    "        config=training_config\n",
    "    )[1]\n",
    "    \n",
    "    memory_requirements_gb = {}\n",
    "    memory_requirements_gb[\"name\"] = LLAMA3_70B_CONFIG.name\n",
    "    memory_requirements_gb['global_batch_size'] = convert_to_million_format(gbs)\n",
    "    memory_requirements_gb['batch_size'] = batch_size\n",
    "    memory_requirements_gb.update(\n",
    "        {k: f\"{(convert_bytes_to_gigabytes(v) if k not in keys_that_convert_to_tb else convert_bytes_to_terabytes(v))} - {percent}%\"\n",
    "         for k, (v, percent) in memory_requirements.items()}\n",
    "    )\n",
    "    act_mem_data.append(memory_requirements_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec1b64df-ed85-40f3-8c59-a8faac08e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mem_df = pd.DataFrame(act_mem_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e7df66-8125-4a65-9dec-0ab511076bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>global_batch_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>linear_proj_input</th>\n",
       "      <th>attn_qkv_matmul</th>\n",
       "      <th>attn_qk_scores</th>\n",
       "      <th>attn_softmax</th>\n",
       "      <th>attn_dropout</th>\n",
       "      <th>attn_v</th>\n",
       "      <th>attn_drop_mask</th>\n",
       "      <th>mlp</th>\n",
       "      <th>ln</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>8192</td>\n",
       "      <td>1</td>\n",
       "      <td>0.134 GB - 0.56%</td>\n",
       "      <td>0.134 GB - 0.56%</td>\n",
       "      <td>0.268 GB - 1.13%</td>\n",
       "      <td>8.590 GB - 36.16%</td>\n",
       "      <td>4.295 GB - 18.08%</td>\n",
       "      <td>8.724 GB - 36.72%</td>\n",
       "      <td>0.067 GB - 0.28%</td>\n",
       "      <td>1.275 GB - 5.37%</td>\n",
       "      <td>0.268 GB - 1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>2.0m</td>\n",
       "      <td>244</td>\n",
       "      <td>32.749 GB - 0.56%</td>\n",
       "      <td>32.749 GB - 0.56%</td>\n",
       "      <td>65.498 GB - 1.13%</td>\n",
       "      <td>2095.944 GB - 36.16%</td>\n",
       "      <td>1047.972 GB - 18.08%</td>\n",
       "      <td>2128.693 GB - 36.72%</td>\n",
       "      <td>16.375 GB - 0.28%</td>\n",
       "      <td>311.117 GB - 5.37%</td>\n",
       "      <td>65.498 GB - 1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>4.0m</td>\n",
       "      <td>488</td>\n",
       "      <td>65.498 GB - 0.56%</td>\n",
       "      <td>65.498 GB - 0.56%</td>\n",
       "      <td>130.997 GB - 1.13%</td>\n",
       "      <td>4191.888 GB - 36.16%</td>\n",
       "      <td>2095.944 GB - 18.08%</td>\n",
       "      <td>4257.386 GB - 36.72%</td>\n",
       "      <td>32.749 GB - 0.28%</td>\n",
       "      <td>622.233 GB - 5.37%</td>\n",
       "      <td>130.997 GB - 1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>10.0m</td>\n",
       "      <td>1220</td>\n",
       "      <td>163.746 GB - 0.56%</td>\n",
       "      <td>163.746 GB - 0.56%</td>\n",
       "      <td>327.491 GB - 1.13%</td>\n",
       "      <td>10479.720 GB - 36.16%</td>\n",
       "      <td>5239.860 GB - 18.08%</td>\n",
       "      <td>10643.466 GB - 36.72%</td>\n",
       "      <td>81.873 GB - 0.28%</td>\n",
       "      <td>1555.583 GB - 5.37%</td>\n",
       "      <td>327.491 GB - 1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3 70b</td>\n",
       "      <td>40.0m</td>\n",
       "      <td>4882</td>\n",
       "      <td>655.251 GB - 0.56%</td>\n",
       "      <td>655.251 GB - 0.56%</td>\n",
       "      <td>1310.502 GB - 1.13%</td>\n",
       "      <td>41936.061 GB - 36.16%</td>\n",
       "      <td>20968.030 GB - 18.08%</td>\n",
       "      <td>42591.312 GB - 36.72%</td>\n",
       "      <td>327.625 GB - 0.28%</td>\n",
       "      <td>6224.884 GB - 5.37%</td>\n",
       "      <td>1310.502 GB - 1.13%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name global_batch_size  batch_size   linear_proj_input  \\\n",
       "0  llama3 70b              8192           1    0.134 GB - 0.56%   \n",
       "1  llama3 70b              2.0m         244   32.749 GB - 0.56%   \n",
       "2  llama3 70b              4.0m         488   65.498 GB - 0.56%   \n",
       "3  llama3 70b             10.0m        1220  163.746 GB - 0.56%   \n",
       "4  llama3 70b             40.0m        4882  655.251 GB - 0.56%   \n",
       "\n",
       "      attn_qkv_matmul       attn_qk_scores           attn_softmax  \\\n",
       "0    0.134 GB - 0.56%     0.268 GB - 1.13%      8.590 GB - 36.16%   \n",
       "1   32.749 GB - 0.56%    65.498 GB - 1.13%   2095.944 GB - 36.16%   \n",
       "2   65.498 GB - 0.56%   130.997 GB - 1.13%   4191.888 GB - 36.16%   \n",
       "3  163.746 GB - 0.56%   327.491 GB - 1.13%  10479.720 GB - 36.16%   \n",
       "4  655.251 GB - 0.56%  1310.502 GB - 1.13%  41936.061 GB - 36.16%   \n",
       "\n",
       "            attn_dropout                 attn_v      attn_drop_mask  \\\n",
       "0      4.295 GB - 18.08%      8.724 GB - 36.72%    0.067 GB - 0.28%   \n",
       "1   1047.972 GB - 18.08%   2128.693 GB - 36.72%   16.375 GB - 0.28%   \n",
       "2   2095.944 GB - 18.08%   4257.386 GB - 36.72%   32.749 GB - 0.28%   \n",
       "3   5239.860 GB - 18.08%  10643.466 GB - 36.72%   81.873 GB - 0.28%   \n",
       "4  20968.030 GB - 18.08%  42591.312 GB - 36.72%  327.625 GB - 0.28%   \n",
       "\n",
       "                   mlp                   ln  \n",
       "0     1.275 GB - 5.37%     0.268 GB - 1.13%  \n",
       "1   311.117 GB - 5.37%    65.498 GB - 1.13%  \n",
       "2   622.233 GB - 5.37%   130.997 GB - 1.13%  \n",
       "3  1555.583 GB - 5.37%   327.491 GB - 1.13%  \n",
       "4  6224.884 GB - 5.37%  1310.502 GB - 1.13%  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_mem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7f9e2-c270-429e-95c7-fd6854fa0ea7",
   "metadata": {},
   "source": [
    "### Datacenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc69d3fd-323b-4777-8eab-03bdc14503bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract minimal GPUs per cluster for batch_size = 1\n",
    "min_gpus_str = memory_df[memory_df['batch_size'] == 1][\"nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)\"].item()\n",
    "min_h100, min_a100, min_v100, min_mi250x = [int(x.replace(\",\", \"\")) for x in min_gpus_str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f22783-c5d9-48a0-a518-faa9a588d457",
   "metadata": {},
   "source": [
    "The minimum number of GPUs required for each cluster using the following hardware: H100, A100, V100, and MI250X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2948bfa-4115-427a-98b9-1d8b2f46fede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('34', '42', '170', '21')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_gpus_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2a1d13e-44c9-4c9d-b326-3fc73eece76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract total GPUs required for global_batch_size = 2.0m\n",
    "total_gpus_str = memory_df[memory_df['global_batch_size'] == \"2.0m\"][\"nums_gpus_for_training_without_grad_accum_(h100,a100,v100,MI250X)\"].item()\n",
    "total_h100, total_a100, total_v100, total_mi250x = [int(x.replace(\",\", \"\")) for x in total_gpus_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "079a4ab6-dce0-4dee-aa95-de879c5ae417",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_names = ['H100', 'A100', 'V100', 'MI250X']\n",
    "base_gpus_per_cluster = [min_h100, min_a100, min_v100, min_mi250x]\n",
    "totals = [total_h100, total_a100, total_v100, total_mi250x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca8182-eab3-45a9-9c04-fa55b20fa78a",
   "metadata": {},
   "source": [
    "The maximum number of clusters, assuming each cluster trains with a batch size of 1, given a global batch size of 2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fade32ea-bf93-46ec-a948-e315d67038ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "factors = [1, 5, 10, 15]\n",
    "for factor in factors:\n",
    "    # Scale the gpus_per_cluster by the factor\n",
    "    scaled_gpus_per_cluster = [g * factor for g in base_gpus_per_cluster]\n",
    "    \n",
    "    # Compute maximum number of clusters for each GPU type\n",
    "    max_clusters = [t // g for t, g in zip(totals, scaled_gpus_per_cluster)]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'gpu_name': gpu_names,\n",
    "        # 'factor': factor,\n",
    "        'gpus_per_cluster': scaled_gpus_per_cluster,\n",
    "        'maximum_number_of_datacenters': max_clusters\n",
    "    })\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames for a final result\n",
    "result_df = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1429681-abb9-4575-a40a-ffbbf7d1cd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_name</th>\n",
       "      <th>gpus_per_cluster</th>\n",
       "      <th>maximum_number_of_datacenters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>34</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A100</td>\n",
       "      <td>42</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V100</td>\n",
       "      <td>170</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MI250X</td>\n",
       "      <td>21</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H100</td>\n",
       "      <td>170</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A100</td>\n",
       "      <td>210</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>V100</td>\n",
       "      <td>850</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MI250X</td>\n",
       "      <td>105</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H100</td>\n",
       "      <td>340</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A100</td>\n",
       "      <td>420</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>V100</td>\n",
       "      <td>1700</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MI250X</td>\n",
       "      <td>210</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>H100</td>\n",
       "      <td>510</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A100</td>\n",
       "      <td>630</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>V100</td>\n",
       "      <td>2550</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MI250X</td>\n",
       "      <td>315</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_name  gpus_per_cluster  maximum_number_of_datacenters\n",
       "0      H100                34                            159\n",
       "1      A100                42                            161\n",
       "2      V100               170                            159\n",
       "3    MI250X                21                            161\n",
       "4      H100               170                             31\n",
       "5      A100               210                             32\n",
       "6      V100               850                             31\n",
       "7    MI250X               105                             32\n",
       "8      H100               340                             15\n",
       "9      A100               420                             16\n",
       "10     V100              1700                             15\n",
       "11   MI250X               210                             16\n",
       "12     H100               510                             10\n",
       "13     A100               630                             10\n",
       "14     V100              2550                             10\n",
       "15   MI250X               315                             10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f309a-5f50-43f6-93fc-a6b36e96a044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5e8c2-eb0e-4a4f-a2cb-24af213469bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
