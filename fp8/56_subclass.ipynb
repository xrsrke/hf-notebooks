{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69525870-05ca-4b32-9b8d-490ad7ae15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61e75a8-d8d1-4e2f-acd2-6cc0a7cbbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is within core, the end user never have to look at this\n",
    "class WrapperTensor(torch.Tensor):\n",
    "    @staticmethod\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        t, kwargs = cls.get_wrapper_properties(*args, **kwargs)\n",
    "        if \"size\" not in kwargs:\n",
    "            size = t.size()\n",
    "        else:\n",
    "            size = kwargs[\"size\"]\n",
    "            del kwargs[\"size\"]\n",
    "        if \"dtype\" not in kwargs:\n",
    "            kwargs[\"dtype\"] = t.dtype\n",
    "        if \"layout\" not in kwargs:\n",
    "            kwargs[\"layout\"] = t.layout\n",
    "        if \"device\" not in kwargs:\n",
    "            kwargs[\"device\"] = t.device\n",
    "        if \"requires_grad\" not in kwargs:\n",
    "            kwargs[\"requires_grad\"] = False\n",
    "        # Ignore memory_format and pin memory for now as I don't know how to\n",
    "        # safely access them on a Tensor (if possible??)\n",
    "\n",
    "        wrapper = torch.Tensor._make_wrapper_subclass(cls, size, **kwargs)\n",
    "        wrapper._validate_methods()\n",
    "        return wrapper\n",
    "\n",
    "    @classmethod\n",
    "    def get_wrapper_properties(cls, *args, **kwargs):\n",
    "        # Should return both an example Tensor and a dictionaly of kwargs\n",
    "        # to override any of that example Tensor's properly.\n",
    "        # This is very similar to the `t.new_*(args)` API\n",
    "        raise NotImplementedError(\"You need to implement get_wrapper_properties\")\n",
    "\n",
    "    def _validate_methods(self):\n",
    "        # Skip this if not in debug mode?\n",
    "        # Changing these on the python side is wrong as it would not be properly reflected\n",
    "        # on the c++ side\n",
    "        # This doesn't catch attributes set in the __init__\n",
    "        forbidden_overrides = [\"size\", \"stride\", \"dtype\", \"layout\", \"device\", \"requires_grad\"]\n",
    "        for el in forbidden_overrides:\n",
    "            if getattr(self.__class__, el) is not getattr(torch.Tensor, el):\n",
    "                raise RuntimeError(f\"Subclass {self.__class__.__name__} is overwriting the \"\n",
    "                                   f\"property {el} but this is not allowed as such change would \"\n",
    "                                   \"not be reflected to c++ callers.\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.__dict__})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78088dae-2f7c-4eec-b78f-76f3fbd97c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils._pytree import tree_map\n",
    "\n",
    "# Concept of wrapper Tensor is that there is a Tensor object without storage\n",
    "# that represent what your Tensor should be. And you can store any other\n",
    "# object in there.\n",
    "# For DiagTensor, the wrapper will be 2D while the stored Tensor is 1D\n",
    "class DiagTensor(WrapperTensor):\n",
    "    @classmethod\n",
    "    def get_wrapper_properties(cls, diag):\n",
    "        return diag, {\"size\": diag.size() + diag.size()}\n",
    "\n",
    "    def __init__(self, diag):\n",
    "        self._diag = diag\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "      return self._diag\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, data):\n",
    "        self._diag = data\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "        def unwrap(e):\n",
    "            return torch.diag(e._diag) if isinstance(e, DiagTensor) else e\n",
    "\n",
    "        def wrap(e):\n",
    "            return DiagTensor(torch.diag(e)) if isinstance(e, torch.Tensor) else e\n",
    "\n",
    "        rs = tree_map(wrap, func(*tree_map(unwrap, args), **tree_map(unwrap, kwargs)))\n",
    "        return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de85488-cb69-41c8-97e4-8f19c771868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DiagTensor(torch.rand(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df81a80-4552-4d63-a8d5-19b4ec583199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing add and mul\n",
      "DiagTensor({'_diag': tensor([1.4086, 2.7097])})\n",
      "Wrapper Tensor size\n",
      "torch.Size([2, 2])\n",
      "Contained Tensor size\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Doing add and mul\")\n",
    "out = (t + 2) * t\n",
    "print(out)\n",
    "\n",
    "print(\"Wrapper Tensor size\")\n",
    "print(out.size())\n",
    "print(\"Contained Tensor size\")\n",
    "print(out._diag.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a4d866-2df4-46aa-a5d4-3972ce7667c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data = torch.tensor([1., 2.], dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ef3beb1-ebb6-448d-90d0-7aa3aef6a197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiagTensor({'_diag': tensor([1, 2], dtype=torch.int8)})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94982a6e-6c90-4e6e-904c-7abe597c4eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
